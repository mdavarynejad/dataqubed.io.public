<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pyspark | Your Data Science Mentor - Mohsen Davarynejad</title>
    <link>https://dataqubed.io/tags/pyspark/</link>
      <atom:link href="https://dataqubed.io/tags/pyspark/index.xml" rel="self" type="application/rss+xml" />
    <description>Pyspark</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 22 Aug 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dataqubed.io/media/icon_hu7729264130191091259.png</url>
      <title>Pyspark</title>
      <link>https://dataqubed.io/tags/pyspark/</link>
    </image>
    
    <item>
      <title>PySpark - Module 5.3</title>
      <link>https://dataqubed.io/teaching/pyspark-spark-streaming-part3/</link>
      <pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-spark-streaming-part3/</guid>
      <description>&lt;p&gt;While we experimented with setting up a Kafka service and accessing it programmatically, the solution provided in Module 5.2 does not leverage the capabilities of Spark and distributed computing as it focuses primarily on Python-based producers and consumers, without integrating Spark&amp;rsquo;s distributed computing power.&lt;/p&gt;
&lt;h2 id=&#34;integration-of-kafka-with-apache-spark-structured-streaming&#34;&gt;Integration of Kafka with Apache Spark Structured Streaming:&lt;/h2&gt;
&lt;h3 id=&#34;reading-data-from-kafka-in-spark-using-pyspark-for-streaming&#34;&gt;Reading Data from Kafka in Spark: Using PySpark for Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Configure Kafka Consumer: Use PySpark&amp;rsquo;s spark.readStream method to configure a Kafka consumer. Specify the Kafka bootstrap servers and the topic you want to read from.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stream Data Processing: Define the schema for the incoming data and apply any necessary transformations using PySpark DataFrame operations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start Streaming: Use writeStream to output the results to a desired sink, like console, HDFS, or another Kafka topic, and start the streaming process.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Import 
 to the same folder, rename it to &lt;code&gt;Kafka-to-Delta Streaming Pipeline&lt;/code&gt;, and follow the instructions.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;Writing Data to Kafka from Spark!&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    How to write processed data back to Kafka topics using PySpark? This is a topic that I encourage motivated students to study it by themselves.
  &lt;/div&gt;
&lt;/details&gt;
&lt;h3 id=&#34;windowed-aggregations-performing-windowed-aggregations-on-streaming-data-from-kafka-using-pyspark&#34;&gt;Windowed Aggregations: Performing windowed aggregations on streaming data from Kafka using PySpark&lt;/h3&gt;
&lt;p&gt;Import 
 to the same folder and rename in to &lt;code&gt;Windowed Aggregations on Streaming Data&lt;/code&gt;. Follow the content and try to understand what is going on there.&lt;/p&gt;
&lt;h3 id=&#34;assignment&#34;&gt;Assignment&lt;/h3&gt;
&lt;p&gt;Import 
 to the same folder and look into the results. Can you explain what is the results you see in table &lt;code&gt;windowed_aggregations&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Whats next?&lt;/strong&gt; Module 5.4 (for year 2025-2026) will be dedicated to the best practices for kafka in databricks! Do you absolutely want to have it this year? Write Mohsen and email.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 5.2</title>
      <link>https://dataqubed.io/teaching/pyspark-spark-streaming-part2/</link>
      <pubDate>Wed, 21 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-spark-streaming-part2/</guid>
      <description>&lt;h2 id=&#34;python-based-producers-and-consumers&#34;&gt;Python-Based Producers and Consumers&lt;/h2&gt;
&lt;p&gt;Our Apache Kafka service is now accessible to the external world, and we&amp;rsquo;ve successfully conducted basic tests to ensure its functionality. The next step is to access Kafka programmatically, specifically using 
  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; height=&#34;1em&#34; viewBox=&#34;0 0 448 512&#34; fill=&#34;currentColor&#34;&gt;&lt;path d=&#34;M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; Python. We&amp;rsquo;ll utilize the Databricks Community Edition for this purpose, just as we have done in previous examples. However, it&amp;rsquo;s important to note that this approach is not limited to Databricksâ€”you should be able to replicate these steps on other platforms (Google Colab, Amazon Sagemarker, Deepnote), ensuring flexibility and adaptability in various environments.&lt;/p&gt;
&lt;h3 id=&#34;reading-and-writing-frominto-a-specific-topic&#34;&gt;Reading and Writing from/into a specific topic&lt;/h3&gt;
&lt;p&gt;To spin up a cluster with the default configuration, follow these steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spin Up the Cluster&lt;/strong&gt;: Launch a cluster using your platform&amp;rsquo;s default settings. Ensure it meets the requirements for your tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create a Folder in Databricks&lt;/strong&gt;: In your Databricks environment, navigate to the workspace and create a new folder named &amp;ldquo;Mod 5&amp;rdquo;. This will be the designated space for all the resources and scripts related to Module 5.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These steps will prepare your environment for further development and organization as you proceed with the tasks in Module 5.&lt;/p&gt;
&lt;p&gt;Import 
, rename it to &lt;code&gt;Producer&lt;/code&gt;, and follow the instructions.&lt;/p&gt;
&lt;p&gt;In a new window in the same folder import 
, rename it to &lt;code&gt;Consumer&lt;/code&gt;, and follow the instructions.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/kafka-db-cluster.png&#34; alt=&#34;Kafka&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Explain what is going on here. Why the Consumer has an entry (The bottom right screen of the image bellow)
















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/kafka-4-screens-3.png&#34; alt=&#34;4 shell for 4 services&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;kafka-producer-expects-json-serializable-data&#34;&gt;Kafka Producer Expects JSON-Serializable Data:&lt;/h3&gt;
&lt;p&gt;The Kafka producer takes as input JSON-serializable data, which can lead to errors if the provided data cannot be serialized properly. Below you will see a few examples of data type that the Kafka producer expects, along with explanations on how to correctly format and serialize this data to avoid common errors.&lt;/p&gt;
&lt;h4 id=&#34;sending-an-integer-value&#34;&gt;Sending an integer value&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;producer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;send&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;buas-data-n-ai-events&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;or-sending-type-information-as-a-string&#34;&gt;Or sending type information as a string&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;producer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;send&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;buas-data-n-ai-events&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;or-sending-a-json-object-with-type-information-and-value&#34;&gt;Or sending a JSON object with type information and value&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;producer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;send&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;buas-data-n-ai-events&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)),&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;create-a-topic&#34;&gt;Create a topic&lt;/h3&gt;
&lt;p&gt;Import 
 tot he same folder, rename it to &lt;code&gt;Topic Management and Verification in Databricks&lt;/code&gt;, and follow the instructions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 5.1</title>
      <link>https://dataqubed.io/teaching/pyspark-spark-streaming-part1/</link>
      <pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-spark-streaming-part1/</guid>
      <description>&lt;h2 id=&#34;deploying-apache-kafka-on-a-single-node&#34;&gt;Deploying Apache Kafka on a Single Node&lt;/h2&gt;
&lt;p&gt;Deploying Kafka on a single node is a great way to get started with understanding its core functionalities. This setup involves configuring a Kafka broker along with Zookeeper, which manages the broker&amp;rsquo;s metadata. In this guide, you&amp;rsquo;ll learn how to install, configure, and run Apache Kafka on a single node, making it accessible for testing and small-scale applications. This foundational setup will prepare you for more complex, distributed deployments.&lt;/p&gt;
&lt;h2 id=&#34;apache-kafka&#34;&gt;Apache Kafka&lt;/h2&gt;
&lt;p&gt;Apache Kafka is an open-source software streaming solution designed for handling real-time data feeds. It ensures events are stored in a durable, fault-tolerant manner, making it reliable for critical applications. Kafka enables the seamless streaming of messages to and from various endpoints, including databases, cloud services, and analytics platforms.&lt;/p&gt;
&lt;h3 id=&#34;key-features&#34;&gt;Key Features:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Durability&lt;/strong&gt;: Events are stored reliably for future use.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fault Tolerance&lt;/strong&gt;: Designed to handle failures without data loss.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Can manage large volumes of data across distributed systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/Kafka1.png&#34; alt=&#34;Kafka&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;For more insights, check out the YouTube video below.


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/06iRM1Ghr1k?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/p&gt;
&lt;h3 id=&#34;setting-up-apache-kafka-on-vultr&#34;&gt;Setting Up Apache Kafka on Vultr&lt;/h3&gt;
&lt;p&gt;To set up Apache Kafka on a Vultr instance, follow these steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deploy a Vultr Server&lt;/strong&gt;: Choose a suitable server size and region, then deploy a Linux instance (preferably Ubuntu).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install Java&lt;/strong&gt;: Kafka requires Java to run, so install the latest version of OpenJDK.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Download and Install Kafka&lt;/strong&gt;: Download Kafka from the official Apache Kafka website and extract the files to your desired directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Configure Kafka&lt;/strong&gt;: Modify the server properties (&lt;code&gt;server.properties&lt;/code&gt;) to match your setup. Set the broker ID, log directories, and Zookeeper address.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Start Kafka and Zookeeper&lt;/strong&gt;: Start the Zookeeper service first, followed by the Kafka server.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Test Kafka&lt;/strong&gt;: Use the provided Kafka tools to create topics, send messages, and consume them to ensure everything is working.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Before diving into the technical setup, it&amp;rsquo;s essential to understand why Vultr is a great choice for deploying Kafka.&lt;/p&gt;
&lt;h3 id=&#34;why-vultr&#34;&gt;Why Vultr?&lt;/h3&gt;
&lt;p&gt;Vultr offers high-performance cloud instances with SSD storage, low latency, and a global network of data centers, making it ideal for running distributed systems like Kafka. Additionally, Vultr&amp;rsquo;s flexible pricing, easy scalability, and user-friendly interface allow you to quickly deploy and manage your Kafka infrastructure, ensuring you have the resources you need as your data processing demands grow.&lt;/p&gt;
&lt;p&gt;While Vultr is an excellent choice for deploying Kafka, other cloud platforms like AWS, Google Cloud, and Azure also offer robust environments for running Kafka. These platforms provide managed services, such as Amazon MSK (Managed Streaming for Apache Kafka), that simplify Kafka deployment and management. Each platform has its strengths, such as extensive integration options, advanced networking, and security features. The choice ultimately depends on your specific needs, budget, and existing cloud infrastructure.&lt;/p&gt;
&lt;p&gt;In these series we will go ahead and implement a cost-effective Kafka deployment on Vultr. As your needs grow, Vultr&amp;rsquo;s easy scalability allows you to upgrade your instance or add more servers to your cluster. Additionally, using Vultr&amp;rsquo;s block storage for Kafka&amp;rsquo;s logs can provide both cost savings and performance improvements. Always monitor your resource usage to optimize costs effectively.&lt;/p&gt;
&lt;h3 id=&#34;a-cost-efficient-solution-on-vultr&#34;&gt;A Cost-Efficient Solution on Vultr&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Vultr VPS&lt;/strong&gt;: Start with a Vultr Virtual Private Server.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shared CPU&lt;/strong&gt;: Opt for a shared CPU plan to keep costs low.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Location&lt;/strong&gt;: Choose a data center location close to your target users for better latency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Operating System&lt;/strong&gt;: Select Ubuntu 22.04 for its up-to-date features and stability. (In case you tried a another version of Ubuntu and followed the same process I would like to know if it worked or not!)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instance Type&lt;/strong&gt;: Choose the &amp;ldquo;25 GB NVMe&amp;rdquo; AMD High-Performance machine without additional features like auto-backup, to minimize costs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSH Key&lt;/strong&gt;: Generate and upload an SSH key during VPS creation for secure and easy access.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This setup provides a balance of performance and cost, making it ideal for small to medium Kafka deployments.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/vultr-choose-plan.png&#34; alt=&#34;Kafka&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/vultr-additional-features.png&#34; alt=&#34;Kafka&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/vultr-deploy.png&#34; alt=&#34;Kafka&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/vultr-ubuntu-image-deployd.png&#34; alt=&#34;Kafka&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;For the installation process, we&amp;rsquo;ll largely follow the guidelines provided in the 
. This includes setting up Kafka with ZooKeeper, ensuring all services are started in the correct order, and making necessary configurations as needed.&lt;/p&gt;
&lt;h3 id=&#34;step-by-step-installation-guide&#34;&gt;Step-by-Step Installation Guide&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Start by SSHing into a machine:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ssh root@45.63.34.218
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;install Java&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;apt install openjdk-8-jre-headless
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;java -version
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Download Kafka version 2.13, unzip it and cd into the directory&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Start the ZooKeeper service&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;wget https://archive.apache.org/dist/kafka/2.6.0/kafka_2.13-2.6.0.tgz
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;tar -xzf kafka_2.13-2.6.0.tgz
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; kafka_2.13-2.6.0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;We next need to start a service called zookeeper. zookeeper is&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/zookeeper-server-start.sh config/zookeeper.properties
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Start the Kafka broker service. For that we need to open another terminal session and run:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-server-start.sh config/server.properties
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Paraphraze: Once all services have successfully launched, you will have a basic Kafka environment running and ready to use.&lt;/p&gt;
&lt;h3 id=&#34;create-a-topic&#34;&gt;Create a topic&lt;/h3&gt;
&lt;p&gt;Ww already know well that lets one read, write, store, and process events (also called records or messages in the documentation) across many machines. The events are organized and stored in topics. You may consider topics are like folder in a filesystem, and the events are the files in that folder.&lt;/p&gt;
&lt;p&gt;So before you can write your first events, you must create a topic. Open another terminal session and run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-topics.sh --create --topic buas-data-n-ai-events --bootstrap-server localhost:9092
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;command breakdown:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;lsquo;bin/kafka-topics.sh&amp;rsquo;: The kafka-topics.sh script is an executable and is used to manage Kafka topics. The file is located in the bin directory of your Kafka installation.&lt;/li&gt;
&lt;li&gt;&amp;lsquo;&amp;ndash;create&amp;rsquo;: This flag tells Kafka to create a new topic.&lt;/li&gt;
&lt;li&gt;&amp;lsquo;&amp;ndash;topic buas-data-n-ai-events&amp;rsquo;: This specifies the name of the topic you want to create. In this case, the topic is named buas-data-n-ai-events.&lt;/li&gt;
&lt;li&gt;&amp;lsquo;&amp;ndash;bootstrap-server localhost:9092&amp;rsquo;: Specifies the &amp;lsquo;Kafka server&amp;rsquo; to connect to in order to create the topic. localhost:9092 refers to a Kafka broker running on your local machine on port 9092. So now the client (which is executing the command) would connect to the &amp;lsquo;Kafka broker&amp;rsquo; running on local machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reading-and-writing-frominto-a-specific-topic&#34;&gt;Reading and Writing from/into a specific topic&lt;/h3&gt;
&lt;p&gt;A Kafka client communicates with the Kafka brokers (via the network) for reading or writing events.&lt;/p&gt;
&lt;p&gt;To write events into the topic run the following line in a new shell&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-console-producer.sh --topic buas-data-n-ai-events --bootstrap-server localhost:9092
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;Spark machine is killed 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;Frank to the rescue 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;from another terminal read the events:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-console-consumer.sh --topic buas-data-n-ai-events --from-beginning --bootstrap-server localhost:9092
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Spark machine is killed
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Frank to the rescue
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/kafka-4-screens.png&#34; alt=&#34;4 shell for 4 services&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;make-the-kafka-broker-accessible-externally&#34;&gt;Make the Kafka broker accessible externally&lt;/h3&gt;
&lt;p&gt;By now, you should have four shell windows open. To stop the running processes, press Ctrl + C and shutdown the services in a reverse order, starting from the last shell. This ensures that any changes made to the Kafka configuration or environment are properly loaded and active. We will then use the first shell (the one that we run the ZooKeeper service) to configure the broker to be accessible externally.&lt;/p&gt;
&lt;p&gt;To make the Kafka broker accessible externally we first need to edit the Kafka config file located at &amp;lsquo;config/server.properties&amp;rsquo;. ZooKeeper service by using the &amp;lsquo;Ctrl + C&amp;rsquo; and then edit the server.properties:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;vim config/server.properties
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;ðŸ‘‰ Click to learn how to use wim&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    Google to learn!
  &lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Navigate to line 36, uncomment and replace &amp;lsquo;your.host.name&amp;rsquo; with &amp;lsquo;your host name&amp;rsquo;: For my case it will be &amp;lsquo;advertised.listeners=PLAINTEXT://155.138.192.245:9092&amp;rsquo;&lt;/p&gt;
&lt;p&gt;Then we need to make port 9092 accessible externally.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ufw allow &lt;span class=&#34;m&#34;&gt;9092&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# And check the status &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ufw status verbose
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/kafka-4-screens-2.png&#34; alt=&#34;4 shell for 4 services&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Now activate the services in the same order as before,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 1 - Restart the ZooKeeper service again.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/zookeeper-server-start.sh config/zookeeper.properties
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 2 - Restart the kafka server again. &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-server-start.sh config/server.properties
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-console-producer.sh --topic buas-data-n-ai-events --bootstrap-server localhost:9092
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 4 - Restart the kafka server again. &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-console-consumer.sh --topic buas-data-n-ai-events --from-beginning --bootstrap-server localhost:9092
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/kafka-4-screens-3.png&#34; alt=&#34;4 shell for 4 services&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extra reading material:&lt;/strong&gt; Read through the 
. The article covers the main concepts of Kafka and compares it to other technologies. For a more detailed understanding of Kafka, refer to the official documentation.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;ðŸ‘‰ What is a Kafka server?&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    Kafka server
  &lt;/div&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;ðŸ‘‰ What is a Kafka broker?&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    Kafka broker
  &lt;/div&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;ðŸ‘‰ What is a Kafka client?&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    Kafka client
  &lt;/div&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;ðŸ‘‰ How to ensure stability and reliability of running Kafka in the background?&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    &lt;p&gt;Using &lt;code&gt;systemd&lt;/code&gt; (or another init system like init.d) to manage Kafka as a service is generally the most stable and robust option. &lt;code&gt;Systemd&lt;/code&gt; is designed to manage system services, ensuring that services like Kafka start automatically on boot, restart on failure, and remain running independently of any user session.&lt;/p&gt;
&lt;p&gt;What we did in this tutorial runs Kafka in the foreground within the SSH session. If you close the SSH connection (e.g., by exiting the terminal or using Ctrl+C), the Kafka process will be terminated.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;systemctl start kafka   &lt;span class=&#34;c1&#34;&gt;# Start Kafka Service&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;systemctl &lt;span class=&#34;nb&#34;&gt;enable&lt;/span&gt; kafka  &lt;span class=&#34;c1&#34;&gt;# Ensures Kafka starts on boot&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;systemctl status kafka  &lt;span class=&#34;c1&#34;&gt;# Check the status of Kafka&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To be able to use &lt;code&gt;systemd&lt;/code&gt; you first need to create a kafka service file and configure it. Then you need to reload systemd daemon to recognize the new service.&lt;/p&gt;
&lt;p&gt;To create a systemd service file for Kafka use vim and place the file at &lt;code&gt;/etc/systemd/system/kafka.service&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo vim /etc/systemd/system/kafka.service
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the file, add the following content. Make sure to adjust paths to match the Kafka installation directory:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Unit&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;Apache Kafka Server
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Documentation&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;https://kafka.apache.org/documentation/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Requires&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;zookeeper.service
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;After&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;zookeeper.service
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Service&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;User&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kafka
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Group&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kafka
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;ExecStart&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/root/kafka_2.13-2.6.0/bin/kafka-server-start.sh /root/kafka_2.13-2.6.0/config/server.properties
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;ExecStop&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/root/kafka_2.13-2.6.0/bin/kafka-server-stop.sh
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Restart&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;on-failure
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;RestartSec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;TimeoutSec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;180&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Install&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;WantedBy&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;multi-user.target
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then reload systemd daemon before starting kafka service:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl daemon-reload
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We need to create a Zookeeper service file just like what we did for kafka server.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;wim /etc/systemd/system/zookeeper.service
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Unit&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;Apache Zookeeper Service
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Documentation&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;https://zookeeper.apache.org/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;After&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;network.target
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Service&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Type&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;simple
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;User&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;root
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Group&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;root
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;ExecStart&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/root/kafka_2.13-2.6.0/bin/zookeeper-server-start.sh /root/kafka_2.13-2.6.0/config/zookeeper.properties
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;ExecStop&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/root/kafka_2.13-2.6.0/bin/zookeeper-server-stop.sh
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Restart&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;on-failure
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;RestartSec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Install&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;WantedBy&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;multi-user.target
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl daemon-reload
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl start zookeeper
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl &lt;span class=&#34;nb&#34;&gt;enable&lt;/span&gt; zookeeper
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Congratulations!
You have successfully finished the Apache Kafka installation. Now, your Kafka broker is up and running, ready to handle real-time data streams. This marks a significant milestone in setting up your data processing pipeline. Next, you can start creating topics, producing and consuming messages, and exploring Kafkaâ€™s powerful features to build scalable and resilient data systems. If you encounter any issues or need to expand your setup, consult the Kafka documentation or explore advanced configurations to optimize your deployment.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 5.0</title>
      <link>https://dataqubed.io/teaching/pyspark-spark-streaming-part0/</link>
      <pubDate>Mon, 19 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-spark-streaming-part0/</guid>
      <description>&lt;h2 id=&#34;spark-streaming&#34;&gt;Spark Streaming&lt;/h2&gt;
&lt;p&gt;So far, we&amp;rsquo;ve covered the fundamentals of PySpark:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Big Data &amp;amp; Apache Spark:&lt;/strong&gt; Introduction to handling large datasets using Spark.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spark Architecture &amp;amp; RDDs:&lt;/strong&gt; Understanding Spark&amp;rsquo;s core structure and Resilient Distributed Datasets (RDDs) for parallel processing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DataFrames &amp;amp; Spark SQL:&lt;/strong&gt; Using DataFrames for structured data and Spark SQL for querying.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Sources &amp;amp; Sinks:&lt;/strong&gt; Connecting to and saving data across various formats.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These topics form the backbone of using PySpark for efficient, large-scale data processing.&lt;/p&gt;
&lt;p&gt;In this module, we will jump into real-time data processing and message brokering. We&amp;rsquo;ll explore streaming in Apache Spark, focusing on how to handle continuous data flows for real-time analytics. Additionally, we&amp;rsquo;ll cover Apache Kafka, a distributed messaging system that plays a crucial role in building robust data pipelines, enabling efficient data ingestion and real-time processing across various systems. These topics will help us manage and process data &lt;code&gt;in motion&lt;/code&gt;, a key aspect of modern data stack.&lt;/p&gt;
&lt;h3 id=&#34;why-stream-processing&#34;&gt;Why stream processing?&lt;/h3&gt;
&lt;p&gt;Stream processing is a powerful technique that enables real-time data handling and analytics. At its core, stream processing exploits &lt;code&gt;parallelism&lt;/code&gt;, allowing multiple data streams to be processed simultaneously. This reduces the need for &lt;code&gt;synchronization between components&lt;/code&gt;, making systems more efficient and scalable. As data is constantly in motion across cloud systems, stream processing ensures that insights can be derived in real-time, whether itâ€™s for &lt;code&gt;real-time recommendations&lt;/code&gt;, &lt;code&gt;predictive maintenance&lt;/code&gt;, or other applications.&lt;/p&gt;
&lt;p&gt;However, stream processing isnâ€™t without its challenges. Building &lt;code&gt;fault tolerance&lt;/code&gt; and &lt;code&gt;buffering mechanisms&lt;/code&gt; into software is crucial to ensure data reliability and smooth operations. Apache Kafka stands out as a leading platform for stream processing, offering robust capabilities for handling real-time data pipelines. Alongside Kafka, other solutions like &lt;code&gt;Apache Flume&lt;/code&gt;, &lt;code&gt;Apache Apex&lt;/code&gt;, and &lt;code&gt;Apache Storm&lt;/code&gt; provide diverse tools to tackle various streaming needs.&lt;/p&gt;
&lt;h3 id=&#34;the-dance-of-data-producers-and-consumers&#34;&gt;The Dance of Data: Producers and Consumers&lt;/h3&gt;
&lt;p&gt;In the world of stream processing, data flows like a continuous river, with &lt;code&gt;producers&lt;/code&gt; generating messages and &lt;code&gt;consumers&lt;/code&gt; retrieving them. Producers are the origin points, creating and sending data to various systems. On the other end, consumers actively listen, process, and utilize this data to drive decisions and actions. This dynamic interaction between producers and consumers is the heartbeat of real-time systems, enabling everything from real-time analytics to instantaneous user experiences.&lt;/p&gt;
&lt;h3 id=&#34;milestones-on-our-streaming-journey&#34;&gt;Milestones on Our Streaming Journey&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deploying Apache Kafka on a Single Node&lt;/strong&gt; (Module 5.1)&lt;br&gt;
Learn how to set up a Kafka streaming platform, the backbone of our real-time data processing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Creating Python-Based Producers and Consumers&lt;/strong&gt; (Module 5.2)&lt;br&gt;
Implement Python processes to produce and consume messages, simulating real-world data flows.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integrating PySpark Streaming in Databricks&lt;/strong&gt; (Module 5.3)&lt;br&gt;
Set up and run PySpark streaming tasks within Databricks, processing and analyzing streaming data in real time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div style=&#34;border:1px solid black; padding:10px;&#34;&gt;
  &lt;mark&gt;Note:&lt;/mark&gt; You can skip `Module 5.1` and `Module 5.2` and jump straight to `Module 5.3` if you desire to skip installing and configuring a Kafka service, though it is not recommended at all. 
&lt;/div&gt;
&lt;p&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 4</title>
      <link>https://dataqubed.io/teaching/pyspark-data-sources-and-sinks/</link>
      <pubDate>Sun, 18 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-data-sources-and-sinks/</guid>
      <description>&lt;h2 id=&#34;data-sources-and-sinks&#34;&gt;Data Sources and Sinks&lt;/h2&gt;
&lt;p&gt;By now, you should have a foundational understanding of Spark and its key concepts, including RDDs and lazy evaluation. We also learned how to use Databricks PySpark for creating and manipulating Spark DataFrames, and querying them with Spark SQL. We performed various DataFrame operations like selecting, filtering, and joining data. Additionally, we explored ways to handle semi-structured data.&lt;/p&gt;
&lt;p&gt;In this module we will study how to read and write from/into CSV, JSON, Parquet, and other formats. We will then continue by making jdbc connection for reading from / writing to databases.&lt;/p&gt;
&lt;h2 id=&#34;reading-and-writing-frominto-csv-json-parquet-and-other-formats&#34;&gt;Reading and writing from/into CSV, JSON, Parquet, and other formats&lt;/h2&gt;
&lt;p&gt;First watch this video bellow.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/ZBI1UFbEM4c?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Now clone 
 into your workspace. Lets put it into a new folder, lets say &lt;strong&gt;Module 4&lt;/strong&gt;  and rename it into &lt;strong&gt;01 - Reading and writing from-into&lt;/strong&gt; for consistency reasons. Follow the steps and cells. The material should be self explanatory.&lt;/p&gt;
&lt;h2 id=&#34;reading-form-and-writing-into-databases-using-jdbc&#34;&gt;Reading form and Writing into databases using JDBC&lt;/h2&gt;
&lt;p&gt;Watch the Youtube video bellow.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/oLIagnUAN2E?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Now clone 
 into your workspace. Rename the file it into &lt;strong&gt;02 - JDBC connector&lt;/strong&gt; for consistency reasons. Follow the steps and cells. The material should be self explanatory.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 3</title>
      <link>https://dataqubed.io/teaching/pyspark-dataframes-and-spark-sql/</link>
      <pubDate>Sat, 17 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-dataframes-and-spark-sql/</guid>
      <description>&lt;h2 id=&#34;module-3-dataframes-and-spark-sql-in-databricks&#34;&gt;Module 3: DataFrames and Spark SQL (in Databricks)&lt;/h2&gt;
&lt;p&gt;By now you have some understating of Spark and distributed computing. You know the RDDs, the importance of lazy evaluations as well as the reason why RDDs are fault tolerant.&lt;/p&gt;
&lt;p&gt;You also gained a bit of experience on how to install Spark on your local machine, how to test it and did a bit of coding in PySpark.&lt;/p&gt;
&lt;p&gt;Today we will learn a bit on DataBricks, what does it have to offer to us and how we can open a community edition of DataBricks. Next we wil run few notebooks and will make Spark Dataframes and will query them using Spark SQL.&lt;/p&gt;
&lt;h3 id=&#34;learning-outcomes&#34;&gt;Learning Outcomes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Understanding DataBricks:&lt;/strong&gt; Gain an overview of what DataBricks is, including its features and benefits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Accessing DataBricks Community Edition:&lt;/strong&gt; Learn how to access and set up the Community Edition of DataBricks for hands-on practice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Working with Notebooks in DataBricks:&lt;/strong&gt; Learn how to create and run notebooks in DataBricks, which are essential for executing code and managing workflows.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Creating Spark DataFrames:&lt;/strong&gt; Understand how to create Spark DataFrames, a core component of working with data in Spark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Querying Data with Spark SQL:&lt;/strong&gt; Learn how to query data using Spark SQL, leveraging SQL-like syntax for data manipulation within Spark DataFrames.&lt;/p&gt;
&lt;h3 id=&#34;why-databricks&#34;&gt;Why DataBricks?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cloud-native: works on any major cloud provider&lt;/li&gt;
&lt;li&gt;Data storage: store a broad range of data including structured, unstructured and streaming&lt;/li&gt;
&lt;li&gt;Governance and management: in-built security controls and governance&lt;/li&gt;
&lt;li&gt;Tools: wide range of production-ready data tooling from engineering to BI, AI and ML&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/whydb.png&#34; alt=&#34;Why DataBricks&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;With Databricks Community Edition, we would like to avoid installing new software and maintaining it. Also it gives us a ton of flexibility and features that we will see in the next sections.&lt;/p&gt;
&lt;p&gt;A non comprehensive list of benefits of DataBricks:&lt;/p&gt;
&lt;h4 id=&#34;1-scalability-and-performance&#34;&gt;1. Scalability and Performance&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Built on Apache Spark for efficient big data processing&lt;/li&gt;
&lt;li&gt;Seamlessly scales from small to large clusters&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-unified-analytics-platform&#34;&gt;2. Unified Analytics Platform&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Integrates data engineering, data science, and machine learning&lt;/li&gt;
&lt;li&gt;Simplifies workflows and enhances team collaboration&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;3-ease-of-use&#34;&gt;3. Ease of Use&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Interactive workspace with collaborative notebooks&lt;/li&gt;
&lt;li&gt;Supports multiple languages: Python, SQL, R, Scala&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;4-advanced-analytics-and-machine-learning&#34;&gt;4. Advanced Analytics and Machine Learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Robust support for machine learning and AI&lt;/li&gt;
&lt;li&gt;Includes MLflow for experiment tracking and model management&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;5-data-lake-integration&#34;&gt;5. Data Lake Integration&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Supports Delta Lake for reliable data lakes&lt;/li&gt;
&lt;li&gt;Ensures ACID transactions and unifies batch and streaming data processing&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;6-cloud-integration&#34;&gt;6. Cloud Integration&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Seamless integration with AWS, Azure, and Google Cloud&lt;/li&gt;
&lt;li&gt;Leverages cloud-native features for security and cost efficiency&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;7-real-time-data-processing&#34;&gt;7. Real-Time Data Processing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Handles real-time data streams for timely insights&lt;/li&gt;
&lt;li&gt;Essential for real-time analytics, monitoring, and fraud detection&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;8-security-and-compliance&#34;&gt;8. Security and Compliance&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Robust security features including data encryption and role-based access control&lt;/li&gt;
&lt;li&gt;Compliance with various industry standards&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;9-cost-efficiency&#34;&gt;9. Cost Efficiency&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Auto-scaling and optimized cluster management&lt;/li&gt;
&lt;li&gt;Efficient resource management reduces costs&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;10-community-and-ecosystem&#34;&gt;10. Community and Ecosystem&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Large and active community with strong support&lt;/li&gt;
&lt;li&gt;Extensive documentation and third-party integrations&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dataframes-and-spark-sql&#34;&gt;DataFrames and Spark SQL&lt;/h3&gt;
&lt;p&gt;Now that we are aware of some of the benefits of running spark on DataBricks env lets go ahead and open our first Databricks Community Edition account. To learn how to create your Databricks Community Edition account and activate it see section &lt;strong&gt;0. Prep&lt;/strong&gt; bellow.&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prep&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before we dive into the detailed process of learning PySpark in the Databricks environment, I&amp;rsquo;d like you to watch this introductory video. It will provide you with a solid overview of what to expect and help you navigate the material more easily. Don&amp;rsquo;t worry about mastering everything in the video or practicing the content; it&amp;rsquo;s just a warm-up.&lt;/li&gt;
&lt;/ul&gt;

   
       
       &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
         &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/lcI1W2_KUPo?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
         &gt;&lt;/iframe&gt;
       &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;[Introduction to Databricks notebooks] (
)&lt;/li&gt;
&lt;li&gt;[Manage notebooks] (
)&lt;/li&gt;
&lt;li&gt;[Develop code in Databricks notebooks] (
)&lt;/li&gt;
&lt;li&gt;[Databricks notebook interface and controls] (
)&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
, and see the bellow youtube.&lt;/li&gt;
&lt;/ul&gt;

   
       
       &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
         &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/Op_KZJm7_qc?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
         &gt;&lt;/iframe&gt;
       &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Now you know the essential procedures to work in DataBricks environment. Now head towards the 
 notebook, clone it into your workspace and follow the steps. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;00 - Test the dev env&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Now clone 
 into your workspace. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;01 - Data Prep&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: As you already know a good practice in programming is to modularize your code. Following the best practices here I have created a notebook called 
. Clone that into you workspace, rename it to &lt;strong&gt;02 - Functions&lt;/strong&gt;, try to read through it, and make adjustments as needed.&lt;/p&gt;
&lt;p&gt;Next, we will explore the fundamentals of DataFrames in Databricks. Weâ€™ll cover how to create and manipulate DataFrames, perform basic operations, and leverage DataFrames for data analysis. This session will guide you through using Spark SQL for querying and transforming data, demonstrating practical examples of DataFrame operations.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Introduction to DataFrames&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Clone 
. Rename the file to &lt;strong&gt;03 - Introduction to DataFrames&lt;/strong&gt;. Go through the material and try to make notes. The notebook covers few topics including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Difference between RDDs and DataFrames&lt;/li&gt;
&lt;li&gt;Creating DataFrames&lt;/li&gt;
&lt;li&gt;Schema inference and manual schema definition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Next we learned how to perform basic DataFrame operations including selecting, filtering, and transforming data using PySpark. These operations are fundamental for data manipulation and analysis. We will then learn how to perform aggregations and group data using PySpark. We covered how to group by single and multiple columns, apply multiple aggregate functions, and filter results after grouping.&lt;/p&gt;
&lt;p&gt;We will then dive into working with joins and unions in Spark using Databricks. We&amp;rsquo;ll explore how to combine data from multiple DataFrames using different types of joins, including inner, left, right, and full outer joins. Additionally, we&amp;rsquo;ll cover how to use unions to append data from one DataFrame to another. These operations are fundamental for integrating and manipulating datasets in Spark, making them essential skills for any data engineer or analyst working in a distributed data environment.&lt;/p&gt;
&lt;p&gt;Finally, in this module, we will learn few tips and tricks on how to work with semi-structured data. The focus is on handling JSON data, including how to read, write, and manipulate JSON files using PySpark. You will learn techniques to efficiently process semi-structured data, including using Spark&amp;rsquo;s &lt;code&gt;explode&lt;/code&gt; function to flatten nested data and leveraging &lt;code&gt;schema inference&lt;/code&gt; to automatically detect data structures. The module provides practical examples and exercises to solidify your understanding, ensuring you can effectively manage complex data formats in your projects.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DataFrame Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Selecting, filtering, and transforming data: Follow 
. Referencing rename the file to &lt;strong&gt;04 - DataFrame Operations&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Aggregations and Grouping: Follow 
. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;05 - Aggregations and Grouping&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Joins and unions: Follow 
. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;06 - Joins and Unions&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Working with Semi-Structured Data: Follow 
. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;07 - Working with semi-structured data&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spark SQL&lt;/strong&gt; (todo: coming soon)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SQL queries with Spark&lt;/li&gt;
&lt;li&gt;Registering DataFrames as tables&lt;/li&gt;
&lt;li&gt;Using SQL queries to manipulate DataFrames&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;&lt;p&gt;&lt;strong&gt;Assignment: Visualizations in Databricks notebooks&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Databricks has built-in support for charts and visualizations in both Databricks SQL and in notebooks. Based on the data prepared in &lt;strong&gt;01 - Data Prep&lt;/strong&gt; make few visualizations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details:&lt;/strong&gt; I will provide you with a git repo and you will be tasked place your notebook there. The repo will not accept new entries after the deadline. Please deliver both the Databricks &lt;strong&gt;.dbc&lt;/strong&gt; archive file format and &lt;strong&gt;.HTML&lt;/strong&gt; format. Make sure to give them identical filenames. Also make sure to put the name of the team and team members in the first cell of the notebook.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deadline:&lt;/strong&gt; Your notebook must be submitted by Sep 29 at 23:59 CET. Please adhere strictly to this deadline.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reading material&lt;/strong&gt;: Need some reading materials? Visit 
.&lt;/p&gt;
&lt;/span&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 2</title>
      <link>https://dataqubed.io/teaching/pyspark-spark-architecture-and-rdds/</link>
      <pubDate>Fri, 16 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-spark-architecture-and-rdds/</guid>
      <description>&lt;h2 id=&#34;spark-architecture-and-rdds&#34;&gt;Spark Architecture and RDDs&lt;/h2&gt;
&lt;h3 id=&#34;a-gentle-intro-to-spark-architecture&#34;&gt;A gentle intro to Spark Architecture&lt;/h3&gt;
&lt;p&gt;Look at the Youtube video bellow carefully.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/vJ0eUZxF80s?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;the above video explores and explains the architecture of Apache Spark, emphasizing the roles of the &lt;em&gt;driver&lt;/em&gt; and &lt;em&gt;executors&lt;/em&gt; in both &lt;em&gt;local&lt;/em&gt; and &lt;em&gt;cluster&lt;/em&gt; mode applications. The driver manages the application and can run on the local machine or within a cluster. In client mode, the driver operates locally, while in cluster mode, it starts in the Application Master container, requesting containers from the cluster manager (like Apache YARN) to launch executors. The video explains how Spark can operate in local mode, where everything runs in a single JVM, or in cluster mode, where the driver and executors communicate via the Application Master, with resource allocation handled by the cluster manager. A demonstration shows starting a Spark application in cluster mode with specific executors and utilizing YARN&amp;rsquo;s dynamic allocation feature to release idle executors.&lt;/p&gt;
&lt;p&gt;The video bellow covers how Spark breaks down and executes applications at the executors. It covers the internal workings of Apache Spark, focusing on how it processes code in parallel. The video covers how Spark reads data, processes it, and writes results back to a destination. Three key data structures in Spark namely &lt;em&gt;DataFrames&lt;/em&gt;, &lt;em&gt;Datasets&lt;/em&gt;, and &lt;em&gt;RDDs&lt;/em&gt; (Resilient Distributed Datasets) are presented. Although the emphasis is on DataFrames and Datasets, RDDs are crucial as they form the underlying structure to which DataFrames and Datasets are compiled. RDDs are &lt;strong&gt;resilient&lt;/strong&gt;, &lt;strong&gt;partitioned&lt;/strong&gt;, &lt;strong&gt;distributed&lt;/strong&gt;, and &lt;strong&gt;immutable collections&lt;/strong&gt; of data that can be created from a source or transformed from another RDD.&lt;/p&gt;
&lt;p&gt;The video demonstrates how to create an RDD from a file and control the number of partitions. It explains the difference between transformations and actions in Sparkâ€”transformations are lazy operations that create new distributed datasets without sending results back to the driver, while actions are non-lazy and send results back to the driver. It also touches on topics such as Spark&amp;rsquo;s way of handling grouping and counting operations on distributed data, which involves repartitioning the data and triggering shuffle and sort activities. The number of parallel tasks in Spark is influenced by the number of partitions and available executors. Concept of functional programming in Spark is briefly touched on as well.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/fyTiJLKEzME?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;rdds&#34;&gt;RDDs&lt;/h3&gt;
&lt;p&gt;We have seen the RDDs and their properties. If you need a refresher go ahead and see the Youtube video bellow.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/nH6C9vqtyYU?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;As we have seen RDDS are closely linked to the concepts of Lazy evaluation and resilience. When you are done download &lt;strong&gt;01-Creating-RDDs.ipynb&lt;/strong&gt; from 
 and follow the instruction and run it locally. Make sure you could run all the cells. The notebook should have sufficient comments in it and that should provide you with some hints about the code.&lt;/p&gt;
&lt;p&gt;The html version of the notebook is available here 
.&lt;/p&gt;
&lt;p&gt;Afterwards download &lt;strong&gt;02-RDD-Lineage-n-Persistence.ipynb&lt;/strong&gt; from 
 and run it locally. The html version of the notebook is available here 
 for immediate access.&lt;/p&gt;
&lt;h3 id=&#34;transformations-and-actions-in-spark&#34;&gt;Transformations and Actions in spark&lt;/h3&gt;
&lt;p&gt;Now continue watching the Youtube video bellow entitled &amp;ldquo;Spark RDD Transformations and Actions&amp;rdquo;. The video should give you a sample of various Transformations and Actions in spark.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/5L0oyrwiNNE?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Download &lt;strong&gt;03-Transformations-n-Actions.ipynb&lt;/strong&gt; from 
 and run it locally. The html version of the notebook is available here 
 for immediate access.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 1</title>
      <link>https://dataqubed.io/teaching/pyspark-introduction-to-big-data-and-apache-spark/</link>
      <pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-introduction-to-big-data-and-apache-spark/</guid>
      <description>&lt;h2 id=&#34;introduction-to-big-data-and-apache-spark&#34;&gt;Introduction to Big Data and Apache Spark&lt;/h2&gt;
&lt;p&gt;In today&amp;rsquo;s data-driven world, organizations generate and process vast amounts of data, often referred to as &amp;ldquo;Big Data&amp;rdquo;. In some literature Big Data is characterized by its large volume, high velocity, and wide variety, making traditional data processing tools inadequate for handling such complex datasets. To address these challenges, specialized tools and frameworks have been developed.&lt;/p&gt;
&lt;p&gt;One of the most powerful tools for processing Big Data is Apache Spark, an open-source, distributed computing system that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Spark is widely used in big data processing due to its ability to perform in-memory computations, which significantly speeds up processing times compared to traditional disk-based methods.&lt;/p&gt;
&lt;p&gt;Apache Spark supports a variety of data processing tasks, including batch processing, stream processing, and machine learning, making it a versatile choice for various big data applications. For Python developers, PySpark offers a powerful and intuitive API to leverage Spark&amp;rsquo;s capabilities. PySpark allows developers to write Python code that seamlessly integrates with Spark&amp;rsquo;s distributed computing framework, enabling efficient processing of large datasets across clusters.&lt;/p&gt;
&lt;p&gt;Before diving deeper into the concepts of PySpark and distributed computing, it&amp;rsquo;s crucial to first understand the fundamentals of Big Data and why specialized tools like Apache Spark are essential for handling it effectively.&lt;/p&gt;
&lt;h2 id=&#34;what-is-big-data&#34;&gt;What is Big Data?&lt;/h2&gt;
&lt;p&gt;See the video bellow:&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/bAyrObl7TYE?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;and also the one bellow:&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/WEqVfo6nJuU?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;We discussed the five key categories of big data (&lt;strong&gt;Volume&lt;/strong&gt;, &lt;strong&gt;Velocity&lt;/strong&gt;, &lt;strong&gt;Variety&lt;/strong&gt;, &lt;strong&gt;Veracity&lt;/strong&gt;, and &lt;strong&gt;Value&lt;/strong&gt;) and provided some examples of how big data can be used in different ways. We also understood that big data is transforming industries across the board. Whether it&amp;rsquo;s improving patient care in healthcare, optimizing supply chains in retail, or enhancing fraud detection in banking, the impact of big data is undeniable.&lt;/p&gt;
&lt;p&gt;Next we will continue to understand the basics of Spark and Spark ecosystem.&lt;/p&gt;
&lt;h2 id=&#34;what-is-apache-spark&#34;&gt;What is Apache Spark?&lt;/h2&gt;
&lt;p&gt;Before diving into this tutorial, I recommend watching the bellow YouTube video entitles &amp;ldquo;An Introduction to PySpark&amp;rdquo;. This video serves as an excellent introduction to the world of PySpark and distributed computing. Itâ€™s a compact guide that walks you through the basics of Apache Spark, comparing it with Pandas, positioning it in the distributed computing ecosystem, and how PySpark fits into the picture.&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t panic if you donâ€™t grasp all the details right away, thatâ€™s perfectly okay! The goal is to give you a general sense of what Spark is, why itâ€™s important, and how it can be used to process large datasets efficiently. This foundational knowledge will make the hands-on sections of this tutorial much more meaningful as you start applying what youâ€™ve learned.&lt;/p&gt;
&lt;p&gt;So, take some time to watch the videoâ€”absorb as much as you canâ€”and then come back here to deepen your understanding. We will review a lot of this content again and again, not only in a different context but also from a various angels.&lt;/p&gt;
&lt;h3 id=&#34;a-short-summary-of-what-the-video-covers&#34;&gt;A short summary of what the video covers:&lt;/h3&gt;
&lt;p&gt;The talk covers what PySpark is, its capabilities compared to Pandas, and when itâ€™s necessary to use it. It highlights the benefits of PySpark for distributed data processing, explaining Spark&amp;rsquo;s map and reduce transformations and the advantages of lazy evaluation. The decision-making process for choosing between PySpark and other tools is also discussed, considering factors like data volume, existing codebases, team skills, and personal preferences. Throughout, code examples are provided, along with an emphasis on the usefulness of window functions and addressing concerns like the underlying Java in the Python library and potential errors during evaluation.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/ZFLOMSuWHxg?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Now that we have a high-level understanding of Big Data, its business value, and a basic understanding of Spark and PySparkâ€”along with how PySpark compares and contrasts with Pandasâ€”letâ€™s dive into some practical implementations. Weâ€™ll begin by installing Spark and PySpark locally on our machines.&lt;/p&gt;
&lt;h3 id=&#34;pyspark-installation&#34;&gt;PySpark installation&lt;/h3&gt;
&lt;p&gt;After you are done with the videos above, go ahead and install PySpark. 
 will give you the instruction on how to install PySpark on WSL but the process should remain almost the same when it comes to the installation on other OSs.&lt;/p&gt;
&lt;p&gt;For installing PySpark on your windows machine follow the instruction of the bellow Youtube video.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/WxhPDK4ffq4?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Visit here for a detailed instructions on how to install PySpark on different platforms. You will also learn how to configure your environment to work with PySpark effectively.&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Do you know a simpler and better instruction? Please share your experience.&lt;/span&gt;
&lt;/div&gt;
&lt;h3 id=&#34;check-the-installation&#34;&gt;Check the installation&lt;/h3&gt;
&lt;p&gt;Now lets check if your installation is complete. Download the file &lt;strong&gt;00-Check-your-spark-installation.ipynb&lt;/strong&gt; from 
 and follow the instruction. Make sure you could run all the cells. The notebook should have sufficient comments in it and that should provide you with some hints about the code.&lt;/p&gt;
&lt;p&gt;The html version of the notebook is available here 
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 0</title>
      <link>https://dataqubed.io/teaching/pyspark-fundamentals-and-advanced-topics/</link>
      <pubDate>Wed, 14 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-fundamentals-and-advanced-topics/</guid>
      <description>&lt;h3 id=&#34;pyspark-fundamentals-and-advanced-topics&#34;&gt;PySpark Fundamentals and Advanced Topics&lt;/h3&gt;
&lt;p&gt;Distributed computing is at the heart of all the recent advancements of Data science and AI models. We can now &lt;code&gt;scale out&lt;/code&gt; as an alternative to &lt;code&gt;scale up&lt;/code&gt;. And that makes it possible to retrieve data, prepare the data and train our models not only at a faster speed, but also cheaper.&lt;/p&gt;
&lt;p&gt;One of the pioneer commercial players on the filed of distributed computing is DataBricks. This the company behind the open source Spark community.&lt;/p&gt;
&lt;h4 id=&#34;course-objective&#34;&gt;Course Objective&lt;/h4&gt;
&lt;p&gt;In today&amp;rsquo;s data-driven world, the volume of data generated is growing exponentially. Traditional data processing systems struggle to handle this deluge efficiently. Distributed computing frameworks like Apache Spark have emerged as powerful tools for processing large datasets quickly and efficiently. Understanding Spark and PySpark (the Python API for Spark) is crucial for data professionals who want to leverage the full potential of big data. This course is designed to equip you with the skills to process and analyse large datasets using Apache Spark and PySpark, emphasizing its importance in modern data science and machine learning workflows. By the end of this course, you will have a good understanding of how to process, analyze, and manipulate large datasets efficiently. You will also gain hands-on experience in building machine learning models and optimizing Spark queries for performance.&lt;/p&gt;
&lt;p&gt;Over 30 hours of content is prepared that would help you develop a solid understanding of distributed computing, Spark&amp;rsquo;s architecture, and how to apply Spark for big data processing. The course includes a mix of theoretical concepts and hands-on exercises, leveraging free online resources.&lt;/p&gt;
&lt;h4 id=&#34;course-highlights&#34;&gt;Course Highlights&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fundamentals of PySpark&lt;/strong&gt;: Learn the basics of PySpark, including its architecture, Resilient Distributed Datasets (RDDs), and DataFrames.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Processing and SQL&lt;/strong&gt;: Master the use of DataFrames and Spark SQL for data manipulation and querying.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming and Real-Time Data&lt;/strong&gt;: Understand how to process real-time data using Spark Streaming.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machine Learning&lt;/strong&gt;: Explore machine learning techniques and build models using Spark&amp;rsquo;s MLlib (for year 2025 - 2026).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimization Techniques&lt;/strong&gt;: Learn how to optimize Spark applications for better performance and efficiency (for year 2025 - 2026).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capstone Project&lt;/strong&gt;: Apply your knowledge to a real-world project, demonstrating your ability to handle big data problems from start to finish (for year 2025 - 2026).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;learning-outcomes&#34;&gt;Learning Outcomes&lt;/h4&gt;
&lt;p&gt;By the end of this course, you will be able to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Understand and explain the architecture and components of Apache Spark.&lt;/li&gt;
&lt;li&gt;Install and configure PySpark on a Linux environment (WSL).&lt;/li&gt;
&lt;li&gt;Work with RDDs and DataFrames for data processing and analysis.&lt;/li&gt;
&lt;li&gt;Use Spark SQL to run queries and manipulate structured data.&lt;/li&gt;
&lt;li&gt;Process streaming data with Spark Streaming.&lt;/li&gt;
&lt;li&gt;Build and evaluate machine learning models using MLlib.&lt;/li&gt;
&lt;li&gt;Optimize Spark applications for improved performance.&lt;/li&gt;
&lt;li&gt;Apply your skills to a small capstone project.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-questions-answered&#34;&gt;Key Questions Answered&lt;/h4&gt;
&lt;p&gt;Throughout the course, you will be able to answer questions such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the key components of Apache Spark, and how do they interact?&lt;/li&gt;
&lt;li&gt;How can you create and manipulate RDDs and DataFrames in PySpark?&lt;/li&gt;
&lt;li&gt;How do you use Spark SQL to perform data queries and transformations?&lt;/li&gt;
&lt;li&gt;What techniques are available for processing real-time data streams in Spark?&lt;/li&gt;
&lt;li&gt;How can you build and deploy machine learning models using Spark&amp;rsquo;s MLlib?&lt;/li&gt;
&lt;li&gt;What strategies can be employed to optimize Spark applications for performance?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;time-commitment&#34;&gt;Time Commitment&lt;/h4&gt;
&lt;p&gt;This course is designed to be completed over approximately 30 hours of self-paced study. Here is a breakdown of the estimated time required for each module:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Module 1: Introduction to Big Data and Apache Spark&lt;/strong&gt;: 3 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 2: Spark Architecture and RDDs&lt;/strong&gt;: 5 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 3: DataFrames and Spark SQL&lt;/strong&gt;: 6 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 4: Data Sources and Sinks&lt;/strong&gt;: 4 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 5: Spark Streaming&lt;/strong&gt;: 12 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 6: Machine Learning with PySpark&lt;/strong&gt;: 0 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 7: Advanced Spark Techniques&lt;/strong&gt;: 0 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 8: Real-World Applications and Capstone Project&lt;/strong&gt;: 0 hours&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This structured approach ensures that you will have ample time to grasp each concept thoroughly, practice through hands-on exercises, and apply what you have learned to real-world scenarios.&lt;/p&gt;
&lt;h4 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h4&gt;
&lt;p&gt;To begin, make sure you can install and configure PySpark. Familiarize yourself with the course materials, including the recommended readings and online resources. Each module might  contain practical assignments and quizzes to reinforce your learning, so take your time to complete these exercises when available.&lt;/p&gt;
&lt;h3 id=&#34;course-outline-pyspark-fundamentals-and-advanced-topicshttpsdataqubedioteachingpyspark-fundamentals-and-advanced-topics&#34;&gt;Course Outline: 
&lt;/h3&gt;
&lt;h4 id=&#34;module-1-introduction-to-big-data-and-apache-sparkhttpsdataqubedioteachingpyspark-introduction-to-big-data-and-apache-spark&#34;&gt;
&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Introduction to Big Data&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;What is Big Data?&lt;/li&gt;
&lt;li&gt;Challenges of Big Data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overview of Apache Spark&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;History and evolution&lt;/li&gt;
&lt;li&gt;Components of the Spark ecosystem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Installing PySpark&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;System requirements&lt;/li&gt;
&lt;li&gt;Installing PySpark on a Linux environment (WSL)&lt;/li&gt;
&lt;li&gt;Configuring PySpark and Jupyter Notebook&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-2-spark-architecture-and-rddshttpsdataqubedioteachingpyspark-spark-architecture-and-rdds&#34;&gt;
&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Spark Architecture&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Spark driver and executors&lt;/li&gt;
&lt;li&gt;SparkContext and SparkSession&lt;/li&gt;
&lt;li&gt;Cluster managers (Standalone, YARN, Mesos, Kubernetes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Resilient Distributed Datasets (RDDs)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;What are RDDs?&lt;/li&gt;
&lt;li&gt;Creating RDDs&lt;/li&gt;
&lt;li&gt;Transformations and Actions&lt;/li&gt;
&lt;li&gt;RDD Lineage and Persistence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-3-dataframes-and-spark-sqlhttpsdataqubedioteachingpyspark-dataframes-and-spark-sql&#34;&gt;
&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Introduction to DataFrames&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Difference between RDDs and DataFrames&lt;/li&gt;
&lt;li&gt;Creating DataFrames&lt;/li&gt;
&lt;li&gt;Schema inference and manual schema definition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DataFrame Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Selecting, filtering, and transforming data.&lt;/li&gt;
&lt;li&gt;Aggregations and Grouping&lt;/li&gt;
&lt;li&gt;Joins and unions&lt;/li&gt;
&lt;li&gt;Working with semi-structured data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spark SQL&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SQL queries with Spark&lt;/li&gt;
&lt;li&gt;Registering DataFrames as tables&lt;/li&gt;
&lt;li&gt;Using SQL queries to manipulate DataFrames&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-4-data-sources-and-sinkshttpsdataqubedioteachingpyspark-data-sources-and-sinks&#34;&gt;
&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Reading Data&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Reading from CSV, JSON, Parquet, and other formats&lt;/li&gt;
&lt;li&gt;Reading from databases (JDBC)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Writing Data&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Writing to CSV, JSON, Parquet, and other formats&lt;/li&gt;
&lt;li&gt;Writing to databases (JDBC)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Sources API&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Introduction to the Data Sources API&lt;/li&gt;
&lt;li&gt;Custom data sources&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-5-spark-streaming&#34;&gt;Module 5: Spark Streaming&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Introduction to Spark Streaming&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;What is Spark Streaming?&lt;/li&gt;
&lt;li&gt;DStreams and micro-batching&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming Sources&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Reading from Kafka, Socket, and other sources&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming Operations&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Transformations on DStreams&lt;/li&gt;
&lt;li&gt;Windowed operations and stateful computations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fault Tolerance and Checkpointing&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Handling fault tolerance in streaming applications&lt;/li&gt;
&lt;li&gt;Checkpointing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-6-machine-learning-with-pyspark-0-hours&#34;&gt;Module 6: Machine Learning with PySpark (0 hours)&lt;/h4&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;hyperparameter optimization framework designed for both single-machine and distributed setups: Optuna and Hyperopt&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Introduction to MLlib&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Overview of MLlib&lt;/li&gt;
&lt;li&gt;Data preprocessing with MLlib&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Building Machine Learning Models&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Supervised learning algorithms (e.g., Linear Regression, Logistic Regression, Decision Trees)&lt;/li&gt;
&lt;li&gt;Unsupervised learning algorithms (e.g., K-means clustering)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Evaluation and Hyperparameter Tuning&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Evaluating model performance&lt;/li&gt;
&lt;li&gt;Cross-validation and hyperparameter tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pipeline API&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Building ML pipelines&lt;/li&gt;
&lt;li&gt;Using feature transformers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Case study: Predictive modeling with MLlib&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-7-advanced-spark-techniques-0-hours&#34;&gt;Module 7: Advanced Spark Techniques (0 hours)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Optimizing Spark Applications&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Understanding Spark execution plan&lt;/li&gt;
&lt;li&gt;Catalyst optimizer&lt;/li&gt;
&lt;li&gt;Tungsten execution engine&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance Tuning&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Caching and persistence strategies&lt;/li&gt;
&lt;li&gt;Memory management and garbage collection&lt;/li&gt;
&lt;li&gt;Shuffle operations and optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Debugging and Monitoring&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Using Spark UI&lt;/li&gt;
&lt;li&gt;Logging and metrics&lt;/li&gt;
&lt;li&gt;Handling and avoiding common pitfalls&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-8-real-world-applications-and-capstone-project-0-hours&#34;&gt;Module 8: Real-World Applications and Capstone Project (0 hours)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Case Studies and Real-World Applications&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Example projects using PySpark&lt;/li&gt;
&lt;li&gt;Industry use cases&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capstone Project&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Define a problem statement&lt;/li&gt;
&lt;li&gt;Design and implement a PySpark solution&lt;/li&gt;
&lt;li&gt;Optimize and present findings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-9-advanced-features&#34;&gt;Module 9: Advanced Features&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Working with GraphX for graph processing&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;learning-resources&#34;&gt;Learning Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Documentation and Tutorials&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Apache Spark official documentation&lt;/li&gt;
&lt;li&gt;PySpark API reference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Books and Online Courses&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Learning PySpark&amp;rdquo; by Tomasz Drabas and Denny Lee&lt;/li&gt;
&lt;li&gt;Online courses on platforms like Coursera, Udacity, and edX&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Community and Support&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;GitHub repositories for sample projects&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assessment&#34;&gt;Assessment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Quizzes&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;You will be provided with quizzes to reinforce learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capstone Project&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Depending on your track (Data Science or Data Engineering) you will do a small project that requires max a day to complete and deliver.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Participation in quizzes is mandatory, and they contribute to your final score.&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;You are offered the opportunity to earn extra credit through the following two deliverables, which are designed to reinforce and showcase your understanding of the material. Please pay close attention to the instructions and deadlines.&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;&lt;p&gt;&lt;strong&gt;Deliverable 1:&lt;/strong&gt; Cheat Sheet on Material Covered Up to Module 5.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; Create a concise and well-organized cheat sheet that summarizes the key concepts, formulas, and methodologies you have learned up to and including Module 5.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; You may submit your cheat sheet in one of the following formats:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Jupyter Notebook (.ipynb)&lt;/li&gt;
&lt;li&gt;A Databricks Notebook&lt;/li&gt;
&lt;li&gt;A well-structured PDF file (maximum of 2 pages).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you choose to submit a PDF, you must also include the original file format (e.g., the source Jupyter Notebook or Databricks Notebook from which the PDF was generated).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deadline:&lt;/strong&gt; Your cheat sheet must be submitted by October 11 at 23:59 CET. Please adhere strictly to this deadline.&lt;/p&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;&lt;p&gt;&lt;strong&gt;Deliverable 2:&lt;/strong&gt; To Be Announced&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details:&lt;/strong&gt; The second deliverable will be announced shortly. I assure you that it will not require more than a full day of work for a single individual.&lt;/p&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;By the end of this material you will be able to provide answers to the questions bellow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Describe the PySpark architecture.&lt;/li&gt;
&lt;li&gt;What are RDDs in PySpark?&lt;/li&gt;
&lt;li&gt;Explain the concept of lazy evaluation in PySpark.&lt;/li&gt;
&lt;li&gt;How does PySpark differ from Apache Hadoop?&lt;/li&gt;
&lt;li&gt;What are DataFrames in PySpark?&lt;/li&gt;
&lt;li&gt;How do you initialize a SparkSession?&lt;/li&gt;
&lt;li&gt;What is the significance of the SparkContext?&lt;/li&gt;
&lt;li&gt;Describe the types of transformations in PySpark.&lt;/li&gt;
&lt;li&gt;How do you read a CSV file into a PySpark DataFrame?&lt;/li&gt;
&lt;li&gt;What are actions in PySpark, and how do they differ from transformations?&lt;/li&gt;
&lt;li&gt;How can you filter rows in a DataFrame?&lt;/li&gt;
&lt;li&gt;Explain how to perform joins in PySpark.&lt;/li&gt;
&lt;li&gt;How do you aggregate data in PySpark?&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;How can you handle missing or null values in PySpark?&lt;/li&gt;
&lt;li&gt;How do you repartition a DataFrame, and why?&lt;/li&gt;
&lt;li&gt;Describe how to cache a DataFrame. Why is it useful?&lt;/li&gt;
&lt;li&gt;How do you save a DataFrame to a file?&lt;/li&gt;
&lt;li&gt;Explain the concept of partitioning in PySpark.&lt;/li&gt;
&lt;li&gt;How can broadcast variables improve performance?&lt;/li&gt;
&lt;li&gt;What are accumulators, and how are they used?&lt;/li&gt;
&lt;li&gt;How does PySpark handle data skewness?&lt;/li&gt;
&lt;li&gt;Explain how checkpointing works in PySpark.&lt;/li&gt;
&lt;li&gt;What is delta lake? Look at 
 and 
!&lt;/li&gt;
&lt;li&gt;What is data lakehouse architecture.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
