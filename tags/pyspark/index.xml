<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pyspark | Your Data Science Mentor - Mohsen Davarynejad</title>
    <link>https://dataqubed.io/tags/pyspark/</link>
      <atom:link href="https://dataqubed.io/tags/pyspark/index.xml" rel="self" type="application/rss+xml" />
    <description>Pyspark</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sun, 18 Aug 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dataqubed.io/media/icon_hu7729264130191091259.png</url>
      <title>Pyspark</title>
      <link>https://dataqubed.io/tags/pyspark/</link>
    </image>
    
    <item>
      <title>PySpark - Module 4</title>
      <link>https://dataqubed.io/teaching/pyspark-data-sources-and-sinks/</link>
      <pubDate>Sun, 18 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-data-sources-and-sinks/</guid>
      <description>&lt;p&gt;So far we have gained enough experience with running Spark locally. Is it now the time to move to DataBricks and gain a bit of experience on how with that platform? Lets do it.&lt;/p&gt;
&lt;h2 id=&#34;data-sources-and-sinks&#34;&gt;Data Sources and Sinks&lt;/h2&gt;
&lt;p&gt;By now you have some understating of Spark and distributed computing. You know the RDDs, the importance of lazy evaluations as well as the reason why RDDs are fault tolerant.&lt;/p&gt;
&lt;p&gt;You also gained a bit of experience on how to install Spark on your local machine, how to test it and did a bit of coding in PySpark.&lt;/p&gt;
&lt;p&gt;Also we did have a look into DataBricks environment, what does it have to offer to us and how we can open a community edition of DataBricks next to coding and practice a bit of PySpark. In this module we will study how to read and write from/into CSV, JSON, Parquet, and other formats. We will then continue by making jdbc connection for reading from / writing to databases.&lt;/p&gt;
&lt;h2 id=&#34;reading-and-writing-frominto-csv-json-parquet-and-other-formats&#34;&gt;Reading and writing from/into CSV, JSON, Parquet, and other formats&lt;/h2&gt;
&lt;p&gt;First watch this video bellow.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/ZBI1UFbEM4c?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Now clone &lt;a href=&#34;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2862089966922105/717620449459134/4682920752598681/latest.html&#34;&gt;this Databricks notebook&lt;/a&gt; into your workspace. Call it &lt;strong&gt;07 - Working with Semi-Structured Data&lt;/strong&gt; for consistency reasons. Follow the steps and cells. The material should be self explanatory.&lt;/p&gt;
&lt;h2 id=&#34;reading-form-and-writing-into-databases-using-jdbc&#34;&gt;Reading form and Writing into databases using JDBC&lt;/h2&gt;
&lt;p&gt;Watch the Youtube video bellow.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/oLIagnUAN2E?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;TODO: What then?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 3</title>
      <link>https://dataqubed.io/teaching/pyspark-dataframes-and-spark-sql/</link>
      <pubDate>Sat, 17 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-dataframes-and-spark-sql/</guid>
      <description>&lt;h2 id=&#34;module-3-dataframes-and-spark-sql-in-databricks&#34;&gt;Module 3: DataFrames and Spark SQL (in Databricks)&lt;/h2&gt;
&lt;p&gt;By now you have some understating of Spark and distributed computing. You know the RDDs, the importance of lazy evaluations as well as the reason why RDDs are fault tolerant.&lt;/p&gt;
&lt;p&gt;You also gained a bit of experience on how to install Spark on your local machine, how to test it and did a bit of coding in PySpark.&lt;/p&gt;
&lt;p&gt;Today we will learn a bit on DataBricks, what does it have to offer to us and how we can open a community edition of DataBricks. Next we wil run few notebooks and will make Spark Dataframes and will query them using Spark SQL.&lt;/p&gt;
&lt;h3 id=&#34;learning-outcomes&#34;&gt;Learning Outcomes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Understanding DataBricks:&lt;/strong&gt; Gain an overview of what DataBricks is, including its features and benefits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Accessing DataBricks Community Edition:&lt;/strong&gt; Learn how to access and set up the Community Edition of DataBricks for hands-on practice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Working with Notebooks in DataBricks:&lt;/strong&gt; Learn how to create and run notebooks in DataBricks, which are essential for executing code and managing workflows.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Creating Spark DataFrames:&lt;/strong&gt; Understand how to create Spark DataFrames, a core component of working with data in Spark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Querying Data with Spark SQL:&lt;/strong&gt; Learn how to query data using Spark SQL, leveraging SQL-like syntax for data manipulation within Spark DataFrames.&lt;/p&gt;
&lt;h3 id=&#34;why-databricks&#34;&gt;Why DataBricks?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cloud-native: works on any major cloud provider&lt;/li&gt;
&lt;li&gt;Data storage: store a broad range of data including structured, unstructured and streaming&lt;/li&gt;
&lt;li&gt;Governance and management: in-built security controls and governance&lt;/li&gt;
&lt;li&gt;Tools: wide range of production-ready data tooling from engineering to BI, AI and ML&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/whydb.png&#34; alt=&#34;Why DataBricks&#34;&gt;&lt;/p&gt;
&lt;p&gt;With Databricks Community Edition, we would like to avoid installing new software and maintaining it. Also it gives us a ton of flexibility and features that we will see in the next sections.&lt;/p&gt;
&lt;p&gt;A non comprehensive list of benefits of DataBricks:&lt;/p&gt;
&lt;h4 id=&#34;1-scalability-and-performance&#34;&gt;1. Scalability and Performance&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Built on Apache Spark for efficient big data processing&lt;/li&gt;
&lt;li&gt;Seamlessly scales from small to large clusters&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-unified-analytics-platform&#34;&gt;2. Unified Analytics Platform&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Integrates data engineering, data science, and machine learning&lt;/li&gt;
&lt;li&gt;Simplifies workflows and enhances team collaboration&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;3-ease-of-use&#34;&gt;3. Ease of Use&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Interactive workspace with collaborative notebooks&lt;/li&gt;
&lt;li&gt;Supports multiple languages: Python, SQL, R, Scala&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;4-advanced-analytics-and-machine-learning&#34;&gt;4. Advanced Analytics and Machine Learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Robust support for machine learning and AI&lt;/li&gt;
&lt;li&gt;Includes MLflow for experiment tracking and model management&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;5-data-lake-integration&#34;&gt;5. Data Lake Integration&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Supports Delta Lake for reliable data lakes&lt;/li&gt;
&lt;li&gt;Ensures ACID transactions and unifies batch and streaming data processing&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;6-cloud-integration&#34;&gt;6. Cloud Integration&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Seamless integration with AWS, Azure, and Google Cloud&lt;/li&gt;
&lt;li&gt;Leverages cloud-native features for security and cost efficiency&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;7-real-time-data-processing&#34;&gt;7. Real-Time Data Processing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Handles real-time data streams for timely insights&lt;/li&gt;
&lt;li&gt;Essential for real-time analytics, monitoring, and fraud detection&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;8-security-and-compliance&#34;&gt;8. Security and Compliance&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Robust security features including data encryption and role-based access control&lt;/li&gt;
&lt;li&gt;Compliance with various industry standards&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;9-cost-efficiency&#34;&gt;9. Cost Efficiency&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Auto-scaling and optimized cluster management&lt;/li&gt;
&lt;li&gt;Efficient resource management reduces costs&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;10-community-and-ecosystem&#34;&gt;10. Community and Ecosystem&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Large and active community with strong support&lt;/li&gt;
&lt;li&gt;Extensive documentation and third-party integrations&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dataframes-and-spark-sql&#34;&gt;DataFrames and Spark SQL&lt;/h3&gt;
&lt;p&gt;Now that we are aware of some of the benefits of running spark on DataBricks env lets go ahead and open our first Databricks Community Edition account. To learn how to create your Databricks Community Edition account and activate it see section &lt;strong&gt;0. Prep&lt;/strong&gt; bellow.&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prep&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before we dive into the detailed process of learning PySpark in the Databricks environment, I&amp;rsquo;d like you to watch this introductory video. It will provide you with a solid overview of what to expect and help you navigate the material more easily. Don&amp;rsquo;t worry about mastering everything in the video or practicing the content; it&amp;rsquo;s just a warm-up.&lt;/li&gt;
&lt;/ul&gt;

   
       
       &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
         &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/lcI1W2_KUPo?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
         &gt;&lt;/iframe&gt;
       &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;[Introduction to Databricks notebooks] (&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/databricks/notebooks/&#34;&gt;https://learn.microsoft.com/en-us/azure/databricks/notebooks/&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;[Manage notebooks] (&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/databricks/notebooks/notebooks-manage&#34;&gt;https://learn.microsoft.com/en-us/azure/databricks/notebooks/notebooks-manage&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;[Develop code in Databricks notebooks] (&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/databricks/notebooks/notebooks-code&#34;&gt;https://learn.microsoft.com/en-us/azure/databricks/notebooks/notebooks-code&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;[Databricks notebook interface and controls] (&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/databricks/notebooks/notebook-ui&#34;&gt;https://learn.microsoft.com/en-us/azure/databricks/notebooks/notebook-ui&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/databricks/notebooks/notebook-export-import&#34;&gt;Export and import Databricks notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/databricks/notebooks/notebook-outputs&#34;&gt;Notebook outputs and results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/databricks/notebooks/dashboards&#34;&gt;Dashboards in notebooks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/databricks/notebooks/ipywidgets&#34;&gt;ipywidgets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/databricks/notebooks/notebook-workflows&#34;&gt;Run a Databricks notebook from another notebook&lt;/a&gt;, and see the bellow youtube.&lt;/li&gt;
&lt;/ul&gt;

   
       
       &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
         &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/Op_KZJm7_qc?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
         &gt;&lt;/iframe&gt;
       &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/databricks/notebooks/best-practices&#34;&gt;Software engineering best practices for notebooks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Now you know the essential procedures to work in DataBricks environment. Now head towards the &lt;a href=&#34;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2862089966922105/3667358723256163/4682920752598681/latest.html&#34;&gt;00 - Test dev env&lt;/a&gt; notebook, clone it into your workspace and follow the steps. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;00 - Test the dev env&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Now clone &lt;a href=&#34;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2862089966922105/3667358723256208/4682920752598681/latest.html&#34;&gt;01 - Data Prep&lt;/a&gt; into your workspace. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;01 - Data Prep&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: As you already know a good practice in programming is to modularize your code. Following the best practices here I have created a notebook called &lt;a href=&#34;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2862089966922105/3667358723256172/4682920752598681/latest.html&#34;&gt;02 - Functions&lt;/a&gt;. Clone that into you workspace, rename it to &lt;strong&gt;02 - Functions&lt;/strong&gt;, try to read through it, and make adjustments as needed.&lt;/p&gt;
&lt;p&gt;Next, we will explore the fundamentals of DataFrames in Databricks. We’ll cover how to create and manipulate DataFrames, perform basic operations, and leverage DataFrames for data analysis. This session will guide you through using Spark SQL for querying and transforming data, demonstrating practical examples of DataFrame operations.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Introduction to DataFrames&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Clone &lt;a href=&#34;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2862089966922105/3667358723256187/4682920752598681/latest.html&#34;&gt;this notebook&lt;/a&gt;. Rename the file to &lt;strong&gt;03 - Introduction to DataFrames&lt;/strong&gt;. Go through the material and try to make notes. The notebook covers few topics including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Difference between RDDs and DataFrames&lt;/li&gt;
&lt;li&gt;Creating DataFrames&lt;/li&gt;
&lt;li&gt;Schema inference and manual schema definition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Next we learned how to perform basic DataFrame operations including selecting, filtering, and transforming data using PySpark. These operations are fundamental for data manipulation and analysis. We will then learn how to perform aggregations and group data using PySpark. We covered how to group by single and multiple columns, apply multiple aggregate functions, and filter results after grouping.&lt;/p&gt;
&lt;p&gt;We will then dive into working with joins and unions in Spark using Databricks. We&amp;rsquo;ll explore how to combine data from multiple DataFrames using different types of joins, including inner, left, right, and full outer joins. Additionally, we&amp;rsquo;ll cover how to use unions to append data from one DataFrame to another. These operations are fundamental for integrating and manipulating datasets in Spark, making them essential skills for any data engineer or analyst working in a distributed data environment.&lt;/p&gt;
&lt;p&gt;Finally, in this module, we will learn few tips and tricks on how to work with semi-structured data. The focus is on handling JSON data, including how to read, write, and manipulate JSON files using PySpark. You will learn techniques to efficiently process semi-structured data, including using Spark&amp;rsquo;s &lt;code&gt;explode&lt;/code&gt; function to flatten nested data and leveraging &lt;code&gt;schema inference&lt;/code&gt; to automatically detect data structures. The module provides practical examples and exercises to solidify your understanding, ensuring you can effectively manage complex data formats in your projects.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DataFrame Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Selecting, filtering, and transforming data: Follow &lt;a href=&#34;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2862089966922105/1195017303654174/4682920752598681/latest.html&#34;&gt;this notebook&lt;/a&gt;. Referencing rename the file to &lt;strong&gt;04 - DataFrame Operations&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Aggregations and Grouping: Follow &lt;a href=&#34;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2862089966922105/1195017303654195/4682920752598681/latest.html&#34;&gt;this notebook&lt;/a&gt;. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;05 - Aggregations and Grouping&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Joins and unions: Follow &lt;a href=&#34;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2862089966922105/717620449459116/4682920752598681/latest.html&#34;&gt;this notebook&lt;/a&gt;. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;06 - Joins and Unions&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Working with Semi-Structured Data: Follow &lt;a href=&#34;https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2862089966922105/717620449459134/4682920752598681/latest.html&#34;&gt;this notebook&lt;/a&gt;. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;07 - Working with semi-structured data&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spark SQL&lt;/strong&gt; (todo: coming soon)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SQL queries with Spark&lt;/li&gt;
&lt;li&gt;Registering DataFrames as tables&lt;/li&gt;
&lt;li&gt;Using SQL queries to manipulate DataFrames&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;&lt;p&gt;&lt;strong&gt;Assignment: Visualizations in Databricks notebooks&lt;/strong&gt; Databricks has built-in support for charts and visualizations in both Databricks SQL and in notebooks. Based on the data prepared in &lt;strong&gt;01 - Data Prep&lt;/strong&gt; make few visualizations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details:&lt;/strong&gt;: I will provide you with a git repo and you will be tasked place your notebook there. The repo will not accept new entries after the deadline. Please deliver both the Databricks &lt;strong&gt;.dbc&lt;/strong&gt; archive file format and &lt;strong&gt;.HTML&lt;/strong&gt; format. Make sure to give them identical filenames. Also make sure to put the name of the team and team members in the first cell of the notebook.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deadline:&lt;/strong&gt;: Your notebook must be submitted by Sep 29 at 23:59 CET. Please adhere strictly to this deadline.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reading material&lt;/strong&gt;: Need some reading materials? Visit &lt;a href=&#34;https://docs.databricks.com/en/visualizations/index.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/span&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 2</title>
      <link>https://dataqubed.io/teaching/pyspark-spark-architecture-and-rdds/</link>
      <pubDate>Fri, 16 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-spark-architecture-and-rdds/</guid>
      <description>&lt;h2 id=&#34;spark-architecture-and-rdds&#34;&gt;Spark Architecture and RDDs&lt;/h2&gt;
&lt;h3 id=&#34;a-gentle-intro-to-spark-architecture&#34;&gt;A gentle intro to Spark Architecture&lt;/h3&gt;
&lt;p&gt;Look at the Youtube video bellow carefully.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/vJ0eUZxF80s?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;the above video explores and explains the architecture of Apache Spark, emphasizing the roles of the &lt;em&gt;driver&lt;/em&gt; and &lt;em&gt;executors&lt;/em&gt; in both &lt;em&gt;local&lt;/em&gt; and &lt;em&gt;cluster&lt;/em&gt; mode applications. The driver manages the application and can run on the local machine or within a cluster. In client mode, the driver operates locally, while in cluster mode, it starts in the Application Master container, requesting containers from the cluster manager (like Apache YARN) to launch executors. The video explains how Spark can operate in local mode, where everything runs in a single JVM, or in cluster mode, where the driver and executors communicate via the Application Master, with resource allocation handled by the cluster manager. A demonstration shows starting a Spark application in cluster mode with specific executors and utilizing YARN&amp;rsquo;s dynamic allocation feature to release idle executors.&lt;/p&gt;
&lt;p&gt;The video bellow covers how Spark breaks down and executes applications at the executors. It covers the internal workings of Apache Spark, focusing on how it processes code in parallel. The video covers how Spark reads data, processes it, and writes results back to a destination. Three key data structures in Spark namely &lt;em&gt;DataFrames&lt;/em&gt;, &lt;em&gt;Datasets&lt;/em&gt;, and &lt;em&gt;RDDs&lt;/em&gt; (Resilient Distributed Datasets) are presented. Although the emphasis is on DataFrames and Datasets, RDDs are crucial as they form the underlying structure to which DataFrames and Datasets are compiled. RDDs are &lt;strong&gt;resilient&lt;/strong&gt;, &lt;strong&gt;partitioned&lt;/strong&gt;, &lt;strong&gt;distributed&lt;/strong&gt;, and &lt;strong&gt;immutable collections&lt;/strong&gt; of data that can be created from a source or transformed from another RDD.&lt;/p&gt;
&lt;p&gt;The video demonstrates how to create an RDD from a file and control the number of partitions. It explains the difference between transformations and actions in Spark—transformations are lazy operations that create new distributed datasets without sending results back to the driver, while actions are non-lazy and send results back to the driver. It also touches on topics such as Spark&amp;rsquo;s way of handling grouping and counting operations on distributed data, which involves repartitioning the data and triggering shuffle and sort activities. The number of parallel tasks in Spark is influenced by the number of partitions and available executors. Concept of functional programming in Spark is briefly touched on as well.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/fyTiJLKEzME?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;rdds&#34;&gt;RDDs&lt;/h3&gt;
&lt;p&gt;We have seen the RDDs and their properties. If you need a refresher go ahead and see the Youtube video bellow.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/nH6C9vqtyYU?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;As we have seen RDDS are closely linked to the concepts of Lazy evaluation and resilience. When you are done download &lt;strong&gt;01-Creating-RDDs.ipynb&lt;/strong&gt; from &lt;a href=&#34;https://dataqubed.io/uploads/pyspark/01-Creating-RDDs.ipynb&#34;&gt;this link&lt;/a&gt; and follow the instruction and run it locally. Make sure you could run all the cells. The notebook should have sufficient comments in it and that should provide you with some hints about the code.&lt;/p&gt;
&lt;p&gt;The html version of the notebook is available here &lt;a href=&#34;https://dataqubed.io/uploads/pyspark/01-Creating-RDDs.html&#34;&gt;online&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Afterwards download &lt;strong&gt;02-RDD-Lineage-n-Persistence.ipynb&lt;/strong&gt; from &lt;a href=&#34;https://dataqubed.io/uploads/pyspark/02-RDD-Lineage-n-Persistence.ipynb&#34;&gt;this link&lt;/a&gt; and run it locally. The html version of the notebook is available here &lt;a href=&#34;https://dataqubed.io/uploads/pyspark/02-RDD-Lineage-n-Persistence.html&#34;&gt;online&lt;/a&gt; for immediate access.&lt;/p&gt;
&lt;h3 id=&#34;transformations-and-actions-in-spark&#34;&gt;Transformations and Actions in spark&lt;/h3&gt;
&lt;p&gt;Now continue watching the Youtube video bellow entitled &amp;ldquo;Spark RDD Transformations and Actions&amp;rdquo;. The video should give you a sample of various Transformations and Actions in spark.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/5L0oyrwiNNE?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Download &lt;strong&gt;03-Transformations-n-Actions.ipynb&lt;/strong&gt; from &lt;a href=&#34;https://dataqubed.io/uploads/pyspark/03-Transformations-n-Actions.ipynb&#34;&gt;this link&lt;/a&gt; and run it locally. The html version of the notebook is available here &lt;a href=&#34;https://dataqubed.io/uploads/pyspark/03-Transformations-n-Actions.html&#34;&gt;online&lt;/a&gt; for immediate access.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 1</title>
      <link>https://dataqubed.io/teaching/pyspark-introduction-to-big-data-and-apache-spark/</link>
      <pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-introduction-to-big-data-and-apache-spark/</guid>
      <description>&lt;h2 id=&#34;introduction-to-big-data-and-apache-spark&#34;&gt;Introduction to Big Data and Apache Spark&lt;/h2&gt;
&lt;p&gt;PySpark is the Python API for Apache Spark, an open-source, distributed computing system that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Spark is widely used in big data processing due to its ability to perform in-memory computations and its support for a variety of data processing tasks, such as batch processing, stream processing, and machine learning.&lt;/p&gt;
&lt;p&gt;Before we start with the whole concept of PySpark and distributed computing let us begin by the understanding Big Data and how would we define it.&lt;/p&gt;
&lt;h2 id=&#34;what-is-big-data&#34;&gt;What is Big Data?&lt;/h2&gt;
&lt;p&gt;See the video bellow:&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/bAyrObl7TYE?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;and also the one bellow:&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/WEqVfo6nJuU?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;We discussed the five key categories of big data (&lt;strong&gt;Volume&lt;/strong&gt;, &lt;strong&gt;Velocity&lt;/strong&gt;, &lt;strong&gt;Variety&lt;/strong&gt;, &lt;strong&gt;Veracity&lt;/strong&gt;, and &lt;strong&gt;Value&lt;/strong&gt;) and provided some examples of how big data can be used in different ways. We also understood that big data is transforming industries across the board. Whether it&amp;rsquo;s improving patient care in healthcare, optimizing supply chains in retail, or enhancing fraud detection in banking, the impact of big data is undeniable.&lt;/p&gt;
&lt;p&gt;Next we will continue to understand the basics of Spark and Spark ecosystem.&lt;/p&gt;
&lt;h2 id=&#34;what-is-apache-spark&#34;&gt;What is Apache Spark?&lt;/h2&gt;
&lt;p&gt;Before diving into this tutorial, I recommend watching the bellow YouTube video entitles &amp;ldquo;An Introduction to PySpark&amp;rdquo;. This video serves as an excellent introduction to the world of PySpark and distributed computing. It’s a compact guide that walks you through the basics of Apache Spark, comparing it with Pandas, positioning it in the distributed computing ecosystem, and how PySpark fits into the picture.&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t panic if you don’t grasp all the details right away, that’s perfectly okay! The goal is to give you a general sense of what Spark is, why it’s important, and how it can be used to process large datasets efficiently. This foundational knowledge will make the hands-on sections of this tutorial much more meaningful as you start applying what you’ve learned.&lt;/p&gt;
&lt;p&gt;So, take some time to watch the video—absorb as much as you can—and then come back here to deepen your understanding. We will review a lot of this content again and again, not only in a different context but also from a various angels.&lt;/p&gt;
&lt;h3 id=&#34;a-short-summary-of-what-the-video-covers&#34;&gt;A short summary of what the video covers:&lt;/h3&gt;
&lt;p&gt;The talk covers what PySpark is, its capabilities compared to Pandas, and when it’s necessary to use it. It highlights the benefits of PySpark for distributed data processing, explaining Spark&amp;rsquo;s map and reduce transformations and the advantages of lazy evaluation. The decision-making process for choosing between PySpark and other tools is also discussed, considering factors like data volume, existing codebases, team skills, and personal preferences. Throughout, code examples are provided, along with an emphasis on the usefulness of window functions and addressing concerns like the underlying Java in the Python library and potential errors during evaluation.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/ZFLOMSuWHxg?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;pyspark-installation&#34;&gt;PySpark installation&lt;/h3&gt;
&lt;p&gt;After you are done with the video go ahead and install PySpark. &lt;a href=&#34;https://github.com/nicholsonjohnc/spark-wsl-install&#34;&gt;This tutorial&lt;/a&gt; will give you the instruction on how to install PySpark on wsl but the process should remain almost the same when it comes to the installation on other OSs.&lt;/p&gt;
&lt;p&gt;For installing PySpark on your windows machine follow the instruction of the bellow Youtube video.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/WxhPDK4ffq4?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Visit here for a detailed instructions on how to install PySpark on different platforms. You will also learn how to configure your environment to work with PySpark effectively.&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Do you know a simpler and better instruction? Please share your experience.&lt;/span&gt;
&lt;/div&gt;
&lt;h3 id=&#34;check-the-installation&#34;&gt;Check the installation&lt;/h3&gt;
&lt;p&gt;Now lets check if your installation is complete. Download the file &lt;strong&gt;00-Check-your-spark-installation.ipynb&lt;/strong&gt; from &lt;a href=&#34;https://dataqubed.io/uploads/pyspark/00-Check-your-spark-installation.ipynb&#34;&gt;this link&lt;/a&gt; and follow the instruction. Make sure you could run all the cells. The notebook should have sufficient comments in it and that should provide you with some hints about the code.&lt;/p&gt;
&lt;p&gt;The html version of the notebook is available here &lt;a href=&#34;https://dataqubed.io/uploads/pyspark/00-Check-your-spark-installation.html&#34;&gt;online&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 0</title>
      <link>https://dataqubed.io/teaching/pyspark-fundamentals-and-advanced-topics/</link>
      <pubDate>Wed, 14 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-fundamentals-and-advanced-topics/</guid>
      <description>&lt;h3 id=&#34;pyspark-fundamentals-and-advanced-topics&#34;&gt;PySpark Fundamentals and Advanced Topics&lt;/h3&gt;
&lt;p&gt;Distributed computing is at the heart of all the recent advancements of Data science and AI models. We can now scale out as an alternative to scale up. And that makes it possible to retrieve data, prepare the data and train our models not only at a faster speed, but also cheaper.&lt;/p&gt;
&lt;p&gt;One of the pioneer commercial players on the filed of distributed computing is DataBricks. This the company behind the open source Spark community.&lt;/p&gt;
&lt;h4 id=&#34;course-objective&#34;&gt;Course Objective&lt;/h4&gt;
&lt;p&gt;In today&amp;rsquo;s data-driven world, the volume of data generated is growing exponentially. Traditional data processing systems struggle to handle this deluge efficiently. Distributed computing frameworks like Apache Spark have emerged as powerful tools for processing large datasets quickly and efficiently. Understanding Spark and PySpark (the Python API for Spark) is crucial for data professionals who want to leverage the full potential of big data. This course is designed to equip you with the skills to process and analyse large datasets using Apache Spark and PySpark, emphasizing its importance in modern data science and machine learning workflows. By the end of this course, you will have a good understanding of how to process, analyze, and manipulate large datasets efficiently. You will also gain hands-on experience in building machine learning models and optimizing Spark queries for performance.&lt;/p&gt;
&lt;p&gt;Over 32 hours of content is prepared that would help you develop a solid understanding of distributed computing, Spark&amp;rsquo;s architecture, and how to apply Spark for big data processing. The course includes a mix of theoretical concepts and hands-on exercises, leveraging free online resources.&lt;/p&gt;
&lt;h4 id=&#34;course-highlights&#34;&gt;Course Highlights&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fundamentals of PySpark&lt;/strong&gt;: Learn the basics of PySpark, including its architecture, Resilient Distributed Datasets (RDDs), and DataFrames.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Processing and SQL&lt;/strong&gt;: Master the use of DataFrames and Spark SQL for data manipulation and querying.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming and Real-Time Data&lt;/strong&gt;: Understand how to process real-time data using Spark Streaming.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machine Learning&lt;/strong&gt;: Explore machine learning techniques and build models using Spark&amp;rsquo;s MLlib.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimization Techniques&lt;/strong&gt;: Learn how to optimize Spark applications for better performance and efficiency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capstone Project&lt;/strong&gt;: Apply your knowledge to a real-world project, demonstrating your ability to handle big data problems from start to finish.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;learning-outcomes&#34;&gt;Learning Outcomes&lt;/h4&gt;
&lt;p&gt;By the end of this course, you will be able to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install and configure PySpark on a Linux environment.&lt;/li&gt;
&lt;li&gt;Understand and explain the architecture and components of Apache Spark.&lt;/li&gt;
&lt;li&gt;Work with RDDs and DataFrames for data processing and analysis.&lt;/li&gt;
&lt;li&gt;Use Spark SQL to run queries and manipulate structured data.&lt;/li&gt;
&lt;li&gt;Process streaming data with Spark Streaming.&lt;/li&gt;
&lt;li&gt;Build and evaluate machine learning models using MLlib.&lt;/li&gt;
&lt;li&gt;Optimize Spark applications for improved performance.&lt;/li&gt;
&lt;li&gt;Apply your skills to a small capstone project.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-questions-answered&#34;&gt;Key Questions Answered&lt;/h4&gt;
&lt;p&gt;Throughout the course, you will be able to answer questions such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the key components of Apache Spark, and how do they interact?&lt;/li&gt;
&lt;li&gt;How can you create and manipulate RDDs and DataFrames in PySpark?&lt;/li&gt;
&lt;li&gt;How do you use Spark SQL to perform data queries and transformations?&lt;/li&gt;
&lt;li&gt;What techniques are available for processing real-time data streams in Spark?&lt;/li&gt;
&lt;li&gt;How can you build and deploy machine learning models using Spark&amp;rsquo;s MLlib?&lt;/li&gt;
&lt;li&gt;What strategies can be employed to optimize Spark applications for performance?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;time-commitment&#34;&gt;Time Commitment&lt;/h4&gt;
&lt;p&gt;This course is designed to be completed over approximately 32 hours of self-paced study. Here is a breakdown of the estimated time required for each module:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Module 1: Introduction to Big Data and Apache Spark&lt;/strong&gt;: 3 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 2: Spark Architecture and RDDs&lt;/strong&gt;: 5 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 3: DataFrames and Spark SQL&lt;/strong&gt;: 6 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 4: Data Sources and Sinks&lt;/strong&gt;: 4 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 5: Spark Streaming&lt;/strong&gt;: 4 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 6: Machine Learning with PySpark&lt;/strong&gt;: 6 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 7: Advanced Spark Techniques&lt;/strong&gt;: 4 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 8: Real-World Applications and Capstone Project&lt;/strong&gt;: 4 hours&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This structured approach ensures that you will have ample time to grasp each concept thoroughly, practice through hands-on exercises, and apply what you have learned to real-world scenarios.&lt;/p&gt;
&lt;h4 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h4&gt;
&lt;p&gt;To begin, make sure you can install and configure PySpark. Familiarize yourself with the course materials, including the recommended readings and online resources. Each module contains practical assignments and quizzes to reinforce your learning, so take your time to complete these exercises.&lt;/p&gt;
&lt;h3 id=&#34;course-outline-pyspark-fundamentals-and-advanced-topicshttpsdataqubedioteachingpyspark-fundamentals-and-advanced-topics&#34;&gt;Course Outline: &lt;a href=&#34;https://dataqubed.io/teaching/pyspark-fundamentals-and-advanced-topics/&#34;&gt;PySpark Fundamentals and Advanced Topics&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id=&#34;module-1-introduction-to-big-data-and-apache-sparkhttpsdataqubedioteachingpyspark-introduction-to-big-data-and-apache-spark&#34;&gt;&lt;a href=&#34;https://dataqubed.io/teaching/pyspark-introduction-to-big-data-and-apache-spark/&#34;&gt;Module 1: Introduction to Big Data and Apache Spark&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Introduction to Big Data&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;What is Big Data?&lt;/li&gt;
&lt;li&gt;Challenges of Big Data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overview of Apache Spark&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;History and evolution&lt;/li&gt;
&lt;li&gt;Comparison with Hadoop MapReduce&lt;/li&gt;
&lt;li&gt;Components of the Spark ecosystem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Installing PySpark&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;System requirements&lt;/li&gt;
&lt;li&gt;Installing PySpark on a Linux environment&lt;/li&gt;
&lt;li&gt;Configuring PySpark and Jupyter Notebook&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-2-spark-architecture-and-rddshttpsdataqubedioteachingpyspark-spark-architecture-and-rdds&#34;&gt;&lt;a href=&#34;https://dataqubed.io/teaching/pyspark-spark-architecture-and-rdds/&#34;&gt;Module 2: Spark Architecture and RDDs&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Spark Architecture&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Spark driver and executors&lt;/li&gt;
&lt;li&gt;SparkContext and SparkSession&lt;/li&gt;
&lt;li&gt;Cluster managers (Standalone, YARN, Mesos, Kubernetes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Resilient Distributed Datasets (RDDs)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;What are RDDs?&lt;/li&gt;
&lt;li&gt;Creating RDDs&lt;/li&gt;
&lt;li&gt;Transformations and Actions&lt;/li&gt;
&lt;li&gt;RDD Lineage and Persistence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-3-dataframes-and-spark-sqlhttpsdataqubedioteachingpyspark-dataframes-and-spark-sql&#34;&gt;&lt;a href=&#34;https://dataqubed.io/teaching/pyspark-dataframes-and-spark-sql/&#34;&gt;Module 3: DataFrames and Spark SQL&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Introduction to DataFrames&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Difference between RDDs and DataFrames&lt;/li&gt;
&lt;li&gt;Creating DataFrames&lt;/li&gt;
&lt;li&gt;Schema inference and manual schema definition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DataFrame Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Selecting, filtering, and transforming data.&lt;/li&gt;
&lt;li&gt;Aggregations and Grouping&lt;/li&gt;
&lt;li&gt;Joins and unions&lt;/li&gt;
&lt;li&gt;Working with semi-structured data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spark SQL&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SQL queries with Spark&lt;/li&gt;
&lt;li&gt;Registering DataFrames as tables&lt;/li&gt;
&lt;li&gt;Using SQL queries to manipulate DataFrames&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-4-data-sources-and-sinkshttpsdataqubedioteachingpyspark-data-sources-and-sinks&#34;&gt;&lt;a href=&#34;https://dataqubed.io/teaching/pyspark-data-sources-and-sinks/&#34;&gt;Module 4: Data Sources and Sinks&lt;/a&gt;&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Reading Data&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Reading from CSV, JSON, Parquet, and other formats&lt;/li&gt;
&lt;li&gt;Reading from databases (JDBC)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Writing Data&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Writing to CSV, JSON, Parquet, and other formats&lt;/li&gt;
&lt;li&gt;Writing to databases (JDBC)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Sources API&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Introduction to the Data Sources API&lt;/li&gt;
&lt;li&gt;Custom data sources&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-5-spark-streaming-4-hours&#34;&gt;Module 5: Spark Streaming (4 hours)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Introduction to Spark Streaming&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;What is Spark Streaming?&lt;/li&gt;
&lt;li&gt;DStreams and micro-batching&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming Sources&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Reading from Kafka, Socket, and other sources&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming Operations&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Transformations on DStreams&lt;/li&gt;
&lt;li&gt;Windowed operations and stateful computations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fault Tolerance and Checkpointing&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Handling fault tolerance in streaming applications&lt;/li&gt;
&lt;li&gt;Checkpointing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-6-machine-learning-with-pyspark-6-hours&#34;&gt;Module 6: Machine Learning with PySpark (6 hours)&lt;/h4&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;hyperparameter optimization framework designed for both single-machine and distributed setups: Optuna and Hyperopt&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Introduction to MLlib&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Overview of MLlib&lt;/li&gt;
&lt;li&gt;Data preprocessing with MLlib&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Building Machine Learning Models&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Supervised learning algorithms (e.g., Linear Regression, Logistic Regression, Decision Trees)&lt;/li&gt;
&lt;li&gt;Unsupervised learning algorithms (e.g., K-means clustering)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Evaluation and Hyperparameter Tuning&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Evaluating model performance&lt;/li&gt;
&lt;li&gt;Cross-validation and hyperparameter tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pipeline API&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Building ML pipelines&lt;/li&gt;
&lt;li&gt;Using feature transformers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Case study: Predictive modeling with MLlib&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-7-advanced-spark-techniques-4-hours&#34;&gt;Module 7: Advanced Spark Techniques (4 hours)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Optimizing Spark Applications&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Understanding Spark execution plan&lt;/li&gt;
&lt;li&gt;Catalyst optimizer&lt;/li&gt;
&lt;li&gt;Tungsten execution engine&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance Tuning&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Caching and persistence strategies&lt;/li&gt;
&lt;li&gt;Memory management and garbage collection&lt;/li&gt;
&lt;li&gt;Shuffle operations and optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Debugging and Monitoring&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Using Spark UI&lt;/li&gt;
&lt;li&gt;Logging and metrics&lt;/li&gt;
&lt;li&gt;Handling and avoiding common pitfalls&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-8-real-world-applications-and-capstone-project-4-hours&#34;&gt;Module 8: Real-World Applications and Capstone Project (4 hours)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Case Studies and Real-World Applications&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Example projects using PySpark&lt;/li&gt;
&lt;li&gt;Industry use cases&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capstone Project&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Define a problem statement&lt;/li&gt;
&lt;li&gt;Design and implement a PySpark solution&lt;/li&gt;
&lt;li&gt;Optimize and present findings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-9-advanced-features&#34;&gt;Module 9: Advanced Features&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Working with GraphX for graph processing&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Using structured streaming for real-time analytics&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;learning-resources&#34;&gt;Learning Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Documentation and Tutorials&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Apache Spark official documentation&lt;/li&gt;
&lt;li&gt;PySpark API reference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Books and Online Courses&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Learning PySpark&amp;rdquo; by Tomasz Drabas and Denny Lee&lt;/li&gt;
&lt;li&gt;Online courses on platforms like Coursera, Udacity, and edX&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Community and Support&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;GitHub repositories for sample projects&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assessment&#34;&gt;Assessment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Quizzes&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;You will be provided with quizzes to reinforce learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capstone Project&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Depending on your track (Data Science or Data Engineering) you will do a small project that requires max a day to complete and deliver.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Participation in quizzes is mandatory, and they contribute to your final score.&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;You are offered the opportunity to earn extra credit through the following two deliverables, which are designed to reinforce and showcase your understanding of the material. Please pay close attention to the instructions and deadlines.&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;&lt;p&gt;&lt;strong&gt;Deliverable 1:&lt;/strong&gt; Cheat Sheet on Material Covered Up to Module 5.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; Create a concise and well-organized cheat sheet that summarizes the key concepts, formulas, and methodologies you have learned up to and including Module 5.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; You may submit your cheat sheet in one of the following formats:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Jupyter Notebook (.ipynb)&lt;/li&gt;
&lt;li&gt;A Databricks Notebook&lt;/li&gt;
&lt;li&gt;A well-structured PDF file (maximum of 2 pages).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you choose to submit a PDF, you must also include the original file format (e.g., the source Jupyter Notebook or Databricks Notebook from which the PDF was generated).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deadline:&lt;/strong&gt; Your cheat sheet must be submitted by October 11 at 23:59 CET. Please adhere strictly to this deadline.&lt;/p&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;&lt;p&gt;&lt;strong&gt;Deliverable 2:&lt;/strong&gt; To Be Announced&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details:&lt;/strong&gt; The second deliverable will be announced shortly. I assure you that it will not require more than a full day of work for a single individual.&lt;/p&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;By the end of this material you will be able to provide answers to the questions bellow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Describe the PySpark architecture.&lt;/li&gt;
&lt;li&gt;What are RDDs in PySpark?&lt;/li&gt;
&lt;li&gt;Explain the concept of lazy evaluation in PySpark.&lt;/li&gt;
&lt;li&gt;How does PySpark differ from Apache Hadoop?&lt;/li&gt;
&lt;li&gt;What are DataFrames in PySpark?&lt;/li&gt;
&lt;li&gt;How do you initialize a SparkSession?&lt;/li&gt;
&lt;li&gt;What is the significance of the SparkContext?&lt;/li&gt;
&lt;li&gt;Describe the types of transformations in PySpark.&lt;/li&gt;
&lt;li&gt;How do you read a CSV file into a PySpark DataFrame?&lt;/li&gt;
&lt;li&gt;What are actions in PySpark, and how do they differ from transformations?&lt;/li&gt;
&lt;li&gt;How can you filter rows in a DataFrame?&lt;/li&gt;
&lt;li&gt;Explain how to perform joins in PySpark.&lt;/li&gt;
&lt;li&gt;How do you aggregate data in PySpark?&lt;/li&gt;
&lt;li&gt;What are UDFs (User Defined Functions), and how are they used? &lt;a href=&#34;https://www.youtube.com/watch?v=-U-_rq1zhiI&amp;amp;list=PLkz1SCf5iB4dXiPdFD4hXwheRGRwhmd6K&amp;amp;index=19&#34;&gt;https://www.youtube.com/watch?v=-U-_rq1zhiI&amp;amp;list=PLkz1SCf5iB4dXiPdFD4hXwheRGRwhmd6K&amp;amp;index=19&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;How can you handle missing or null values in PySpark?&lt;/li&gt;
&lt;li&gt;How do you repartition a DataFrame, and why?&lt;/li&gt;
&lt;li&gt;Describe how to cache a DataFrame. Why is it useful?&lt;/li&gt;
&lt;li&gt;How do you save a DataFrame to a file?&lt;/li&gt;
&lt;li&gt;What is the Catalyst Optimizer?&lt;/li&gt;
&lt;li&gt;Explain the concept of partitioning in PySpark.&lt;/li&gt;
&lt;li&gt;How can broadcast variables improve performance?&lt;/li&gt;
&lt;li&gt;What are accumulators, and how are they used?&lt;/li&gt;
&lt;li&gt;Describe strategies for optimizing PySpark jobs.&lt;/li&gt;
&lt;li&gt;How does PySpark handle data skewness?&lt;/li&gt;
&lt;li&gt;Explain how checkpointing works in PySpark.&lt;/li&gt;
&lt;li&gt;What is delta lake &lt;a href=&#34;https://www.youtube.com/watch?v=0GhFAzN4qs4&amp;amp;list=PLkz1SCf5iB4dXiPdFD4hXwheRGRwhmd6K&amp;amp;index=20&#34;&gt;https://www.youtube.com/watch?v=0GhFAzN4qs4&amp;amp;list=PLkz1SCf5iB4dXiPdFD4hXwheRGRwhmd6K&amp;amp;index=20&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/watch?v=xYtU6fpsS3M&amp;amp;list=PLkz1SCf5iB4dXiPdFD4hXwheRGRwhmd6K&amp;amp;index=21&#34;&gt;https://www.youtube.com/watch?v=xYtU6fpsS3M&amp;amp;list=PLkz1SCf5iB4dXiPdFD4hXwheRGRwhmd6K&amp;amp;index=21&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;What is data lakehouse architecture.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
