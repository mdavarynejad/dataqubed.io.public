{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2cabad-ad8f-42ac-bb53-04e1079520be",
   "metadata": {},
   "source": [
    "## RDD Lineage\n",
    "RDD Lineage is the sequence of transformations that have been applied to create a particular RDD. It's essentially a logical execution plan, which Spark uses to rebuild an RDD in case of a failure. Lineage information helps in recomputing lost data in case of node failures.\n",
    "\n",
    "Example:\n",
    "Let's create an RDD and perform some transformations to understand the concept of lineage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071317bd-b66c-4510-ae28-7c78027a1f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/16 11:50:38 WARN Utils: Your hostname, AlienDVD resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/06/16 11:50:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/16 11:50:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/16 11:50:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext(\"local\", \"RDD Lineage Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "468fe111-011a-4944-804e-86565e56baba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(1) PythonRDD[1] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289 []'\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Apply some transformations\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "rdd3 = rdd2.filter(lambda x: x > 5)\n",
    "\n",
    "# To get the lineage, we can use the toDebugString method\n",
    "print(rdd3.toDebugString())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4824ada1-d319-4118-9429-dd0893418253",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- rdd: This RDD is created from a list of numbers.\n",
    "- rdd2: This is derived from rdd by applying a map transformation.\n",
    "- rdd3: This is derived from rdd2 by applying a filter transformation.\n",
    "\n",
    "The toDebugString method will print out the lineage of the RDDs, showing how rdd3 was derived from rdd2, which was derived from rdd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f9e6d-94c3-4603-89ff-99cf05b53113",
   "metadata": {},
   "source": [
    "## RDD Persistence (Caching)\n",
    "\n",
    "RDD persistence (or caching) allows us to store RDDs in memory (or disk) across operations. This is useful when an RDD is used multiple times, as it avoids recomputation and speeds up the computation process.\n",
    "\n",
    "### Persisting an RDD\n",
    "\n",
    "You can persist an RDD using the persist method. By default, RDDs are not persisted. Spark provides several storage levels, such as MEMORY_ONLY, MEMORY_AND_DISK, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eec47afa-ba55-47a0-97c5-c66d8562d7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Apply a transformation\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "\n",
    "# Persist the RDD in memory\n",
    "rdd2.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# Perform actions on the persisted RDD\n",
    "print(rdd2.collect())  # First action will trigger computation and caching\n",
    "print(rdd2.count())    # This action will use the cached RDD\n",
    "\n",
    "# Unpersist the RDD when done\n",
    "rdd2.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce47bbe6-ffc9-4a21-964c-577a92c31928",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "- rdd: This RDD is created from a list of numbers.\n",
    "- rdd2: This RDD is derived from rdd by applying a map transformation.\n",
    "- rdd2.persist(StorageLevel.MEMORY_ONLY): This line tells Spark to persist rdd2 in memory. When an action is performed on rdd2 for the first time, it is computed and cached in memory. Subsequent actions on rdd2 will use the cached data, making them faster.\n",
    "\n",
    "### Storage Levels\n",
    "- MEMORY_ONLY: Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they are needed.\n",
    "- MEMORY_AND_DISK: Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that do not fit on disk, and read them from there when needed.\n",
    "- MEMORY_ONLY_SER: Store RDD as serialized Java objects (one-byte array per partition). This is more space-efficient but slower to access.\n",
    "- DISK_ONLY: Store RDD partitions only on disk.\n",
    "MEMORY_AND_DISK_SER: Similar to MEMORY_AND_DISK, but store as serialized Java objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c35a029-fa57-4bc1-85f6-5e93b87fd992",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **RDD Lineage:** The logical execution plan or the sequence of transformations to create an RDD.\n",
    "- **RDD Persistence:** Storing RDDs in memory or disk to avoid recomputation, which speeds up subsequent actions on the RDD.\n",
    "- **Storage Levels:** Different strategies for persisting RDDs, balancing between memory usage and speed of access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a195ea-5168-4541-a147-49caecdec970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
