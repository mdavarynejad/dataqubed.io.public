<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Your Data Science Mentor - Mohsen Davarynejad</title>
    <link>https://dataqubed.io/</link>
      <atom:link href="https://dataqubed.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Your Data Science Mentor - Mohsen Davarynejad</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://dataqubed.io/media/icon_hu7729264130191091259.png</url>
      <title>Your Data Science Mentor - Mohsen Davarynejad</title>
      <link>https://dataqubed.io/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://dataqubed.io/event/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/event/example/</guid>
      <description>&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Hugo Blox Builder&amp;rsquo;s 
 feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including 
 such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Sparkling Water - Module 6.2</title>
      <link>https://dataqubed.io/teaching/pyspark-sparklingwater-part2/</link>
      <pubDate>Sun, 08 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-sparklingwater-part2/</guid>
      <description>&lt;h2 id=&#34;installing-h2o-on-wsl-ubuntu-2204&#34;&gt;Installing H2O on WSL Ubuntu 22.04&lt;/h2&gt;
&lt;p&gt;In this guide, I&amp;rsquo;ll walk you through the steps to install H2O on a WSL (Windows Subsystem for Linux) Ubuntu 22.04 environment. We assume you already have Java 11 and Jupyter Notebook installed and running. By the end, you&amp;rsquo;ll be able to access the H2O web UI directly from your Windows machine.&lt;/p&gt;
&lt;h3 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;WSL running Ubuntu 22.04.&lt;/li&gt;
&lt;li&gt;Java 11 installed (required for H2O).&lt;/li&gt;
&lt;li&gt;Jupyter Notebook already installed and running.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;step-1-install-h2o-on-ubuntu&#34;&gt;Step 1: Install H2O on Ubuntu&lt;/h3&gt;
&lt;p&gt;First, we need to install the H2O Python module, which will allow us to run H2O directly within our Jupyter Notebook and also access the H2O web interface. Follow these steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Update your system packages:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt update &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo apt upgrade -y
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install Python dependencies:&lt;/strong&gt;
Ensure &lt;code&gt;pip&lt;/code&gt; is installed and up-to-date:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt install python3-pip -y
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip3 install --upgrade pip
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install the H2O Python library:&lt;/strong&gt;
Use the following commands to install the H2O Python library from the H2O repository:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip3 install h2o
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Verify the installation:&lt;/strong&gt;
To verify that H2O was successfully installed, open Python in your terminal and run the following commands:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;h2o&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;h2o&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;init&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If the installation was successful, you should see a message indicating that H2O is running locally.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;step-2-configuring-h2o-web-ui-for-remote-access&#34;&gt;Step 2: Configuring H2O Web UI for Remote Access&lt;/h3&gt;
&lt;p&gt;Now, let&amp;rsquo;s configure the H2O web interface to be accessible from your Windows machine. H2O’s web interface runs on a specific port (default is &lt;code&gt;54321&lt;/code&gt;), and we&amp;rsquo;ll expose this port so that you can access it via a browser on Windows.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Start H2O:&lt;/strong&gt;
Open Python in your WSL terminal, and start H2O using:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;h2o&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;h2o&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;init&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;bind_to_localhost&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;54321&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;bind_to_localhost=False&lt;/code&gt;: This ensures that H2O listens on all available interfaces.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;port=54321&lt;/code&gt;: This specifies the port on which H2O will run.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Check WSL Network Settings:&lt;/strong&gt;
To find the IP address of your WSL instance, run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;hostname -I
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will return the IP address (e.g., &lt;code&gt;172.22.66.1&lt;/code&gt;) that we will use to access H2O from your Windows browser.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;step-3-accessing-h2o-web-ui-from-windows&#34;&gt;Step 3: Accessing H2O Web UI from Windows&lt;/h3&gt;
&lt;p&gt;With H2O running, you can now access the H2O web interface from your Windows machine.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Open a browser on Windows (Chrome, Firefox, etc.).&lt;/li&gt;
&lt;li&gt;Enter the following URL in your browser’s address bar, replacing &lt;code&gt;&amp;lt;WSL_IP&amp;gt;&lt;/code&gt; with the IP address you retrieved earlier:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;http://&amp;lt;WSL_IP&amp;gt;:54321
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, if your WSL IP is &lt;code&gt;172.22.66.1&lt;/code&gt;, you would enter:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;http://172.22.66.1:54321
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You should now see the H2O web interface, where you can manage models, datasets, and much more. Get yourself familiarized with the H2O web interface.&lt;/p&gt;
&lt;p&gt;You might also want to explore the &lt;code&gt;H2O Wave&lt;/code&gt; Web App for building AI web based applications. See the Youtube video bellow.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/5Ba6fE8WF8Q?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Or you might as well want to look into 
 if you have interest in bulding ML web application.&lt;/p&gt;
&lt;h3 id=&#34;step-4-running-h2o-in-jupyter-notebooks&#34;&gt;Step 4: Running H2O in Jupyter Notebooks&lt;/h3&gt;
&lt;p&gt;Now that H2O is installed and accessible via the web interface, you can also run it in your Jupyter Notebook.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Launch Jupyter Notebook from your WSL terminal:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;jupyter notebook
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;In a new notebook, use the following code to initialize H2O:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;h2o&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;h2o&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;init&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will initialize H2O and you can start working with it within your Jupyter environment.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Sparkling Water - Module 6.1</title>
      <link>https://dataqubed.io/teaching/pyspark-sparklingwater-part1/</link>
      <pubDate>Sat, 07 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-sparklingwater-part1/</guid>
      <description>&lt;h2 id=&#34;introduction-to-distributed-machine-learning-a-practical-approach-with-h2o-and-sparkling-water&#34;&gt;Introduction to Distributed Machine Learning: A Practical Approach with H2O and Sparkling Water&lt;/h2&gt;
&lt;p&gt;As datasets continue to grow in size, the limitations of traditional machine learning systems become more apparent. The need to process, analyze, and train models on massive datasets calls for a new approach—&lt;strong&gt;Distributed Machine Learning&lt;/strong&gt;. In this module, we will explore the basics of distributed ML through analogies, architectures, real-world applications, and tools like H2O and Sparkling Water.&lt;/p&gt;
&lt;h3 id=&#34;analogy-to-real-world-distributed-systems&#34;&gt;Analogy to Real-World Distributed Systems&lt;/h3&gt;
&lt;p&gt;Imagine a large construction project. If one worker had to build an entire house alone, it would take months to complete. However, if you break the tasks into smaller chunks—one person working on the foundation, another on plumbing, another on roofing—the project is completed much faster. Each worker handles a piece of the puzzle simultaneously, and by working in parallel, the project progresses at a much quicker pace.&lt;/p&gt;
&lt;p&gt;Distributed machine learning works in a similar way. Instead of asking a single machine to handle all computations for large datasets, the work is split across several machines (or nodes). Each node processes a smaller portion of the data, and together, they complete the task far more efficiently than a single machine ever could.&lt;/p&gt;
&lt;h3 id=&#34;traditional-vs-distributed-ml-architectures&#34;&gt;Traditional vs. Distributed ML Architectures&lt;/h3&gt;
&lt;p&gt;In a traditional machine learning architecture, everything happens on one machine. You load the dataset into memory, preprocess it, train your model, and make predictions—all on a single machine. This setup works perfectly well for small datasets and relatively simple models, but as your data grows, so do the problems. The dataset might be too large to fit into memory, and the computation might become so slow that it takes days or weeks to train a model.&lt;/p&gt;
&lt;p&gt;In a distributed architecture, the dataset is partitioned across multiple machines. Each machine, or node, processes a subset of the data in parallel. The results are then aggregated to form a final model. The major difference here is the ability to scale horizontally. By adding more machines to the cluster, you can handle larger datasets and speed up computation.&lt;/p&gt;
&lt;p&gt;Frameworks like &lt;strong&gt;Apache Spark&lt;/strong&gt; make this process easier by managing the distribution of data and computation across a cluster of machines. Machine learning libraries like &lt;strong&gt;H2O&lt;/strong&gt; build on this, offering powerful, distributed ML algorithms that can handle huge amounts of data without being limited by a single machine&amp;rsquo;s resources.&lt;/p&gt;
&lt;h3 id=&#34;distributed-machine-learning-in-practice&#34;&gt;Distributed Machine Learning in Practice&lt;/h3&gt;
&lt;p&gt;Distributed machine learning is already widely used in industries that deal with massive amounts of data. For instance, streaming services like &lt;strong&gt;Netflix&lt;/strong&gt; use distributed machine learning to recommend movies and shows based on billions of user interactions. Similarly, social media platforms like &lt;strong&gt;Facebook&lt;/strong&gt; use distributed models to detect spam, suggest friends, and show targeted advertisements—all while processing vast amounts of real-time data.&lt;/p&gt;
&lt;p&gt;Take the example of a recommendation system at Netflix. When a user watches a movie or interacts with content, Netflix collects data to improve its recommendation engine. Since there are millions of users and countless data points, Netflix’s ML models need to process this data quickly. They rely on distributed systems to handle the training of these models, which would otherwise take an impractical amount of time on a single machine.&lt;/p&gt;
&lt;p&gt;Distributed ML is the key to scaling machine learning across industries where data is constantly growing and demands for real-time insights are increasing.&lt;/p&gt;
&lt;p&gt;Distributed machine learning offers several benefits that make it essential for handling large-scale data:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalability:&lt;/strong&gt; Distributed ML allows you to handle datasets that are too large to fit into the memory of a single machine. By distributing data and computations across multiple nodes, you can scale to whatever size your dataset requires.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Processing data in parallel across multiple machines means that training times are significantly reduced. This is especially important for iterative algorithms like gradient boosting, which require repeated passes over the data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fault Tolerance:&lt;/strong&gt; Distributed systems are often designed to be fault-tolerant. If one machine in the cluster fails, the system can redistribute the task to another machine without losing any data or progress, ensuring robustness in large-scale applications.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resource Optimization:&lt;/strong&gt; Distributed ML systems can dynamically allocate resources based on demand. For example, frameworks like Spark can add or remove nodes from a cluster, depending on the current computational load, making resource use more efficient.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;distributed-machine-learning-with-h2o-and-sparkling-water&#34;&gt;Distributed Machine Learning with H2O and Sparkling Water&lt;/h3&gt;
&lt;p&gt;To implement distributed machine learning, you&amp;rsquo;ll need tools that abstract the complexity of working with multiple machines while providing robust ML capabilities. This is where &lt;strong&gt;H2O&lt;/strong&gt; and &lt;strong&gt;Sparkling Water&lt;/strong&gt; come in.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;H2O&lt;/strong&gt; is an open-source, distributed machine learning platform that provides a wide range of scalable ML algorithms such as Random Forest, Gradient Boosting Machines (GBM), and Generalized Linear Models (GLM). H2O is designed to be highly scalable, able to run across multiple nodes in a cluster, allowing for both efficient model training and inference on large datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sparkling Water&lt;/strong&gt; is an extension of H2O that integrates with &lt;strong&gt;Apache Spark&lt;/strong&gt;. It combines the strengths of both systems: the scalability of Spark for large-scale data processing and the advanced machine learning algorithms provided by H2O. Sparkling Water allows you to build machine learning models on top of Spark&amp;rsquo;s distributed data structures, making it easy to apply distributed ML to big data problems.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By using H2O and Sparkling Water, you can take advantage of the best features of both platforms, enabling you to scale your machine learning models from your local machine to a distributed environment with minimal code changes. This makes it easier to handle the growing data demands of modern machine learning applications while leveraging the power of distributed systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Sparkling Water - Module 6.0</title>
      <link>https://dataqubed.io/teaching/pyspark-sparklingwater-part0/</link>
      <pubDate>Fri, 06 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-sparklingwater-part0/</guid>
      <description>&lt;h2 id=&#34;scalable-machine-learning-with-h2o-and-sparkling-water&#34;&gt;Scalable Machine Learning with H2O and Sparkling Water&lt;/h2&gt;
&lt;h3 id=&#34;1-introduction-to-distributed-machine-learning&#34;&gt;1. Introduction to Distributed Machine Learning&lt;/h3&gt;
&lt;p&gt;We&amp;rsquo;ll begin by exploring the core concepts behind distributed machine learning. This includes a look at how H2O and Sparkling Water fit into this space. You&amp;rsquo;ll understand H2O’s role in scaling ML models and how it integrates with Spark for large-scale data processing.&lt;/p&gt;
&lt;h4 id=&#34;2-dataset-selection&#34;&gt;2. Dataset Selection&lt;/h4&gt;
&lt;p&gt;To test the power of distributed computing, ideally we’ll work with a sizable dataset, but since we are sitting on Community edition of DataBricks we will skip using large datasets.&lt;/p&gt;
&lt;p&gt;We will load some dataset using H2O, and apply various data preprocessing techniques, exploring both traditional pandas/sklearn methods and H2O’s distributed capabilities.&lt;/p&gt;
&lt;h4 id=&#34;3-module-breakdown&#34;&gt;3. Module Breakdown&lt;/h4&gt;
&lt;h5 id=&#34;a-preprocessing&#34;&gt;a. &lt;strong&gt;Preprocessing:&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Begin by loading and exploring the dataset using H2O. From there, handle data cleaning, feature engineering, and scaling. Experiment with both local methods and H2O-specific techniques to compare workflows.&lt;/p&gt;
&lt;h5 id=&#34;b-model-building-local&#34;&gt;b. &lt;strong&gt;Model Building (Local):&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Train machine learning models like Random Forests or Gradient Boosting Machines using H2O locally. Track performance metrics like accuracy and F1-score, along with the time it takes to train each model.&lt;/p&gt;
&lt;h5 id=&#34;c-distributed-computing-with-sparkling-water&#34;&gt;c. &lt;strong&gt;Distributed Computing with Sparkling Water:&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Next, you’ll set up a Spark cluster, either locally or in the cloud, to train the same models on Sparkling Water. Compare the performance of your distributed models against those trained on your local machine, with an emphasis on training time and model accuracy.&lt;/p&gt;
&lt;h5 id=&#34;d-hyperparameter-tuning&#34;&gt;d. &lt;strong&gt;Hyperparameter Tuning:&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Apply H2O’s grid search to tune hyperparameters for your models. Evaluate how well the tuning process scales when done in a distributed setting via Sparkling Water.&lt;/p&gt;
&lt;h5 id=&#34;e-model-evaluation&#34;&gt;e. &lt;strong&gt;Model Evaluation:&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Assess the performance of your models based on accuracy, precision, and training time. You&amp;rsquo;ll analyze how distributed computing impacts both the quality and speed of training, especially when working with larger datasets.&lt;/p&gt;
&lt;h5 id=&#34;f-visualization--reporting&#34;&gt;f. &lt;strong&gt;Visualization &amp;amp; Reporting:&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;You’ll present your results through visualizations, comparing model performance between local and distributed setups. This will culminate in a final report summarizing your findings and reflecting on the advantages and challenges of distributed ML.&lt;/p&gt;
&lt;h4 id=&#34;bonus-automl-and-cloud-deployment&#34;&gt;Bonus: AutoML and Cloud Deployment&lt;/h4&gt;
&lt;p&gt;For those who want to dive deeper, you can experiment with H2O’s AutoML functionality, comparing its performance in both local and distributed environments. Additionally, deploying your Spark and H2O cluster on cloud platforms like Azure or GCP will give you a real-world glimpse into scalable machine learning.&lt;/p&gt;
&lt;h3 id=&#34;what-youll-learn&#34;&gt;What You’ll Learn&lt;/h3&gt;
&lt;p&gt;By the end of this project, you’ll have a practical understanding of distributed machine learning. You’ll not only develop scalable ML models using H2O and Sparkling Water, but also understand when to transition from local training to a distributed cluster for better efficiency.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Applying Conway&#39;s Law - A Blueprint for Successful Projects</title>
      <link>https://dataqubed.io/blog/a-blueprint-for-successful-student-projects/</link>
      <pubDate>Mon, 26 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/blog/a-blueprint-for-successful-student-projects/</guid>
      <description>&lt;p&gt;As you prepare for the kick off session of your project with your preferred and already identified client, there&amp;rsquo;s an important concept I&amp;rsquo;d like to introduce you to: &lt;code&gt;Conway&#39;s Law&lt;/code&gt;. Understanding this principle will not only help you navigate your project more effectively but also give you insights into how organizations and teams work in the real world.&lt;/p&gt;
&lt;h3 id=&#34;what-is-conways-law&#34;&gt;What is Conway&amp;rsquo;s Law?&lt;/h3&gt;
&lt;p&gt;Conway&amp;rsquo;s Law is a concept that was first introduced by Melvin Conway in 1968. It states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations.
— Melvin E. Conway, How Do Committees Invent?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In simpler terms, the way our team communicates and is organized will directly influence the design and outcome of the project we&amp;rsquo;re working on. If we have clear, efficient communication, our project is likely to be well-structured and cohesive. On the other hand, if our communication is disjointed or fragmented, that might reflect in the final product.&lt;/p&gt;
&lt;h3 id=&#34;why-should-you-care-about-conways-law&#34;&gt;Why Should You Care About Conway&amp;rsquo;s Law?&lt;/h3&gt;
&lt;p&gt;As students, you&amp;rsquo;re stepping into a project that has real-world implications and business impact. Our client is counting on us to deliver something valuable and functional. Understanding Conway&amp;rsquo;s Law can help us ensure that we meet (and hopefully exceed) their expectations. Here’s how:&lt;/p&gt;
&lt;h4 id=&#34;1-structuring-our-team-for-success&#34;&gt;1. Structuring Our Team for Success&lt;/h4&gt;
&lt;p&gt;Before we dive into the project, we need to think about how we’re going to structure our team. The way we organize ourselves can have a big impact on the project’s success:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Dividing and Conquering:&lt;/em&gt; If the project involves different components such as data preprocessing, model development, and algorithm optimization, it might make sense to break into smaller groups, each responsible for a specific part. This approach can help us manage the work more effectively and ensure that each part of the project is well-developed.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Collaboration Across Groups:&lt;/em&gt; Even if we’re divided into smaller teams, it’s important that we communicate and collaborate with each other regularly. This will help prevent any parts of the project from becoming isolated or disconnected from the whole.&lt;/p&gt;
&lt;h4 id=&#34;2-communication-is-key&#34;&gt;2. Communication is Key&lt;/h4&gt;
&lt;p&gt;Given the complexity of machine learning workflows, clear communication between team members working on different stages—like data handling, feature engineering, and model evaluation—is crucial. For our project to be successful, we need to ensure that our communication is clear, efficient, and consistent:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Regular Check-ins:&lt;/em&gt; We’ll have regular meetings to discuss progress, challenges, and next steps. These check-ins are crucial for keeping everyone on the same page and ensuring that we’re all moving in the same direction.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Clear Roles:&lt;/em&gt; Each of us will have specific roles and responsibilities within the project. Knowing who to go to for what will help streamline our communication and make sure that nothing falls through the cracks.&lt;/p&gt;
&lt;h4 id=&#34;3-working-with-the-client&#34;&gt;3. Working with the Client&lt;/h4&gt;
&lt;p&gt;Aligning our approach with the client&amp;rsquo;s expectations, especially in terms of the machine learning model&amp;rsquo;s performance and scalability, will be key to delivering a successful outcome. Our client is an important part of this project, and understanding how they work will help us deliver a product that meets their needs:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Aligning with the Client:&lt;/em&gt; We should take the time to understand the client’s organizational structure and how they communicate. If we can mirror their structure in our team, it might make it easier for us to collaborate and deliver something that fits seamlessly into their operations.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Engaging Stakeholders:&lt;/em&gt; We’ll make sure to involve the right people from the client’s side throughout the project. Their feedback is invaluable in making sure we’re on the right track.&lt;/p&gt;
&lt;h4 id=&#34;4-learning-and-growing&#34;&gt;4. Learning and Growing&lt;/h4&gt;
&lt;p&gt;This project isn’t just about delivering a great product—it’s also a learning experience. Understanding and applying Conway’s Law can help you become more effective team members and designers:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Reflecting on Our Work:&lt;/em&gt; As we work through the project, think about how our team’s structure and communication are affecting the outcome. After the project is completed, we’ll have a chance to reflect on what worked well and what we could do better next time.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Real-World Skills:&lt;/em&gt; The skills you’re developing by thinking about these concepts are directly applicable to any future projects or jobs you take on. Understanding how team dynamics influence project outcomes is a valuable skill in any career.&lt;/p&gt;
&lt;h4 id=&#34;5-stay-flexible&#34;&gt;5. Stay Flexible&lt;/h4&gt;
&lt;p&gt;Finally, remember that projects are dynamic, and we may need to adjust our structure or communication as we go:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Adapting to Challenges:&lt;/em&gt; If something isn’t working, we’ll adapt. Being flexible and open to change is key to overcoming challenges and finding innovative solutions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Continuous Improvement:&lt;/em&gt; Let’s use this project as an opportunity to continuously improve our teamwork and communication skills.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Conway’s Law is more than just a theory—it’s a practical tool that can help us succeed in this project. By paying attention to how we’re organized and how we communicate, we can produce a project that not only meets our client’s needs but also showcases our abilities as a team.&lt;/p&gt;
&lt;p&gt;Let’s keep these ideas in mind as we move forward, and I’m confident that we’ll create something we can all be proud of.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Shaping Your Success and Impact</title>
      <link>https://dataqubed.io/teaching/practicing-civility-can-enhance-your-leadership/</link>
      <pubDate>Sat, 24 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/practicing-civility-can-enhance-your-leadership/</guid>
      <description>&lt;h2 id=&#34;the-power-of-civility-shaping-your-success-and-impact&#34;&gt;The Power of Civility: Shaping Your Success and Impact&lt;/h2&gt;
&lt;p&gt;As you prepare to step into the professional world and take on real projects with companies, there&amp;rsquo;s one question that will shape your success more than any other: Who do you want to be? This isn&amp;rsquo;t just about your skills or knowledge—it&amp;rsquo;s rather more about &lt;code&gt;how you choose to show up every day&lt;/code&gt;, &lt;code&gt;how you treat the people around you&lt;/code&gt;, and &lt;code&gt;the environment you help create&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Watch to bellow TedTalk and remind yourselves of the powerful impact of civility—or the lack of it—on everything from &lt;code&gt;team performance&lt;/code&gt; to &lt;code&gt;personal well-being&lt;/code&gt;. You&amp;rsquo;ll hear compelling research and real-world stories that reveal just how critical it is to be mindful of our interactions. In the workplace and within your small study group, incivility isn’t just unpleasant; &lt;code&gt;it’s a productivity killer&lt;/code&gt;. It drains motivation, diminishes creativity, and even puts lives at risk in critical situations. But the good news is that civility pays off—in your &lt;code&gt;relationships&lt;/code&gt;, your &lt;code&gt;work&lt;/code&gt;, and your &lt;code&gt;overall success&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As you watch, think about the culture you want to foster in your teams and the kind of leader you aspire to be. The insights you&amp;rsquo;ll gain here can make all the difference in how you approach your projects, your teammates, and your career. Let this be the foundation for the journey ahead—one where respect, empathy, and thoughtful communication set you apart and drive you forward.&lt;/p&gt;
&lt;p&gt;
 on Ted.com,&lt;/p&gt;
&lt;p&gt;Or just Watch (Listen) to the video bellow:&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/kfcOK9yZkzs?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>BRD for Successful Project Execution</title>
      <link>https://dataqubed.io/teaching/business-requirement-document-brd/</link>
      <pubDate>Fri, 23 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/business-requirement-document-brd/</guid>
      <description>&lt;p&gt;Why I never say &lt;code&gt;Yes&lt;/code&gt; to a project without a BRD. I either drop it, or we have to agree to make one.&lt;/p&gt;
&lt;h2 id=&#34;importance-of-a-business-requirement-document-brd-for-successful-project-execution&#34;&gt;Importance of a Business Requirement Document (BRD) for Successful Project Execution&lt;/h2&gt;
&lt;p&gt;When working on data-heavy projects that involve developing machine learning models and directly interacting with key stakeholders, having a well-defined Business Requirement Document (BRD) is crucial. A BRD acts as a blueprint for the project, ensuring that all team members and stakeholders are aligned on the goals, scope, and deliverables. Without a BRD, projects can easily suffer from scope creep, miscommunication, and a lack of clear direction. This post will serve as a guideline for students on how to approach their projects using a BRD, which will be a critical tool throughout the year.&lt;/p&gt;
&lt;h2 id=&#34;why-a-brd-is-essential&#34;&gt;Why a BRD is Essential&lt;/h2&gt;
&lt;h3 id=&#34;1-reduces-scope-creep&#34;&gt;1. &lt;strong&gt;Reduces Scope Creep&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;A BRD helps to clearly define what is and isn&amp;rsquo;t part of the project. This clarity prevents additional, unplanned tasks from creeping into the project scope, which can derail timelines and resources.&lt;/p&gt;
&lt;h3 id=&#34;2-aligns-project-goals&#34;&gt;2. &lt;strong&gt;Aligns Project Goals&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;By outlining the project goals and getting sign-off from all stakeholders, the BRD ensures that everyone involved has the same understanding of the project&amp;rsquo;s objectives. This alignment is crucial for the project&amp;rsquo;s success.&lt;/p&gt;
&lt;h3 id=&#34;3-formalizes-project-efforts&#34;&gt;3. &lt;strong&gt;Formalizes Project Efforts&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;A BRD serves as a formal document that tracks the progress of the project. It provides a reference point for what has been agreed upon, making it easier to track milestones and hold team members accountable.&lt;/p&gt;
&lt;h2 id=&#34;steps-to-create-a-brd&#34;&gt;Steps to Create a BRD&lt;/h2&gt;
&lt;h3 id=&#34;1-sign-off-grid&#34;&gt;1. &lt;strong&gt;Sign-off Grid&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose:&lt;/strong&gt; To document agreement from all team members on the project goals and timeline.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action:&lt;/strong&gt; List each team member&amp;rsquo;s name, email, and the date they signed off on the BRD. This ensures that everyone is on the same page from the start.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-problem-statement&#34;&gt;2. &lt;strong&gt;Problem Statement&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose:&lt;/strong&gt; To clearly define the focus of the project.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action:&lt;/strong&gt; Write a concise statement that outlines the problem the project aims to solve.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-background--context&#34;&gt;3. &lt;strong&gt;Background &amp;amp; Context&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose:&lt;/strong&gt; To provide the necessary background information and define the scope of the project.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action:&lt;/strong&gt; Describe the context of the project, including what is in scope and what is not. This section should give all stakeholders a clear understanding of the project’s environment and limitations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;4-business-impact-metrics&#34;&gt;4. &lt;strong&gt;Business Impact Metrics&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose:&lt;/strong&gt; To set benchmarks and KPIs that will measure the project&amp;rsquo;s success.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action:&lt;/strong&gt; Define the metrics that will be used to assess the project&amp;rsquo;s impact. These should be quantifiable and directly linked to the project&amp;rsquo;s goals.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;5-business-requirements&#34;&gt;5. &lt;strong&gt;Business Requirements&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose:&lt;/strong&gt; To list all the features and deliverables that the project will produce.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action:&lt;/strong&gt; Document the key business requirements, prioritizing them based on their importance to the project&amp;rsquo;s success. Each requirement should have a clear description and rationale.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;6-key-dates&#34;&gt;6. &lt;strong&gt;Key Dates&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose:&lt;/strong&gt; To establish a timeline for the project.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action:&lt;/strong&gt; Define the key phases of the project and the dates by which deliverables are expected. This timeline will help manage expectations and keep the project on track.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;7-resources&#34;&gt;7. &lt;strong&gt;Resources&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Purpose:&lt;/strong&gt; To list any additional materials or references that are relevant to the project.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action:&lt;/strong&gt; Include links to documents, code files, or other resources that team members may need throughout the project.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;suggested-template-for-your-brd&#34;&gt;Suggested Template for Your BRD&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gh&#34;&gt;# Business Requirement Document (BRD)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gh&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gs&#34;&gt;**Project Name:**&lt;/span&gt; [Insert Project Name]  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gs&#34;&gt;**Status:**&lt;/span&gt; [Draft/Final]  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gs&#34;&gt;**Last Updated:**&lt;/span&gt; [Insert Date]  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gs&#34;&gt;**Authors:**&lt;/span&gt; [Insert Author Names]  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gs&#34;&gt;**Collaborators:**&lt;/span&gt; [Insert Collaborator Names]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;## Table of Contents
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;1.&lt;/span&gt; [&lt;span class=&#34;nt&#34;&gt;Sign-off Grid&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;#sign-off-grid&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;2.&lt;/span&gt; [&lt;span class=&#34;nt&#34;&gt;Problem Statement&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;#problem-statement&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;3.&lt;/span&gt; [&lt;span class=&#34;nt&#34;&gt;Background &amp;amp; Context&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;#background--context&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;4.&lt;/span&gt; [&lt;span class=&#34;nt&#34;&gt;Business Impact Metrics&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;#business-impact-metrics&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;5.&lt;/span&gt; [&lt;span class=&#34;nt&#34;&gt;Business Requirements&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;#business-requirements&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;6.&lt;/span&gt; [&lt;span class=&#34;nt&#34;&gt;Key Dates&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;#key-dates&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;7.&lt;/span&gt; [&lt;span class=&#34;nt&#34;&gt;Resources&lt;/span&gt;](&lt;span class=&#34;na&#34;&gt;#resources&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;## Sign-off Grid
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;| Team Member | Role | Date |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;| ----------- | ---- | ---- |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;| [Name] | [Role] | [Date] |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;| [Name] | [Role] | [Date] |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;## Problem Statement
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[Insert Problem Statement]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[min 2 - 4 sentences]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;## Background &amp;amp; Context
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[Insert Background &lt;span class=&#34;err&#34;&gt;&amp;amp;&lt;/span&gt; Context]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[Min 2 - 4 paragraphs describing the project background and context. Detail what is in scope and not in scope]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;## Business Impact Metrics
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[Insert Business Impact Metrics]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[1 - 2 paragraphs describing the metrics and benchmark to gauge project success]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;## Business Requirements
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;| Requirements | Priority | Notes |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;| ------------ | -------- | ----- |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;| [Requirement 1] | [P0/P1] | [Notes] |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;| [Requirement 2] | [P0/P1] | [Notes] |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;## Key Dates
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;| Phase | Artifact | Date | Owner | Status |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;| ----- | -------- | ---- | ------ | ------ |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;| [Phase 1] | [Artifact] | [Date] | [Owner] | [Status] |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;| [Phase 2] | [Artifact] | [Date] | [Owner] | [Status] |
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;## Resources
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;gu&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;*&lt;/span&gt; Technical Design Doc: &lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;link&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;*&lt;/span&gt; Related Docs: &lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;link&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;*&lt;/span&gt; Artifacts: &lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;link&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;an-example-of-business-requirements&#34;&gt;An example of Business Requirements&lt;/h2&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;Requirements&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;Priority&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;Notes&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;BRD:&lt;/strong&gt; Create a requirement doc that defines the business scope of the ML project&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;P0&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Data Pipeline:&lt;/strong&gt; Create a data pipeline that ingests, transforms, and loads data into the client’s data warehouse.&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;P0&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ML Model:&lt;/strong&gt; Create a model the client can use for churn model scoring.&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;P0&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ML Score:&lt;/strong&gt; Output churn scores across ~10M users in a daily batch.&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;P0&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;ML Monitoring:&lt;/strong&gt; Create a model monitoring service that checks for model drifts.&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;P1&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Feature Importance:&lt;/strong&gt; Display features that are strong predictors for user churn.&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;P1&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;an-example-of-key-dates&#34;&gt;An example of Key Dates&lt;/h2&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;Phase&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;Artifact&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;Date&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;Owner&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;Status&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Problem Scoping&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;BRD, Feasibility Analysis&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;07/18/24&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Mohsen, Elavendan&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Done&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Data Preparation&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Data Pipeline&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;07/30/24&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Jan, Mohsen&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;In Progress&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Model Development&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Benchmark Model&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;08/30/24&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Frank, Arash&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;In Progress&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Productionization&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Productionized Model with Batch Inference&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;09/15/24&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Zhanna, Dean&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Not Started&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Staging&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;09/30/24&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Bram, Alican&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Launch&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;10/15/24&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;By following this guideline and using the provided template, students can ensure that their projects are well-structured and have a clear path to success. The BRD is not just a formality but a strategic tool that helps to drive the project forward with clarity and purpose.&lt;/p&gt;
&lt;p&gt;Need to read more?&lt;br&gt;
Have a look at 
 published on Stanford University&amp;rsquo;s website. It provides an in-depth guide on creating a Business Requirements Document (BRD), outlining the process, benefits, and tips for successful implementation. This resource is invaluable for project teams looking to enhance clarity, efficiency, and communication throughout their project development and delivery.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 5.3</title>
      <link>https://dataqubed.io/teaching/pyspark-spark-streaming-part3/</link>
      <pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-spark-streaming-part3/</guid>
      <description>&lt;p&gt;While we experimented with setting up a Kafka service and accessing it programmatically, the solution provided in Module 5.2 does not leverage the capabilities of Spark and distributed computing as it focuses primarily on Python-based producers and consumers, without integrating Spark&amp;rsquo;s distributed computing power.&lt;/p&gt;
&lt;h2 id=&#34;integration-of-kafka-with-apache-spark-structured-streaming&#34;&gt;Integration of Kafka with Apache Spark Structured Streaming:&lt;/h2&gt;
&lt;h3 id=&#34;reading-data-from-kafka-in-spark-using-pyspark-for-streaming&#34;&gt;Reading Data from Kafka in Spark: Using PySpark for Streaming&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Configure Kafka Consumer: Use PySpark&amp;rsquo;s spark.readStream method to configure a Kafka consumer. Specify the Kafka bootstrap servers and the topic you want to read from.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Stream Data Processing: Define the schema for the incoming data and apply any necessary transformations using PySpark DataFrame operations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start Streaming: Use writeStream to output the results to a desired sink, like console, HDFS, or another Kafka topic, and start the streaming process.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Import 
 to the same folder, rename it to &lt;code&gt;Kafka-to-Delta Streaming Pipeline&lt;/code&gt;, and follow the instructions.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;Writing Data to Kafka from Spark!&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    How to write processed data back to Kafka topics using PySpark? This is a topic that I encourage motivated students to study it by themselves.
  &lt;/div&gt;
&lt;/details&gt;
&lt;h3 id=&#34;windowed-aggregations-performing-windowed-aggregations-on-streaming-data-from-kafka-using-pyspark&#34;&gt;Windowed Aggregations: Performing windowed aggregations on streaming data from Kafka using PySpark&lt;/h3&gt;
&lt;p&gt;Import 
 to the same folder and rename in to &lt;code&gt;Windowed Aggregations on Streaming Data&lt;/code&gt;. Follow the content and try to understand what is going on there.&lt;/p&gt;
&lt;h3 id=&#34;assignment&#34;&gt;Assignment&lt;/h3&gt;
&lt;p&gt;Import 
 to the same folder and look into the results. Can you explain what is the results you see in table &lt;code&gt;windowed_aggregations&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Whats next?&lt;/strong&gt; Module 5.4 (for year 2025-2026) will be dedicated to the best practices for kafka in databricks! Do you absolutely want to have it this year? Write Mohsen and email.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 5.2</title>
      <link>https://dataqubed.io/teaching/pyspark-spark-streaming-part2/</link>
      <pubDate>Wed, 21 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-spark-streaming-part2/</guid>
      <description>&lt;h2 id=&#34;python-based-producers-and-consumers&#34;&gt;Python-Based Producers and Consumers&lt;/h2&gt;
&lt;p&gt;Our Apache Kafka service is now accessible to the external world, and we&amp;rsquo;ve successfully conducted basic tests to ensure its functionality. The next step is to access Kafka programmatically, specifically using 
  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; height=&#34;1em&#34; viewBox=&#34;0 0 448 512&#34; fill=&#34;currentColor&#34;&gt;&lt;path d=&#34;M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; Python. We&amp;rsquo;ll utilize the Databricks Community Edition for this purpose, just as we have done in previous examples. However, it&amp;rsquo;s important to note that this approach is not limited to Databricks—you should be able to replicate these steps on other platforms (Google Colab, Amazon Sagemarker, Deepnote), ensuring flexibility and adaptability in various environments.&lt;/p&gt;
&lt;h3 id=&#34;reading-and-writing-frominto-a-specific-topic&#34;&gt;Reading and Writing from/into a specific topic&lt;/h3&gt;
&lt;p&gt;To spin up a cluster with the default configuration, follow these steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spin Up the Cluster&lt;/strong&gt;: Launch a cluster using your platform&amp;rsquo;s default settings. Ensure it meets the requirements for your tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Create a Folder in Databricks&lt;/strong&gt;: In your Databricks environment, navigate to the workspace and create a new folder named &amp;ldquo;Mod 5&amp;rdquo;. This will be the designated space for all the resources and scripts related to Module 5.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These steps will prepare your environment for further development and organization as you proceed with the tasks in Module 5.&lt;/p&gt;
&lt;p&gt;Import 
, rename it to &lt;code&gt;Producer&lt;/code&gt;, and follow the instructions.&lt;/p&gt;
&lt;p&gt;In a new window in the same folder import 
, rename it to &lt;code&gt;Consumer&lt;/code&gt;, and follow the instructions.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/kafka-db-cluster.png&#34; alt=&#34;Kafka&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Explain what is going on here. Why the Consumer has an entry (The bottom right screen of the image bellow)
















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/kafka-4-screens-3.png&#34; alt=&#34;4 shell for 4 services&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;kafka-producer-expects-json-serializable-data&#34;&gt;Kafka Producer Expects JSON-Serializable Data:&lt;/h3&gt;
&lt;p&gt;The Kafka producer takes as input JSON-serializable data, which can lead to errors if the provided data cannot be serialized properly. Below you will see a few examples of data type that the Kafka producer expects, along with explanations on how to correctly format and serialize this data to avoid common errors.&lt;/p&gt;
&lt;h4 id=&#34;sending-an-integer-value&#34;&gt;Sending an integer value&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;producer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;send&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;buas-data-n-ai-events&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;or-sending-type-information-as-a-string&#34;&gt;Or sending type information as a string&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;producer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;send&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;buas-data-n-ai-events&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;or-sending-a-json-object-with-type-information-and-value&#34;&gt;Or sending a JSON object with type information and value&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;producer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;send&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;buas-data-n-ai-events&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)),&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;create-a-topic&#34;&gt;Create a topic&lt;/h3&gt;
&lt;p&gt;Import 
 tot he same folder, rename it to &lt;code&gt;Topic Management and Verification in Databricks&lt;/code&gt;, and follow the instructions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 5.1</title>
      <link>https://dataqubed.io/teaching/pyspark-spark-streaming-part1/</link>
      <pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-spark-streaming-part1/</guid>
      <description>&lt;h2 id=&#34;deploying-apache-kafka-on-a-single-node&#34;&gt;Deploying Apache Kafka on a Single Node&lt;/h2&gt;
&lt;p&gt;Deploying Kafka on a single node is a great way to get started with understanding its core functionalities. This setup involves configuring a Kafka broker along with Zookeeper, which manages the broker&amp;rsquo;s metadata. In this guide, you&amp;rsquo;ll learn how to install, configure, and run Apache Kafka on a single node, making it accessible for testing and small-scale applications. This foundational setup will prepare you for more complex, distributed deployments.&lt;/p&gt;
&lt;h2 id=&#34;apache-kafka&#34;&gt;Apache Kafka&lt;/h2&gt;
&lt;p&gt;Apache Kafka is an open-source software streaming solution designed for handling real-time data feeds. It ensures events are stored in a durable, fault-tolerant manner, making it reliable for critical applications. Kafka enables the seamless streaming of messages to and from various endpoints, including databases, cloud services, and analytics platforms.&lt;/p&gt;
&lt;h3 id=&#34;key-features&#34;&gt;Key Features:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Durability&lt;/strong&gt;: Events are stored reliably for future use.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fault Tolerance&lt;/strong&gt;: Designed to handle failures without data loss.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Can manage large volumes of data across distributed systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/Kafka1.png&#34; alt=&#34;Kafka&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;For more insights, check out the YouTube video below.


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/06iRM1Ghr1k?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;
&lt;/p&gt;
&lt;h3 id=&#34;setting-up-apache-kafka-on-vultr&#34;&gt;Setting Up Apache Kafka on Vultr&lt;/h3&gt;
&lt;p&gt;To set up Apache Kafka on a Vultr instance, follow these steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deploy a Vultr Server&lt;/strong&gt;: Choose a suitable server size and region, then deploy a Linux instance (preferably Ubuntu).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Install Java&lt;/strong&gt;: Kafka requires Java to run, so install the latest version of OpenJDK.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Download and Install Kafka&lt;/strong&gt;: Download Kafka from the official Apache Kafka website and extract the files to your desired directory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Configure Kafka&lt;/strong&gt;: Modify the server properties (&lt;code&gt;server.properties&lt;/code&gt;) to match your setup. Set the broker ID, log directories, and Zookeeper address.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Start Kafka and Zookeeper&lt;/strong&gt;: Start the Zookeeper service first, followed by the Kafka server.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Test Kafka&lt;/strong&gt;: Use the provided Kafka tools to create topics, send messages, and consume them to ensure everything is working.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Before diving into the technical setup, it&amp;rsquo;s essential to understand why Vultr is a great choice for deploying Kafka.&lt;/p&gt;
&lt;h3 id=&#34;why-vultr&#34;&gt;Why Vultr?&lt;/h3&gt;
&lt;p&gt;Vultr offers high-performance cloud instances with SSD storage, low latency, and a global network of data centers, making it ideal for running distributed systems like Kafka. Additionally, Vultr&amp;rsquo;s flexible pricing, easy scalability, and user-friendly interface allow you to quickly deploy and manage your Kafka infrastructure, ensuring you have the resources you need as your data processing demands grow.&lt;/p&gt;
&lt;p&gt;While Vultr is an excellent choice for deploying Kafka, other cloud platforms like AWS, Google Cloud, and Azure also offer robust environments for running Kafka. These platforms provide managed services, such as Amazon MSK (Managed Streaming for Apache Kafka), that simplify Kafka deployment and management. Each platform has its strengths, such as extensive integration options, advanced networking, and security features. The choice ultimately depends on your specific needs, budget, and existing cloud infrastructure.&lt;/p&gt;
&lt;p&gt;In these series we will go ahead and implement a cost-effective Kafka deployment on Vultr. As your needs grow, Vultr&amp;rsquo;s easy scalability allows you to upgrade your instance or add more servers to your cluster. Additionally, using Vultr&amp;rsquo;s block storage for Kafka&amp;rsquo;s logs can provide both cost savings and performance improvements. Always monitor your resource usage to optimize costs effectively.&lt;/p&gt;
&lt;h3 id=&#34;a-cost-efficient-solution-on-vultr&#34;&gt;A Cost-Efficient Solution on Vultr&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Vultr VPS&lt;/strong&gt;: Start with a Vultr Virtual Private Server.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shared CPU&lt;/strong&gt;: Opt for a shared CPU plan to keep costs low.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Location&lt;/strong&gt;: Choose a data center location close to your target users for better latency.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Operating System&lt;/strong&gt;: Select Ubuntu 22.04 for its up-to-date features and stability. (In case you tried a another version of Ubuntu and followed the same process I would like to know if it worked or not!)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Instance Type&lt;/strong&gt;: Choose the &amp;ldquo;25 GB NVMe&amp;rdquo; AMD High-Performance machine without additional features like auto-backup, to minimize costs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSH Key&lt;/strong&gt;: Generate and upload an SSH key during VPS creation for secure and easy access.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This setup provides a balance of performance and cost, making it ideal for small to medium Kafka deployments.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/vultr-choose-plan.png&#34; alt=&#34;Kafka&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/vultr-additional-features.png&#34; alt=&#34;Kafka&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/vultr-deploy.png&#34; alt=&#34;Kafka&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/vultr-ubuntu-image-deployd.png&#34; alt=&#34;Kafka&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;For the installation process, we&amp;rsquo;ll largely follow the guidelines provided in the 
. This includes setting up Kafka with ZooKeeper, ensuring all services are started in the correct order, and making necessary configurations as needed.&lt;/p&gt;
&lt;h3 id=&#34;step-by-step-installation-guide&#34;&gt;Step-by-Step Installation Guide&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Start by SSHing into a machine:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ssh root@45.63.34.218
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;install Java&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;apt install openjdk-8-jre-headless
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;java -version
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Download Kafka version 2.13, unzip it and cd into the directory&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Start the ZooKeeper service&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;wget https://archive.apache.org/dist/kafka/2.6.0/kafka_2.13-2.6.0.tgz
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;tar -xzf kafka_2.13-2.6.0.tgz
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; kafka_2.13-2.6.0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;We next need to start a service called zookeeper. zookeeper is&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/zookeeper-server-start.sh config/zookeeper.properties
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Start the Kafka broker service. For that we need to open another terminal session and run:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-server-start.sh config/server.properties
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Paraphraze: Once all services have successfully launched, you will have a basic Kafka environment running and ready to use.&lt;/p&gt;
&lt;h3 id=&#34;create-a-topic&#34;&gt;Create a topic&lt;/h3&gt;
&lt;p&gt;Ww already know well that lets one read, write, store, and process events (also called records or messages in the documentation) across many machines. The events are organized and stored in topics. You may consider topics are like folder in a filesystem, and the events are the files in that folder.&lt;/p&gt;
&lt;p&gt;So before you can write your first events, you must create a topic. Open another terminal session and run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-topics.sh --create --topic buas-data-n-ai-events --bootstrap-server localhost:9092
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;command breakdown:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;lsquo;bin/kafka-topics.sh&amp;rsquo;: The kafka-topics.sh script is an executable and is used to manage Kafka topics. The file is located in the bin directory of your Kafka installation.&lt;/li&gt;
&lt;li&gt;&amp;lsquo;&amp;ndash;create&amp;rsquo;: This flag tells Kafka to create a new topic.&lt;/li&gt;
&lt;li&gt;&amp;lsquo;&amp;ndash;topic buas-data-n-ai-events&amp;rsquo;: This specifies the name of the topic you want to create. In this case, the topic is named buas-data-n-ai-events.&lt;/li&gt;
&lt;li&gt;&amp;lsquo;&amp;ndash;bootstrap-server localhost:9092&amp;rsquo;: Specifies the &amp;lsquo;Kafka server&amp;rsquo; to connect to in order to create the topic. localhost:9092 refers to a Kafka broker running on your local machine on port 9092. So now the client (which is executing the command) would connect to the &amp;lsquo;Kafka broker&amp;rsquo; running on local machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reading-and-writing-frominto-a-specific-topic&#34;&gt;Reading and Writing from/into a specific topic&lt;/h3&gt;
&lt;p&gt;A Kafka client communicates with the Kafka brokers (via the network) for reading or writing events.&lt;/p&gt;
&lt;p&gt;To write events into the topic run the following line in a new shell&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-console-producer.sh --topic buas-data-n-ai-events --bootstrap-server localhost:9092
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;Spark machine is killed 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;Frank to the rescue 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;from another terminal read the events:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-console-consumer.sh --topic buas-data-n-ai-events --from-beginning --bootstrap-server localhost:9092
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Spark machine is killed
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Frank to the rescue
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/kafka-4-screens.png&#34; alt=&#34;4 shell for 4 services&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;make-the-kafka-broker-accessible-externally&#34;&gt;Make the Kafka broker accessible externally&lt;/h3&gt;
&lt;p&gt;By now, you should have four shell windows open. To stop the running processes, press Ctrl + C and shutdown the services in a reverse order, starting from the last shell. This ensures that any changes made to the Kafka configuration or environment are properly loaded and active. We will then use the first shell (the one that we run the ZooKeeper service) to configure the broker to be accessible externally.&lt;/p&gt;
&lt;p&gt;To make the Kafka broker accessible externally we first need to edit the Kafka config file located at &amp;lsquo;config/server.properties&amp;rsquo;. ZooKeeper service by using the &amp;lsquo;Ctrl + C&amp;rsquo; and then edit the server.properties:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;vim config/server.properties
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;👉 Click to learn how to use wim&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    Google to learn!
  &lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Navigate to line 36, uncomment and replace &amp;lsquo;your.host.name&amp;rsquo; with &amp;lsquo;your host name&amp;rsquo;: For my case it will be &amp;lsquo;advertised.listeners=PLAINTEXT://155.138.192.245:9092&amp;rsquo;&lt;/p&gt;
&lt;p&gt;Then we need to make port 9092 accessible externally.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ufw allow &lt;span class=&#34;m&#34;&gt;9092&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# And check the status &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ufw status verbose
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/kafka-4-screens-2.png&#34; alt=&#34;4 shell for 4 services&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Now activate the services in the same order as before,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 1 - Restart the ZooKeeper service again.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/zookeeper-server-start.sh config/zookeeper.properties
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 2 - Restart the kafka server again. &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-server-start.sh config/server.properties
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-console-producer.sh --topic buas-data-n-ai-events --bootstrap-server localhost:9092
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 4 - Restart the kafka server again. &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;bin/kafka-console-consumer.sh --topic buas-data-n-ai-events --from-beginning --bootstrap-server localhost:9092
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/kafka-4-screens-3.png&#34; alt=&#34;4 shell for 4 services&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extra reading material:&lt;/strong&gt; Read through the 
. The article covers the main concepts of Kafka and compares it to other technologies. For a more detailed understanding of Kafka, refer to the official documentation.&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;👉 What is a Kafka server?&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    Kafka server
  &lt;/div&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;👉 What is a Kafka broker?&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    Kafka broker
  &lt;/div&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;👉 What is a Kafka client?&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    Kafka client
  &lt;/div&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;👉 How to ensure stability and reliability of running Kafka in the background?&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    &lt;p&gt;Using &lt;code&gt;systemd&lt;/code&gt; (or another init system like init.d) to manage Kafka as a service is generally the most stable and robust option. &lt;code&gt;Systemd&lt;/code&gt; is designed to manage system services, ensuring that services like Kafka start automatically on boot, restart on failure, and remain running independently of any user session.&lt;/p&gt;
&lt;p&gt;What we did in this tutorial runs Kafka in the foreground within the SSH session. If you close the SSH connection (e.g., by exiting the terminal or using Ctrl+C), the Kafka process will be terminated.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;systemctl start kafka   &lt;span class=&#34;c1&#34;&gt;# Start Kafka Service&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;systemctl &lt;span class=&#34;nb&#34;&gt;enable&lt;/span&gt; kafka  &lt;span class=&#34;c1&#34;&gt;# Ensures Kafka starts on boot&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;systemctl status kafka  &lt;span class=&#34;c1&#34;&gt;# Check the status of Kafka&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To be able to use &lt;code&gt;systemd&lt;/code&gt; you first need to create a kafka service file and configure it. Then you need to reload systemd daemon to recognize the new service.&lt;/p&gt;
&lt;p&gt;To create a systemd service file for Kafka use vim and place the file at &lt;code&gt;/etc/systemd/system/kafka.service&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo vim /etc/systemd/system/kafka.service
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the file, add the following content. Make sure to adjust paths to match the Kafka installation directory:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Unit&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;Apache Kafka Server
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Documentation&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;https://kafka.apache.org/documentation/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Requires&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;zookeeper.service
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;After&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;zookeeper.service
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Service&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;User&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kafka
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Group&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kafka
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;ExecStart&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/root/kafka_2.13-2.6.0/bin/kafka-server-start.sh /root/kafka_2.13-2.6.0/config/server.properties
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;ExecStop&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/root/kafka_2.13-2.6.0/bin/kafka-server-stop.sh
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Restart&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;on-failure
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;RestartSec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;TimeoutSec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;180&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Install&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;WantedBy&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;multi-user.target
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then reload systemd daemon before starting kafka service:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl daemon-reload
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We need to create a Zookeeper service file just like what we did for kafka server.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;wim /etc/systemd/system/zookeeper.service
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Unit&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Description&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;Apache Zookeeper Service
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Documentation&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;https://zookeeper.apache.org/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;After&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;network.target
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Service&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Type&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;simple
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;User&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;root
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Group&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;root
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;ExecStart&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/root/kafka_2.13-2.6.0/bin/zookeeper-server-start.sh /root/kafka_2.13-2.6.0/config/zookeeper.properties
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;ExecStop&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/root/kafka_2.13-2.6.0/bin/zookeeper-server-stop.sh
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;Restart&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;on-failure
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;RestartSec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;Install&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;WantedBy&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;multi-user.target
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl daemon-reload
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl start zookeeper
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo systemctl &lt;span class=&#34;nb&#34;&gt;enable&lt;/span&gt; zookeeper
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
  &lt;/div&gt;
&lt;/details&gt;
&lt;p&gt;Congratulations!
You have successfully finished the Apache Kafka installation. Now, your Kafka broker is up and running, ready to handle real-time data streams. This marks a significant milestone in setting up your data processing pipeline. Next, you can start creating topics, producing and consuming messages, and exploring Kafka’s powerful features to build scalable and resilient data systems. If you encounter any issues or need to expand your setup, consult the Kafka documentation or explore advanced configurations to optimize your deployment.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How to Deploy Your Hugo Site to GitHub Pages - A Simple Step-by-Step Guide</title>
      <link>https://dataqubed.io/blog/hugo-how-to/</link>
      <pubDate>Mon, 19 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/blog/hugo-how-to/</guid>
      <description>&lt;h1 id=&#34;how-to-deploy-your-hugo-site-to-github-pages-a-simple-step-by-step-guide&#34;&gt;How to Deploy Your Hugo Site to GitHub Pages: A Simple Step-by-Step Guide&lt;/h1&gt;
&lt;p&gt;Many guides online, including the Hugo documentation, tend to complicate the process of deploying a Hugo site to GitHub Pages. They often suggest creating multiple repositories or using submodules, which can be unnecessary. This guide aims to simplify the process, making it as straightforward as possible.&lt;/p&gt;
&lt;h3 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;Before you begin, ensure the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You have a GitHub account.&lt;/li&gt;
&lt;li&gt;You have a text editor installed.&lt;/li&gt;
&lt;li&gt;You have a fully functioning local Hugo site. This means you can run your site locally by using &lt;code&gt;hugo serve -D&lt;/code&gt; in your command prompt while in the Hugo site folder. This guide specifically covers hosting a local, working site on GitHub Pages.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;step-1-creating-the-github-repository&#34;&gt;Step 1: Creating the GitHub Repository&lt;/h3&gt;
&lt;p&gt;First, your local Hugo folder needs a place on GitHub.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Go to your GitHub profile page and navigate to &lt;strong&gt;Repositories &amp;gt; New&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Under &lt;strong&gt;Repository name&lt;/strong&gt;, enter the exact name of your Hugo site folder and click &lt;strong&gt;Create Repository&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;You’ll be directed to a page with an HTML link; copy this link.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;step-2-preparing-your-hugo-folder&#34;&gt;Step 2: Preparing Your Hugo Folder&lt;/h3&gt;
&lt;p&gt;Next, we need to adjust your local Hugo folder for web deployment.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Open your Hugo site folder in your preferred text editor.&lt;/li&gt;
&lt;li&gt;In the &lt;code&gt;config.toml&lt;/code&gt; file, add the following line at the top:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;publishDir&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;docs&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Find the line that says &lt;code&gt;baseURL = &amp;quot;https://example.org&amp;quot;&lt;/code&gt; and replace the URL with:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-toml&#34; data-lang=&#34;toml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nx&#34;&gt;baseURL&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;https://yourgithubname.github.io/yourgithubproject/&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;Make sure the URL follows this format exactly.&lt;/li&gt;
&lt;li&gt;Save the file.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;step-3-connecting-your-hugo-folder-to-the-github-repository&#34;&gt;Step 3: Connecting Your Hugo Folder to the GitHub Repository&lt;/h3&gt;
&lt;p&gt;Now it’s time to push your files to the GitHub repository.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Open Command Prompt and navigate to your Hugo site folder.&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;hugo serve&lt;/code&gt; to generate the &lt;code&gt;docs&lt;/code&gt; folder we configured earlier.&lt;/li&gt;
&lt;li&gt;Initialize an empty Git repository by entering &lt;code&gt;git init&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Add your GitHub repository as the remote origin with:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git remote add origin &amp;lt;paste the link you copied from the &lt;span class=&#34;s2&#34;&gt;&amp;#34;Creating the GitHub Repo&amp;#34;&lt;/span&gt; step&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Check the status of your files with &lt;code&gt;git status&lt;/code&gt;—you should see uncommitted items in red.&lt;/li&gt;
&lt;li&gt;Stage all your files with &lt;code&gt;git add --all&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Commit the files with:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git commit -m &lt;span class=&#34;s2&#34;&gt;&amp;#34;initial commit&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Push your files to GitHub with:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git push -u origin master
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;step-4-enabling-github-pages&#34;&gt;Step 4: Enabling GitHub Pages&lt;/h3&gt;
&lt;p&gt;Finally, let’s set up GitHub Pages to host your website.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In your GitHub repository, go to &lt;strong&gt;Settings&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Scroll down to &lt;strong&gt;GitHub Pages&lt;/strong&gt; and under &lt;strong&gt;Source&lt;/strong&gt;, select &lt;strong&gt;master branch /docs folder&lt;/strong&gt; and click &lt;strong&gt;Save&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Refresh the page and scroll back down to GitHub Pages—you should see a link to your new website!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And that’s it! Your Hugo site is now live on GitHub Pages.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 5.0</title>
      <link>https://dataqubed.io/teaching/pyspark-spark-streaming-part0/</link>
      <pubDate>Mon, 19 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-spark-streaming-part0/</guid>
      <description>&lt;h2 id=&#34;spark-streaming&#34;&gt;Spark Streaming&lt;/h2&gt;
&lt;p&gt;So far, we&amp;rsquo;ve covered the fundamentals of PySpark:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Big Data &amp;amp; Apache Spark:&lt;/strong&gt; Introduction to handling large datasets using Spark.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spark Architecture &amp;amp; RDDs:&lt;/strong&gt; Understanding Spark&amp;rsquo;s core structure and Resilient Distributed Datasets (RDDs) for parallel processing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DataFrames &amp;amp; Spark SQL:&lt;/strong&gt; Using DataFrames for structured data and Spark SQL for querying.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Sources &amp;amp; Sinks:&lt;/strong&gt; Connecting to and saving data across various formats.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These topics form the backbone of using PySpark for efficient, large-scale data processing.&lt;/p&gt;
&lt;p&gt;In this module, we will jump into real-time data processing and message brokering. We&amp;rsquo;ll explore streaming in Apache Spark, focusing on how to handle continuous data flows for real-time analytics. Additionally, we&amp;rsquo;ll cover Apache Kafka, a distributed messaging system that plays a crucial role in building robust data pipelines, enabling efficient data ingestion and real-time processing across various systems. These topics will help us manage and process data &lt;code&gt;in motion&lt;/code&gt;, a key aspect of modern data stack.&lt;/p&gt;
&lt;h3 id=&#34;why-stream-processing&#34;&gt;Why stream processing?&lt;/h3&gt;
&lt;p&gt;Stream processing is a powerful technique that enables real-time data handling and analytics. At its core, stream processing exploits &lt;code&gt;parallelism&lt;/code&gt;, allowing multiple data streams to be processed simultaneously. This reduces the need for &lt;code&gt;synchronization between components&lt;/code&gt;, making systems more efficient and scalable. As data is constantly in motion across cloud systems, stream processing ensures that insights can be derived in real-time, whether it’s for &lt;code&gt;real-time recommendations&lt;/code&gt;, &lt;code&gt;predictive maintenance&lt;/code&gt;, or other applications.&lt;/p&gt;
&lt;p&gt;However, stream processing isn’t without its challenges. Building &lt;code&gt;fault tolerance&lt;/code&gt; and &lt;code&gt;buffering mechanisms&lt;/code&gt; into software is crucial to ensure data reliability and smooth operations. Apache Kafka stands out as a leading platform for stream processing, offering robust capabilities for handling real-time data pipelines. Alongside Kafka, other solutions like &lt;code&gt;Apache Flume&lt;/code&gt;, &lt;code&gt;Apache Apex&lt;/code&gt;, and &lt;code&gt;Apache Storm&lt;/code&gt; provide diverse tools to tackle various streaming needs.&lt;/p&gt;
&lt;h3 id=&#34;the-dance-of-data-producers-and-consumers&#34;&gt;The Dance of Data: Producers and Consumers&lt;/h3&gt;
&lt;p&gt;In the world of stream processing, data flows like a continuous river, with &lt;code&gt;producers&lt;/code&gt; generating messages and &lt;code&gt;consumers&lt;/code&gt; retrieving them. Producers are the origin points, creating and sending data to various systems. On the other end, consumers actively listen, process, and utilize this data to drive decisions and actions. This dynamic interaction between producers and consumers is the heartbeat of real-time systems, enabling everything from real-time analytics to instantaneous user experiences.&lt;/p&gt;
&lt;h3 id=&#34;milestones-on-our-streaming-journey&#34;&gt;Milestones on Our Streaming Journey&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Deploying Apache Kafka on a Single Node&lt;/strong&gt; (Module 5.1)&lt;br&gt;
Learn how to set up a Kafka streaming platform, the backbone of our real-time data processing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Creating Python-Based Producers and Consumers&lt;/strong&gt; (Module 5.2)&lt;br&gt;
Implement Python processes to produce and consume messages, simulating real-world data flows.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Integrating PySpark Streaming in Databricks&lt;/strong&gt; (Module 5.3)&lt;br&gt;
Set up and run PySpark streaming tasks within Databricks, processing and analyzing streaming data in real time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div style=&#34;border:1px solid black; padding:10px;&#34;&gt;
  &lt;mark&gt;Note:&lt;/mark&gt; You can skip `Module 5.1` and `Module 5.2` and jump straight to `Module 5.3` if you desire to skip installing and configuring a Kafka service, though it is not recommended at all. 
&lt;/div&gt;
&lt;p&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Useful links</title>
      <link>https://dataqubed.io/blog/useful-links/</link>
      <pubDate>Mon, 19 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/blog/useful-links/</guid>
      <description>&lt;h2 id=&#34;useful-links&#34;&gt;useful links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;People&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Explore project ideas&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;[SideProject]https://www.reddit.com/r/SideProject/&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;Online courses (I&amp;rsquo;m not endorsing any of the courses/materials listed bellow. This is only a place for me to look at them at a later stage!)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 4</title>
      <link>https://dataqubed.io/teaching/pyspark-data-sources-and-sinks/</link>
      <pubDate>Sun, 18 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-data-sources-and-sinks/</guid>
      <description>&lt;h2 id=&#34;data-sources-and-sinks&#34;&gt;Data Sources and Sinks&lt;/h2&gt;
&lt;p&gt;By now, you should have a foundational understanding of Spark and its key concepts, including RDDs and lazy evaluation. We also learned how to use Databricks PySpark for creating and manipulating Spark DataFrames, and querying them with Spark SQL. We performed various DataFrame operations like selecting, filtering, and joining data. Additionally, we explored ways to handle semi-structured data.&lt;/p&gt;
&lt;p&gt;In this module we will study how to read and write from/into CSV, JSON, Parquet, and other formats. We will then continue by making jdbc connection for reading from / writing to databases.&lt;/p&gt;
&lt;h2 id=&#34;reading-and-writing-frominto-csv-json-parquet-and-other-formats&#34;&gt;Reading and writing from/into CSV, JSON, Parquet, and other formats&lt;/h2&gt;
&lt;p&gt;First watch this video bellow.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/ZBI1UFbEM4c?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Now clone 
 into your workspace. Lets put it into a new folder, lets say &lt;strong&gt;Module 4&lt;/strong&gt;  and rename it into &lt;strong&gt;01 - Reading and writing from-into&lt;/strong&gt; for consistency reasons. Follow the steps and cells. The material should be self explanatory.&lt;/p&gt;
&lt;h2 id=&#34;reading-form-and-writing-into-databases-using-jdbc&#34;&gt;Reading form and Writing into databases using JDBC&lt;/h2&gt;
&lt;p&gt;Watch the Youtube video bellow.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/oLIagnUAN2E?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Now clone 
 into your workspace. Rename the file it into &lt;strong&gt;02 - JDBC connector&lt;/strong&gt; for consistency reasons. Follow the steps and cells. The material should be self explanatory.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 3</title>
      <link>https://dataqubed.io/teaching/pyspark-dataframes-and-spark-sql/</link>
      <pubDate>Sat, 17 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-dataframes-and-spark-sql/</guid>
      <description>&lt;h2 id=&#34;module-3-dataframes-and-spark-sql-in-databricks&#34;&gt;Module 3: DataFrames and Spark SQL (in Databricks)&lt;/h2&gt;
&lt;p&gt;By now you have some understating of Spark and distributed computing. You know the RDDs, the importance of lazy evaluations as well as the reason why RDDs are fault tolerant.&lt;/p&gt;
&lt;p&gt;You also gained a bit of experience on how to install Spark on your local machine, how to test it and did a bit of coding in PySpark.&lt;/p&gt;
&lt;p&gt;Today we will learn a bit on DataBricks, what does it have to offer to us and how we can open a community edition of DataBricks. Next we wil run few notebooks and will make Spark Dataframes and will query them using Spark SQL.&lt;/p&gt;
&lt;h3 id=&#34;learning-outcomes&#34;&gt;Learning Outcomes&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Understanding DataBricks:&lt;/strong&gt; Gain an overview of what DataBricks is, including its features and benefits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Accessing DataBricks Community Edition:&lt;/strong&gt; Learn how to access and set up the Community Edition of DataBricks for hands-on practice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Working with Notebooks in DataBricks:&lt;/strong&gt; Learn how to create and run notebooks in DataBricks, which are essential for executing code and managing workflows.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Creating Spark DataFrames:&lt;/strong&gt; Understand how to create Spark DataFrames, a core component of working with data in Spark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Querying Data with Spark SQL:&lt;/strong&gt; Learn how to query data using Spark SQL, leveraging SQL-like syntax for data manipulation within Spark DataFrames.&lt;/p&gt;
&lt;h3 id=&#34;why-databricks&#34;&gt;Why DataBricks?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cloud-native: works on any major cloud provider&lt;/li&gt;
&lt;li&gt;Data storage: store a broad range of data including structured, unstructured and streaming&lt;/li&gt;
&lt;li&gt;Governance and management: in-built security controls and governance&lt;/li&gt;
&lt;li&gt;Tools: wide range of production-ready data tooling from engineering to BI, AI and ML&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://dataqubed.io/uploads/pyspark/whydb.png&#34; alt=&#34;Why DataBricks&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;With Databricks Community Edition, we would like to avoid installing new software and maintaining it. Also it gives us a ton of flexibility and features that we will see in the next sections.&lt;/p&gt;
&lt;p&gt;A non comprehensive list of benefits of DataBricks:&lt;/p&gt;
&lt;h4 id=&#34;1-scalability-and-performance&#34;&gt;1. Scalability and Performance&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Built on Apache Spark for efficient big data processing&lt;/li&gt;
&lt;li&gt;Seamlessly scales from small to large clusters&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-unified-analytics-platform&#34;&gt;2. Unified Analytics Platform&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Integrates data engineering, data science, and machine learning&lt;/li&gt;
&lt;li&gt;Simplifies workflows and enhances team collaboration&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;3-ease-of-use&#34;&gt;3. Ease of Use&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Interactive workspace with collaborative notebooks&lt;/li&gt;
&lt;li&gt;Supports multiple languages: Python, SQL, R, Scala&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;4-advanced-analytics-and-machine-learning&#34;&gt;4. Advanced Analytics and Machine Learning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Robust support for machine learning and AI&lt;/li&gt;
&lt;li&gt;Includes MLflow for experiment tracking and model management&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;5-data-lake-integration&#34;&gt;5. Data Lake Integration&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Supports Delta Lake for reliable data lakes&lt;/li&gt;
&lt;li&gt;Ensures ACID transactions and unifies batch and streaming data processing&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;6-cloud-integration&#34;&gt;6. Cloud Integration&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Seamless integration with AWS, Azure, and Google Cloud&lt;/li&gt;
&lt;li&gt;Leverages cloud-native features for security and cost efficiency&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;7-real-time-data-processing&#34;&gt;7. Real-Time Data Processing&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Handles real-time data streams for timely insights&lt;/li&gt;
&lt;li&gt;Essential for real-time analytics, monitoring, and fraud detection&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;8-security-and-compliance&#34;&gt;8. Security and Compliance&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Robust security features including data encryption and role-based access control&lt;/li&gt;
&lt;li&gt;Compliance with various industry standards&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;9-cost-efficiency&#34;&gt;9. Cost Efficiency&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Auto-scaling and optimized cluster management&lt;/li&gt;
&lt;li&gt;Efficient resource management reduces costs&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;10-community-and-ecosystem&#34;&gt;10. Community and Ecosystem&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Large and active community with strong support&lt;/li&gt;
&lt;li&gt;Extensive documentation and third-party integrations&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dataframes-and-spark-sql&#34;&gt;DataFrames and Spark SQL&lt;/h3&gt;
&lt;p&gt;Now that we are aware of some of the benefits of running spark on DataBricks env lets go ahead and open our first Databricks Community Edition account. To learn how to create your Databricks Community Edition account and activate it see section &lt;strong&gt;0. Prep&lt;/strong&gt; bellow.&lt;/p&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prep&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Before we dive into the detailed process of learning PySpark in the Databricks environment, I&amp;rsquo;d like you to watch this introductory video. It will provide you with a solid overview of what to expect and help you navigate the material more easily. Don&amp;rsquo;t worry about mastering everything in the video or practicing the content; it&amp;rsquo;s just a warm-up.&lt;/li&gt;
&lt;/ul&gt;

   
       
       &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
         &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/lcI1W2_KUPo?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
         &gt;&lt;/iframe&gt;
       &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;[Introduction to Databricks notebooks] (
)&lt;/li&gt;
&lt;li&gt;[Manage notebooks] (
)&lt;/li&gt;
&lt;li&gt;[Develop code in Databricks notebooks] (
)&lt;/li&gt;
&lt;li&gt;[Databricks notebook interface and controls] (
)&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;
, and see the bellow youtube.&lt;/li&gt;
&lt;/ul&gt;

   
       
       &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
         &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/Op_KZJm7_qc?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
         &gt;&lt;/iframe&gt;
       &lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Now you know the essential procedures to work in DataBricks environment. Now head towards the 
 notebook, clone it into your workspace and follow the steps. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;00 - Test the dev env&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Now clone 
 into your workspace. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;01 - Data Prep&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: As you already know a good practice in programming is to modularize your code. Following the best practices here I have created a notebook called 
. Clone that into you workspace, rename it to &lt;strong&gt;02 - Functions&lt;/strong&gt;, try to read through it, and make adjustments as needed.&lt;/p&gt;
&lt;p&gt;Next, we will explore the fundamentals of DataFrames in Databricks. We’ll cover how to create and manipulate DataFrames, perform basic operations, and leverage DataFrames for data analysis. This session will guide you through using Spark SQL for querying and transforming data, demonstrating practical examples of DataFrame operations.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Introduction to DataFrames&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Clone 
. Rename the file to &lt;strong&gt;03 - Introduction to DataFrames&lt;/strong&gt;. Go through the material and try to make notes. The notebook covers few topics including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Difference between RDDs and DataFrames&lt;/li&gt;
&lt;li&gt;Creating DataFrames&lt;/li&gt;
&lt;li&gt;Schema inference and manual schema definition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Next we learned how to perform basic DataFrame operations including selecting, filtering, and transforming data using PySpark. These operations are fundamental for data manipulation and analysis. We will then learn how to perform aggregations and group data using PySpark. We covered how to group by single and multiple columns, apply multiple aggregate functions, and filter results after grouping.&lt;/p&gt;
&lt;p&gt;We will then dive into working with joins and unions in Spark using Databricks. We&amp;rsquo;ll explore how to combine data from multiple DataFrames using different types of joins, including inner, left, right, and full outer joins. Additionally, we&amp;rsquo;ll cover how to use unions to append data from one DataFrame to another. These operations are fundamental for integrating and manipulating datasets in Spark, making them essential skills for any data engineer or analyst working in a distributed data environment.&lt;/p&gt;
&lt;p&gt;Finally, in this module, we will learn few tips and tricks on how to work with semi-structured data. The focus is on handling JSON data, including how to read, write, and manipulate JSON files using PySpark. You will learn techniques to efficiently process semi-structured data, including using Spark&amp;rsquo;s &lt;code&gt;explode&lt;/code&gt; function to flatten nested data and leveraging &lt;code&gt;schema inference&lt;/code&gt; to automatically detect data structures. The module provides practical examples and exercises to solidify your understanding, ensuring you can effectively manage complex data formats in your projects.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DataFrame Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Selecting, filtering, and transforming data: Follow 
. Referencing rename the file to &lt;strong&gt;04 - DataFrame Operations&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Aggregations and Grouping: Follow 
. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;05 - Aggregations and Grouping&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Joins and unions: Follow 
. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;06 - Joins and Unions&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Working with Semi-Structured Data: Follow 
. For consistency in the learning material and easy referencing rename the file to &lt;strong&gt;07 - Working with semi-structured data&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spark SQL&lt;/strong&gt; (todo: coming soon)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SQL queries with Spark&lt;/li&gt;
&lt;li&gt;Registering DataFrames as tables&lt;/li&gt;
&lt;li&gt;Using SQL queries to manipulate DataFrames&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;&lt;p&gt;&lt;strong&gt;Assignment: Visualizations in Databricks notebooks&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Databricks has built-in support for charts and visualizations in both Databricks SQL and in notebooks. Based on the data prepared in &lt;strong&gt;01 - Data Prep&lt;/strong&gt; make few visualizations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details:&lt;/strong&gt; I will provide you with a git repo and you will be tasked place your notebook there. The repo will not accept new entries after the deadline. Please deliver both the Databricks &lt;strong&gt;.dbc&lt;/strong&gt; archive file format and &lt;strong&gt;.HTML&lt;/strong&gt; format. Make sure to give them identical filenames. Also make sure to put the name of the team and team members in the first cell of the notebook.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deadline:&lt;/strong&gt; Your notebook must be submitted by Sep 29 at 23:59 CET. Please adhere strictly to this deadline.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reading material&lt;/strong&gt;: Need some reading materials? Visit 
.&lt;/p&gt;
&lt;/span&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 2</title>
      <link>https://dataqubed.io/teaching/pyspark-spark-architecture-and-rdds/</link>
      <pubDate>Fri, 16 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-spark-architecture-and-rdds/</guid>
      <description>&lt;h2 id=&#34;spark-architecture-and-rdds&#34;&gt;Spark Architecture and RDDs&lt;/h2&gt;
&lt;h3 id=&#34;a-gentle-intro-to-spark-architecture&#34;&gt;A gentle intro to Spark Architecture&lt;/h3&gt;
&lt;p&gt;Look at the Youtube video bellow carefully.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/vJ0eUZxF80s?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;the above video explores and explains the architecture of Apache Spark, emphasizing the roles of the &lt;em&gt;driver&lt;/em&gt; and &lt;em&gt;executors&lt;/em&gt; in both &lt;em&gt;local&lt;/em&gt; and &lt;em&gt;cluster&lt;/em&gt; mode applications. The driver manages the application and can run on the local machine or within a cluster. In client mode, the driver operates locally, while in cluster mode, it starts in the Application Master container, requesting containers from the cluster manager (like Apache YARN) to launch executors. The video explains how Spark can operate in local mode, where everything runs in a single JVM, or in cluster mode, where the driver and executors communicate via the Application Master, with resource allocation handled by the cluster manager. A demonstration shows starting a Spark application in cluster mode with specific executors and utilizing YARN&amp;rsquo;s dynamic allocation feature to release idle executors.&lt;/p&gt;
&lt;p&gt;The video bellow covers how Spark breaks down and executes applications at the executors. It covers the internal workings of Apache Spark, focusing on how it processes code in parallel. The video covers how Spark reads data, processes it, and writes results back to a destination. Three key data structures in Spark namely &lt;em&gt;DataFrames&lt;/em&gt;, &lt;em&gt;Datasets&lt;/em&gt;, and &lt;em&gt;RDDs&lt;/em&gt; (Resilient Distributed Datasets) are presented. Although the emphasis is on DataFrames and Datasets, RDDs are crucial as they form the underlying structure to which DataFrames and Datasets are compiled. RDDs are &lt;strong&gt;resilient&lt;/strong&gt;, &lt;strong&gt;partitioned&lt;/strong&gt;, &lt;strong&gt;distributed&lt;/strong&gt;, and &lt;strong&gt;immutable collections&lt;/strong&gt; of data that can be created from a source or transformed from another RDD.&lt;/p&gt;
&lt;p&gt;The video demonstrates how to create an RDD from a file and control the number of partitions. It explains the difference between transformations and actions in Spark—transformations are lazy operations that create new distributed datasets without sending results back to the driver, while actions are non-lazy and send results back to the driver. It also touches on topics such as Spark&amp;rsquo;s way of handling grouping and counting operations on distributed data, which involves repartitioning the data and triggering shuffle and sort activities. The number of parallel tasks in Spark is influenced by the number of partitions and available executors. Concept of functional programming in Spark is briefly touched on as well.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/fyTiJLKEzME?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h3 id=&#34;rdds&#34;&gt;RDDs&lt;/h3&gt;
&lt;p&gt;We have seen the RDDs and their properties. If you need a refresher go ahead and see the Youtube video bellow.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/nH6C9vqtyYU?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;As we have seen RDDS are closely linked to the concepts of Lazy evaluation and resilience. When you are done download &lt;strong&gt;01-Creating-RDDs.ipynb&lt;/strong&gt; from 
 and follow the instruction and run it locally. Make sure you could run all the cells. The notebook should have sufficient comments in it and that should provide you with some hints about the code.&lt;/p&gt;
&lt;p&gt;The html version of the notebook is available here 
.&lt;/p&gt;
&lt;p&gt;Afterwards download &lt;strong&gt;02-RDD-Lineage-n-Persistence.ipynb&lt;/strong&gt; from 
 and run it locally. The html version of the notebook is available here 
 for immediate access.&lt;/p&gt;
&lt;h3 id=&#34;transformations-and-actions-in-spark&#34;&gt;Transformations and Actions in spark&lt;/h3&gt;
&lt;p&gt;Now continue watching the Youtube video bellow entitled &amp;ldquo;Spark RDD Transformations and Actions&amp;rdquo;. The video should give you a sample of various Transformations and Actions in spark.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/5L0oyrwiNNE?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Download &lt;strong&gt;03-Transformations-n-Actions.ipynb&lt;/strong&gt; from 
 and run it locally. The html version of the notebook is available here 
 for immediate access.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 1</title>
      <link>https://dataqubed.io/teaching/pyspark-introduction-to-big-data-and-apache-spark/</link>
      <pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-introduction-to-big-data-and-apache-spark/</guid>
      <description>&lt;h2 id=&#34;introduction-to-big-data-and-apache-spark&#34;&gt;Introduction to Big Data and Apache Spark&lt;/h2&gt;
&lt;p&gt;In today&amp;rsquo;s data-driven world, organizations generate and process vast amounts of data, often referred to as &amp;ldquo;Big Data&amp;rdquo;. In some literature Big Data is characterized by its large volume, high velocity, and wide variety, making traditional data processing tools inadequate for handling such complex datasets. To address these challenges, specialized tools and frameworks have been developed.&lt;/p&gt;
&lt;p&gt;One of the most powerful tools for processing Big Data is Apache Spark, an open-source, distributed computing system that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Spark is widely used in big data processing due to its ability to perform in-memory computations, which significantly speeds up processing times compared to traditional disk-based methods.&lt;/p&gt;
&lt;p&gt;Apache Spark supports a variety of data processing tasks, including batch processing, stream processing, and machine learning, making it a versatile choice for various big data applications. For Python developers, PySpark offers a powerful and intuitive API to leverage Spark&amp;rsquo;s capabilities. PySpark allows developers to write Python code that seamlessly integrates with Spark&amp;rsquo;s distributed computing framework, enabling efficient processing of large datasets across clusters.&lt;/p&gt;
&lt;p&gt;Before diving deeper into the concepts of PySpark and distributed computing, it&amp;rsquo;s crucial to first understand the fundamentals of Big Data and why specialized tools like Apache Spark are essential for handling it effectively.&lt;/p&gt;
&lt;h2 id=&#34;what-is-big-data&#34;&gt;What is Big Data?&lt;/h2&gt;
&lt;p&gt;See the video bellow:&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/bAyrObl7TYE?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;and also the one bellow:&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/WEqVfo6nJuU?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;We discussed the five key categories of big data (&lt;strong&gt;Volume&lt;/strong&gt;, &lt;strong&gt;Velocity&lt;/strong&gt;, &lt;strong&gt;Variety&lt;/strong&gt;, &lt;strong&gt;Veracity&lt;/strong&gt;, and &lt;strong&gt;Value&lt;/strong&gt;) and provided some examples of how big data can be used in different ways. We also understood that big data is transforming industries across the board. Whether it&amp;rsquo;s improving patient care in healthcare, optimizing supply chains in retail, or enhancing fraud detection in banking, the impact of big data is undeniable.&lt;/p&gt;
&lt;p&gt;Next we will continue to understand the basics of Spark and Spark ecosystem.&lt;/p&gt;
&lt;h2 id=&#34;what-is-apache-spark&#34;&gt;What is Apache Spark?&lt;/h2&gt;
&lt;p&gt;Before diving into this tutorial, I recommend watching the bellow YouTube video entitles &amp;ldquo;An Introduction to PySpark&amp;rdquo;. This video serves as an excellent introduction to the world of PySpark and distributed computing. It’s a compact guide that walks you through the basics of Apache Spark, comparing it with Pandas, positioning it in the distributed computing ecosystem, and how PySpark fits into the picture.&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t panic if you don’t grasp all the details right away, that’s perfectly okay! The goal is to give you a general sense of what Spark is, why it’s important, and how it can be used to process large datasets efficiently. This foundational knowledge will make the hands-on sections of this tutorial much more meaningful as you start applying what you’ve learned.&lt;/p&gt;
&lt;p&gt;So, take some time to watch the video—absorb as much as you can—and then come back here to deepen your understanding. We will review a lot of this content again and again, not only in a different context but also from a various angels.&lt;/p&gt;
&lt;h3 id=&#34;a-short-summary-of-what-the-video-covers&#34;&gt;A short summary of what the video covers:&lt;/h3&gt;
&lt;p&gt;The talk covers what PySpark is, its capabilities compared to Pandas, and when it’s necessary to use it. It highlights the benefits of PySpark for distributed data processing, explaining Spark&amp;rsquo;s map and reduce transformations and the advantages of lazy evaluation. The decision-making process for choosing between PySpark and other tools is also discussed, considering factors like data volume, existing codebases, team skills, and personal preferences. Throughout, code examples are provided, along with an emphasis on the usefulness of window functions and addressing concerns like the underlying Java in the Python library and potential errors during evaluation.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/ZFLOMSuWHxg?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Now that we have a high-level understanding of Big Data, its business value, and a basic understanding of Spark and PySpark—along with how PySpark compares and contrasts with Pandas—let’s dive into some practical implementations. We’ll begin by installing Spark and PySpark locally on our machines.&lt;/p&gt;
&lt;h3 id=&#34;pyspark-installation&#34;&gt;PySpark installation&lt;/h3&gt;
&lt;p&gt;After you are done with the videos above, go ahead and install PySpark. 
 will give you the instruction on how to install PySpark on WSL but the process should remain almost the same when it comes to the installation on other OSs.&lt;/p&gt;
&lt;p&gt;For installing PySpark on your windows machine follow the instruction of the bellow Youtube video.&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/WxhPDK4ffq4?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;Visit here for a detailed instructions on how to install PySpark on different platforms. You will also learn how to configure your environment to work with PySpark effectively.&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Do you know a simpler and better instruction? Please share your experience.&lt;/span&gt;
&lt;/div&gt;
&lt;h3 id=&#34;check-the-installation&#34;&gt;Check the installation&lt;/h3&gt;
&lt;p&gt;Now lets check if your installation is complete. Download the file &lt;strong&gt;00-Check-your-spark-installation.ipynb&lt;/strong&gt; from 
 and follow the instruction. Make sure you could run all the cells. The notebook should have sufficient comments in it and that should provide you with some hints about the code.&lt;/p&gt;
&lt;p&gt;The html version of the notebook is available here 
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 1-1</title>
      <link>https://dataqubed.io/teaching/pyspark-python-and-jupyter-notebook-instalation-on-wsl-ubuntu-22-04/</link>
      <pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-python-and-jupyter-notebook-instalation-on-wsl-ubuntu-22-04/</guid>
      <description>&lt;p&gt;In this guide, we&amp;rsquo;ll walk you through the steps to set up Apache Spark, Python, and Jupyter Notebook on a freshly installed WSL Ubuntu 22.04. You&amp;rsquo;ll also learn how to make Jupyter Notebook accessible from the host machine.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A Windows machine with WSL installed.&lt;/li&gt;
&lt;li&gt;WSL set to run Ubuntu 22.04.&lt;/li&gt;
&lt;li&gt;Basic knowledge of command-line operations.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s get started!&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;step-1-install-python-and-pip&#34;&gt;Step 1: Install Python and Pip&lt;/h2&gt;
&lt;p&gt;WSL comes with Python pre-installed. To ensure you have the latest version of Python and pip, update your packages:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt update &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo apt upgrade -y
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt install python3-pip -y
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Verify that Python and pip are installed correctly:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python3 --version
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip3 --version
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;step-2-install-java&#34;&gt;Step 2: Install Java&lt;/h2&gt;
&lt;p&gt;Apache Spark requires Java to run. We&amp;rsquo;ll install OpenJDK:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo apt install openjdk-11-jdk -y
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Verify the Java installation:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;java -version
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You should see output indicating that Java 11 is installed.&lt;/p&gt;
&lt;h2 id=&#34;step-3-download-and-install-apache-spark&#34;&gt;Step 3: Download and Install Apache Spark&lt;/h2&gt;
&lt;p&gt;Now, let&amp;rsquo;s download and install Apache Spark. We&amp;rsquo;ll be using Spark version 3.5.2 with Hadoop 3:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;wget https://downloads.apache.org/spark/spark-3.5.2/spark-3.5.2-bin-hadoop3.tgz
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Extract the downloaded file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;tar xvf spark-3.5.2-bin-hadoop3.tgz
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo mv spark-3.5.2-bin-hadoop3 /opt/spark
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;step-4-set-up-environment-variables&#34;&gt;Step 4: Set Up Environment Variables&lt;/h2&gt;
&lt;p&gt;To make Spark available globally, we need to set up the necessary environment variables. Open your .bashrc file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;vim ~/.bashrc
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Add the following lines at the end of the file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Spark environment variables&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;SPARK_HOME&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/opt/spark
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;PATH&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$PATH&lt;/span&gt;:&lt;span class=&#34;nv&#34;&gt;$SPARK_HOME&lt;/span&gt;/bin:&lt;span class=&#34;nv&#34;&gt;$SPARK_HOME&lt;/span&gt;/sbin
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Save and exit the file. Exit the VM and run it again to load the changes.&lt;/p&gt;
&lt;p&gt;To verify that Spark is set up correctly, run:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;spark-shell
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This should launch the Spark shell.&lt;/p&gt;
&lt;h2 id=&#34;step-5-install-jupyter-notebook&#34;&gt;Step 5: Install Jupyter Notebook&lt;/h2&gt;
&lt;p&gt;Now that we have Python and Spark installed, we can set up Jupyter Notebook:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip3 install notebook
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;step-6-configure-jupyter-notebook-for-remote-access&#34;&gt;Step 6: Configure Jupyter Notebook for Remote Access&lt;/h2&gt;
&lt;p&gt;To access Jupyter Notebook from the host machine, you&amp;rsquo;ll need to set up a configuration that allows remote access.&lt;/p&gt;
&lt;p&gt;Generate the Jupyter Notebook configuration file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;jupyter notebook --generate-config
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, edit the config file (You might need to exit the vm and relaunch it again):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;vim ~/.jupyter/jupyter_notebook_config.py
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Add the following lines,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;c.NotebookApp.ip &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;0.0.0.0&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;c.NotebookApp.open_browser &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; False
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;c.NotebookApp.port &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;8888&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you want to add password protection for your Jupyter Notebook, you can generate a hashed password using the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;python3 -c &lt;span class=&#34;s2&#34;&gt;&amp;#34;from notebook.auth import passwd; print(passwd())&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Copy the output and add it to your Jupyter configuration file as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;c.NotebookApp.password &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;lt;your-hashed-password&amp;gt;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Save the changes.&lt;/p&gt;
&lt;h2 id=&#34;step-7-launch-jupyter-notebook&#34;&gt;Step 7: Launch Jupyter Notebook&lt;/h2&gt;
&lt;p&gt;Start Jupyter Notebook using the following command:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;jupyter notebook
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can now access Jupyter Notebook from your host machine by visiting http://localhost:8888 in your browser.&lt;/p&gt;
&lt;h2 id=&#34;step-8-verify-the-pyspark-setup&#34;&gt;Step 8: Verify the PySpark Setup&lt;/h2&gt;
&lt;p&gt;To ensure PySpark is set up correctly, you can create a new notebook in Jupyter and try the following code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;!&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pip&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pyspark&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pyspark&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pyspark.sql&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SparkSession&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;spark&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SparkSession&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;builder&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;appName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Test&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;getOrCreate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;spark&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;createDataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;foo&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;bar&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;id&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;value&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If everything is set up correctly, you should see a small DataFrame printed in the notebook.&lt;/p&gt;
&lt;h2 id=&#34;troubleshooting&#34;&gt;Troubleshooting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If Jupyter Notebook does not start or is not accessible, ensure that your firewall allows access to port 8888.&lt;/li&gt;
&lt;li&gt;Make sure WSL is configured properly, and networking is enabled between WSL and your Windows host.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>PySpark - Module 0</title>
      <link>https://dataqubed.io/teaching/pyspark-fundamentals-and-advanced-topics/</link>
      <pubDate>Wed, 14 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/pyspark-fundamentals-and-advanced-topics/</guid>
      <description>&lt;h3 id=&#34;pyspark-fundamentals-and-advanced-topics&#34;&gt;PySpark Fundamentals and Advanced Topics&lt;/h3&gt;
&lt;p&gt;Distributed computing is at the heart of all the recent advancements of Data science and AI models. We can now &lt;code&gt;scale out&lt;/code&gt; as an alternative to &lt;code&gt;scale up&lt;/code&gt;. And that makes it possible to retrieve data, prepare the data and train our models not only at a faster speed, but also cheaper.&lt;/p&gt;
&lt;p&gt;One of the pioneer commercial players on the filed of distributed computing is DataBricks. This the company behind the open source Spark community.&lt;/p&gt;
&lt;h4 id=&#34;course-objective&#34;&gt;Course Objective&lt;/h4&gt;
&lt;p&gt;In today&amp;rsquo;s data-driven world, the volume of data generated is growing exponentially. Traditional data processing systems struggle to handle this deluge efficiently. Distributed computing frameworks like Apache Spark have emerged as powerful tools for processing large datasets quickly and efficiently. Understanding Spark and PySpark (the Python API for Spark) is crucial for data professionals who want to leverage the full potential of big data. This course is designed to equip you with the skills to process and analyse large datasets using Apache Spark and PySpark, emphasizing its importance in modern data science and machine learning workflows. By the end of this course, you will have a good understanding of how to process, analyze, and manipulate large datasets efficiently. You will also gain hands-on experience in building machine learning models and optimizing Spark queries for performance.&lt;/p&gt;
&lt;p&gt;Over 30 hours of content is prepared that would help you develop a solid understanding of distributed computing, Spark&amp;rsquo;s architecture, and how to apply Spark for big data processing. The course includes a mix of theoretical concepts and hands-on exercises, leveraging free online resources.&lt;/p&gt;
&lt;h4 id=&#34;course-highlights&#34;&gt;Course Highlights&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fundamentals of PySpark&lt;/strong&gt;: Learn the basics of PySpark, including its architecture, Resilient Distributed Datasets (RDDs), and DataFrames.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Processing and SQL&lt;/strong&gt;: Master the use of DataFrames and Spark SQL for data manipulation and querying.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming and Real-Time Data&lt;/strong&gt;: Understand how to process real-time data using Spark Streaming.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Machine Learning&lt;/strong&gt;: Explore machine learning techniques and build models using Spark&amp;rsquo;s MLlib (for year 2025 - 2026).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimization Techniques&lt;/strong&gt;: Learn how to optimize Spark applications for better performance and efficiency (for year 2025 - 2026).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capstone Project&lt;/strong&gt;: Apply your knowledge to a real-world project, demonstrating your ability to handle big data problems from start to finish (for year 2025 - 2026).&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;learning-outcomes&#34;&gt;Learning Outcomes&lt;/h4&gt;
&lt;p&gt;By the end of this course, you will be able to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Understand and explain the architecture and components of Apache Spark.&lt;/li&gt;
&lt;li&gt;Install and configure PySpark on a Linux environment (WSL).&lt;/li&gt;
&lt;li&gt;Work with RDDs and DataFrames for data processing and analysis.&lt;/li&gt;
&lt;li&gt;Use Spark SQL to run queries and manipulate structured data.&lt;/li&gt;
&lt;li&gt;Process streaming data with Spark Streaming.&lt;/li&gt;
&lt;li&gt;Build and evaluate machine learning models using MLlib.&lt;/li&gt;
&lt;li&gt;Optimize Spark applications for improved performance.&lt;/li&gt;
&lt;li&gt;Apply your skills to a small capstone project.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;key-questions-answered&#34;&gt;Key Questions Answered&lt;/h4&gt;
&lt;p&gt;Throughout the course, you will be able to answer questions such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the key components of Apache Spark, and how do they interact?&lt;/li&gt;
&lt;li&gt;How can you create and manipulate RDDs and DataFrames in PySpark?&lt;/li&gt;
&lt;li&gt;How do you use Spark SQL to perform data queries and transformations?&lt;/li&gt;
&lt;li&gt;What techniques are available for processing real-time data streams in Spark?&lt;/li&gt;
&lt;li&gt;How can you build and deploy machine learning models using Spark&amp;rsquo;s MLlib?&lt;/li&gt;
&lt;li&gt;What strategies can be employed to optimize Spark applications for performance?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;time-commitment&#34;&gt;Time Commitment&lt;/h4&gt;
&lt;p&gt;This course is designed to be completed over approximately 30 hours of self-paced study. Here is a breakdown of the estimated time required for each module:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Module 1: Introduction to Big Data and Apache Spark&lt;/strong&gt;: 3 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 2: Spark Architecture and RDDs&lt;/strong&gt;: 5 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 3: DataFrames and Spark SQL&lt;/strong&gt;: 6 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 4: Data Sources and Sinks&lt;/strong&gt;: 4 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 5: Spark Streaming&lt;/strong&gt;: 12 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 6: Machine Learning with PySpark&lt;/strong&gt;: 0 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 7: Advanced Spark Techniques&lt;/strong&gt;: 0 hours&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Module 8: Real-World Applications and Capstone Project&lt;/strong&gt;: 0 hours&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This structured approach ensures that you will have ample time to grasp each concept thoroughly, practice through hands-on exercises, and apply what you have learned to real-world scenarios.&lt;/p&gt;
&lt;h4 id=&#34;getting-started&#34;&gt;Getting Started&lt;/h4&gt;
&lt;p&gt;To begin, make sure you can install and configure PySpark. Familiarize yourself with the course materials, including the recommended readings and online resources. Each module might  contain practical assignments and quizzes to reinforce your learning, so take your time to complete these exercises when available.&lt;/p&gt;
&lt;h3 id=&#34;course-outline-pyspark-fundamentals-and-advanced-topicshttpsdataqubedioteachingpyspark-fundamentals-and-advanced-topics&#34;&gt;Course Outline: 
&lt;/h3&gt;
&lt;h4 id=&#34;module-1-introduction-to-big-data-and-apache-sparkhttpsdataqubedioteachingpyspark-introduction-to-big-data-and-apache-spark&#34;&gt;
&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Introduction to Big Data&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;What is Big Data?&lt;/li&gt;
&lt;li&gt;Challenges of Big Data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overview of Apache Spark&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;History and evolution&lt;/li&gt;
&lt;li&gt;Components of the Spark ecosystem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Installing PySpark&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;System requirements&lt;/li&gt;
&lt;li&gt;Installing PySpark on a Linux environment (WSL)&lt;/li&gt;
&lt;li&gt;Configuring PySpark and Jupyter Notebook&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-2-spark-architecture-and-rddshttpsdataqubedioteachingpyspark-spark-architecture-and-rdds&#34;&gt;
&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Spark Architecture&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Spark driver and executors&lt;/li&gt;
&lt;li&gt;SparkContext and SparkSession&lt;/li&gt;
&lt;li&gt;Cluster managers (Standalone, YARN, Mesos, Kubernetes)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Resilient Distributed Datasets (RDDs)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;What are RDDs?&lt;/li&gt;
&lt;li&gt;Creating RDDs&lt;/li&gt;
&lt;li&gt;Transformations and Actions&lt;/li&gt;
&lt;li&gt;RDD Lineage and Persistence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-3-dataframes-and-spark-sqlhttpsdataqubedioteachingpyspark-dataframes-and-spark-sql&#34;&gt;
&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Introduction to DataFrames&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Difference between RDDs and DataFrames&lt;/li&gt;
&lt;li&gt;Creating DataFrames&lt;/li&gt;
&lt;li&gt;Schema inference and manual schema definition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DataFrame Operations&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Selecting, filtering, and transforming data.&lt;/li&gt;
&lt;li&gt;Aggregations and Grouping&lt;/li&gt;
&lt;li&gt;Joins and unions&lt;/li&gt;
&lt;li&gt;Working with semi-structured data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spark SQL&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SQL queries with Spark&lt;/li&gt;
&lt;li&gt;Registering DataFrames as tables&lt;/li&gt;
&lt;li&gt;Using SQL queries to manipulate DataFrames&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-4-data-sources-and-sinkshttpsdataqubedioteachingpyspark-data-sources-and-sinks&#34;&gt;
&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Reading Data&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Reading from CSV, JSON, Parquet, and other formats&lt;/li&gt;
&lt;li&gt;Reading from databases (JDBC)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Writing Data&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Writing to CSV, JSON, Parquet, and other formats&lt;/li&gt;
&lt;li&gt;Writing to databases (JDBC)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data Sources API&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Introduction to the Data Sources API&lt;/li&gt;
&lt;li&gt;Custom data sources&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-5-spark-streaming&#34;&gt;Module 5: Spark Streaming&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Introduction to Spark Streaming&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;What is Spark Streaming?&lt;/li&gt;
&lt;li&gt;DStreams and micro-batching&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming Sources&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Reading from Kafka, Socket, and other sources&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Streaming Operations&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Transformations on DStreams&lt;/li&gt;
&lt;li&gt;Windowed operations and stateful computations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fault Tolerance and Checkpointing&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Handling fault tolerance in streaming applications&lt;/li&gt;
&lt;li&gt;Checkpointing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-6-machine-learning-with-pyspark-0-hours&#34;&gt;Module 6: Machine Learning with PySpark (0 hours)&lt;/h4&gt;
&lt;ol start=&#34;0&#34;&gt;
&lt;li&gt;hyperparameter optimization framework designed for both single-machine and distributed setups: Optuna and Hyperopt&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Introduction to MLlib&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Overview of MLlib&lt;/li&gt;
&lt;li&gt;Data preprocessing with MLlib&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Building Machine Learning Models&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Supervised learning algorithms (e.g., Linear Regression, Logistic Regression, Decision Trees)&lt;/li&gt;
&lt;li&gt;Unsupervised learning algorithms (e.g., K-means clustering)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Evaluation and Hyperparameter Tuning&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Evaluating model performance&lt;/li&gt;
&lt;li&gt;Cross-validation and hyperparameter tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pipeline API&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Building ML pipelines&lt;/li&gt;
&lt;li&gt;Using feature transformers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Case study: Predictive modeling with MLlib&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-7-advanced-spark-techniques-0-hours&#34;&gt;Module 7: Advanced Spark Techniques (0 hours)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Optimizing Spark Applications&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Understanding Spark execution plan&lt;/li&gt;
&lt;li&gt;Catalyst optimizer&lt;/li&gt;
&lt;li&gt;Tungsten execution engine&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Performance Tuning&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Caching and persistence strategies&lt;/li&gt;
&lt;li&gt;Memory management and garbage collection&lt;/li&gt;
&lt;li&gt;Shuffle operations and optimization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Debugging and Monitoring&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Using Spark UI&lt;/li&gt;
&lt;li&gt;Logging and metrics&lt;/li&gt;
&lt;li&gt;Handling and avoiding common pitfalls&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-8-real-world-applications-and-capstone-project-0-hours&#34;&gt;Module 8: Real-World Applications and Capstone Project (0 hours)&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Case Studies and Real-World Applications&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Example projects using PySpark&lt;/li&gt;
&lt;li&gt;Industry use cases&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capstone Project&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Define a problem statement&lt;/li&gt;
&lt;li&gt;Design and implement a PySpark solution&lt;/li&gt;
&lt;li&gt;Optimize and present findings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;module-9-advanced-features&#34;&gt;Module 9: Advanced Features&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Working with GraphX for graph processing&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;learning-resources&#34;&gt;Learning Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Documentation and Tutorials&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Apache Spark official documentation&lt;/li&gt;
&lt;li&gt;PySpark API reference&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Books and Online Courses&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Learning PySpark&amp;rdquo; by Tomasz Drabas and Denny Lee&lt;/li&gt;
&lt;li&gt;Online courses on platforms like Coursera, Udacity, and edX&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Community and Support&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;GitHub repositories for sample projects&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;assessment&#34;&gt;Assessment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Quizzes&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;You will be provided with quizzes to reinforce learning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Capstone Project&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Depending on your track (Data Science or Data Engineering) you will do a small project that requires max a day to complete and deliver.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Participation in quizzes is mandatory, and they contribute to your final score.&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;You are offered the opportunity to earn extra credit through the following two deliverables, which are designed to reinforce and showcase your understanding of the material. Please pay close attention to the instructions and deadlines.&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;&lt;p&gt;&lt;strong&gt;Deliverable 1:&lt;/strong&gt; Cheat Sheet on Material Covered Up to Module 5.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; Create a concise and well-organized cheat sheet that summarizes the key concepts, formulas, and methodologies you have learned up to and including Module 5.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Format:&lt;/strong&gt; You may submit your cheat sheet in one of the following formats:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Jupyter Notebook (.ipynb)&lt;/li&gt;
&lt;li&gt;A Databricks Notebook&lt;/li&gt;
&lt;li&gt;A well-structured PDF file (maximum of 2 pages).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you choose to submit a PDF, you must also include the original file format (e.g., the source Jupyter Notebook or Databricks Notebook from which the PDF was generated).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deadline:&lt;/strong&gt; Your cheat sheet must be submitted by October 11 at 23:59 CET. Please adhere strictly to this deadline.&lt;/p&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;&lt;p&gt;&lt;strong&gt;Deliverable 2:&lt;/strong&gt; To Be Announced&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Details:&lt;/strong&gt; The second deliverable will be announced shortly. I assure you that it will not require more than a full day of work for a single individual.&lt;/p&gt;
&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;By the end of this material you will be able to provide answers to the questions bellow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Describe the PySpark architecture.&lt;/li&gt;
&lt;li&gt;What are RDDs in PySpark?&lt;/li&gt;
&lt;li&gt;Explain the concept of lazy evaluation in PySpark.&lt;/li&gt;
&lt;li&gt;How does PySpark differ from Apache Hadoop?&lt;/li&gt;
&lt;li&gt;What are DataFrames in PySpark?&lt;/li&gt;
&lt;li&gt;How do you initialize a SparkSession?&lt;/li&gt;
&lt;li&gt;What is the significance of the SparkContext?&lt;/li&gt;
&lt;li&gt;Describe the types of transformations in PySpark.&lt;/li&gt;
&lt;li&gt;How do you read a CSV file into a PySpark DataFrame?&lt;/li&gt;
&lt;li&gt;What are actions in PySpark, and how do they differ from transformations?&lt;/li&gt;
&lt;li&gt;How can you filter rows in a DataFrame?&lt;/li&gt;
&lt;li&gt;Explain how to perform joins in PySpark.&lt;/li&gt;
&lt;li&gt;How do you aggregate data in PySpark?&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;li&gt;How can you handle missing or null values in PySpark?&lt;/li&gt;
&lt;li&gt;How do you repartition a DataFrame, and why?&lt;/li&gt;
&lt;li&gt;Describe how to cache a DataFrame. Why is it useful?&lt;/li&gt;
&lt;li&gt;How do you save a DataFrame to a file?&lt;/li&gt;
&lt;li&gt;Explain the concept of partitioning in PySpark.&lt;/li&gt;
&lt;li&gt;How can broadcast variables improve performance?&lt;/li&gt;
&lt;li&gt;What are accumulators, and how are they used?&lt;/li&gt;
&lt;li&gt;How does PySpark handle data skewness?&lt;/li&gt;
&lt;li&gt;Explain how checkpointing works in PySpark.&lt;/li&gt;
&lt;li&gt;What is delta lake? Look at 
 and 
!&lt;/li&gt;
&lt;li&gt;What is data lakehouse architecture.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Explainable boosting machine</title>
      <link>https://dataqubed.io/blog/explainable-boosting-machine/</link>
      <pubDate>Mon, 05 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/blog/explainable-boosting-machine/</guid>
      <description>&lt;link rel=&#34;canonical&#34; href=&#34;https://dataqubed.io/blog/explainable-boosting-machine/&#34; /&gt;
&lt;blockquote&gt;
&lt;p&gt;You can have data without information, but you cannot have information without data.&lt;/p&gt;
&lt;p&gt;Daniel Keys Moran, science fiction writer&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The business impact of machine learning models is becoming increasingly significant whether it is in marketing campaign budget allocation or credit risk management. However, the lack of interpretability and explainability of many machine learning models makes them not only difficult to &lt;em&gt;trust&lt;/em&gt; but also difficult to &lt;em&gt;debug&lt;/em&gt;. When the end user can comprehend the decision-making process of a machine learning model, they are more likely to accept, trust, and adopt the model&amp;rsquo;s outputs, giving rise to reliable model-based decisions.&lt;/p&gt;
&lt;p&gt;The easiest way to ensure interpretability is to use interpretable models. Consequently, there is a growing interest in the development and utilization of interpretable machine learning models.&lt;/p&gt;
&lt;p&gt;Explainable Boosting Machine (EBM) originally developed at Microsoft Research is an inherently interpretable machine learning model that gained increasing popularity in a variety of business applications due to its predictive performance and low computational cost. It falls into a category of ML models known as &amp;ldquo;Glass box&amp;rdquo;. Not only it can capture non-linear relationships between dependent and independent variables, but it also has a built-in mechanism to capture interactions among independent variables. at its heart, it uses some of the well-known ML techniques like bagging and gradient boosting.&lt;/p&gt;
&lt;p&gt;Explainable Boosting Machine is a special case of ensemble models, which involve training multiple very simple decision trees and combining their predictions to obtain a final prediction.&lt;/p&gt;
&lt;p&gt;Imagine we are given a dataset with &lt;em&gt;n&lt;/em&gt; features. The first decision tree is trained only on the first feature. The residuals are then calculated and we continue by training another model but this time only using the second feature. The process continues until the last model is trained in a boosting fashion using the last feature. The order of the features will not impact the results since we ensure to keep the learning rate very small. Now we have the &lt;em&gt;first iteration&lt;/em&gt; completed. We continue the process for many many iterations, let&amp;rsquo;s say 10,000 iterations. At the end of the cycle, we have 10,000 small trees trained on feature 1 that can be summarized as a graph. We can extract a graph for all the features. These series of graphs with size &lt;em&gt;n&lt;/em&gt; construct our model.&lt;/p&gt;
&lt;p&gt;The ability to model complex relationships and interactions while maintaining explainability makes the Explainable Boosting Machine a powerful tool for a wide range of predictive modeling tasks.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;dataqubed.io EBM Boosting machine&#34;
           src=&#34;https://dataqubed.io/blog/explainable-boosting-machine/dataqubed_ebm_davarynejad.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion:&lt;/h3&gt;
&lt;p&gt;You probably heard about the trade-off between explainability and accuracy of machine learning models. The more accurate the model is, the more complex it is and less explainable. EBMs violate this trade-off. With EBMs you will get high accuracy while getting a highly explainable model with an added bonus of editability. This makes EBMs the perfect candidate of choice for many real-world applications that support model-based decisions.&lt;/p&gt;
&lt;h3 id=&#34;inherently-interpretable-ml-models&#34;&gt;Inherently interpretable ML models&lt;/h3&gt;
&lt;p&gt;Interested in this topic? Read more on &amp;ldquo;How to design inherently interpretable machine learning models&amp;rdquo; by Sudjianto and Zhang (2021)&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In my next post, I should follow up on the fairness, weakness detection, reliability, robustness, and resilience of XAI.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Unlocking Entrepreneurial Success</title>
      <link>https://dataqubed.io/teaching/unlocking-entrepreneurial-success/</link>
      <pubDate>Sat, 03 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/teaching/unlocking-entrepreneurial-success/</guid>
      <description>&lt;p&gt;As you prepare to have a head start in your third year, where you&amp;rsquo;ll dive into exciting projects with real companies, there&amp;rsquo;s no better time to get inspired by someone who&amp;rsquo;s mastered the art of entrepreneurship. Steven Bartlett’s podcast on Spotify features a conversation with Marc Randolph, the former CEO of Netflix, and it&amp;rsquo;s packed with insights that could reshape how you approach your own projects.&lt;/p&gt;
&lt;p&gt;Marc Randolph has been through the highs and lows of building a business from the ground up. In this podcast, he shares his journey and the tough lessons he’s learned along the way. One of his core beliefs? Every idea is bad—until you prove otherwise. This mindset is not just about being skeptical; it’s about understanding that success comes from testing, refining, and relentlessly improving your ideas.&lt;/p&gt;
&lt;p&gt;Listening to this podcast will give you a fresh perspective on how to tackle the challenges you&amp;rsquo;ll face this year. It’s a must-hear for anyone serious about turning ideas into reality. Get ready to learn from one of the best, and start your year with the motivation to make your projects truly exceptional.&lt;/p&gt;
&lt;iframe style=&#34;border-radius:12px&#34; src=&#34;https://open.spotify.com/embed/episode/1B2NLib3h8733YbcX5ofNm/video?utm_source=generator&amp;theme=0&#34; width=&#34;100%&#34; frameBorder=&#34;0&#34; allowfullscreen=&#34;&#34; allow=&#34;autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture&#34; loading=&#34;lazy&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;This is  your 
  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; height=&#34;1em&#34; viewBox=&#34;0 0 448 512&#34; fill=&#34;currentColor&#34;&gt;&lt;path d=&#34;M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; Python free day. So enjoy it!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>11 must-know Python Pandas tips and tricks for data scientists</title>
      <link>https://dataqubed.io/blog/pandas-tips-and-tricks-for-data-scientists/</link>
      <pubDate>Thu, 01 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/blog/pandas-tips-and-tricks-for-data-scientists/</guid>
      <description>&lt;link rel=&#34;canonical&#34; href=&#34;https://dataqubed.io/blog/pandas-tips-and-tricks-for-data-scientists/&#34; /&gt;
&lt;blockquote&gt;
&lt;p&gt;You can have data without information, but you cannot have information without data.&lt;/p&gt;
&lt;p&gt;Daniel Keys Moran, science fiction writer&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Pandas is considered the most widely used tool for data manipulation, filtering, and wrangling. It comes with enormous features and functionalities designed for fast and easy data analytics. As a data scientist, I use its features on a daily basis, and in this post, I’d love to share with you some of the tricks of pandas.&lt;/p&gt;
&lt;p&gt;I hope that this crafted list of features serves as a quick reference and gives you a good starting point for learning Pandas. The listed features come in no specific order, so no conclusions can be made on the order.&lt;/p&gt;
&lt;p&gt;I will be using the NVDA ticker price data set for this post. So before going into the feature lists, I’ll share with you a few lines of code to download the data locally.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;flex justify-center	&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://cdn.hashnode.com/res/hashnode/image/upload/v1706247367147/77d9c9e5-3598-4c6e-8361-889f3740408d.png&#34; alt=&#34;Davarynejad NVDA price&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pull the NVDA exchange pricing data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We will be using Yahoo! Finance&amp;rsquo;s API to download some market data. For that, we will be using the yfinance Python package. Follow 
. I installed it using the following single line of code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;err&#34;&gt;!&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pip&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;install&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;yfinance&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;os&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pickle&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;yfinance&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;yf&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s first pull the historical NVDA exchange rate from the Yahoo! Finance&amp;rsquo;s API.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;get_yf_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ticker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;period&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;1mo&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&amp;#39;Pull and cache yfinance data into a dataframe. Caching is important for avoiding rate limits.&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;pkl_file_name&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ticker&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;replace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;_&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;_&amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;period&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;cache_path&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;.pkl&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pkl_file_name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;replace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;-&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;try&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# Check if the pickle file is in the working directory.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cache_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;ticker_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pickle&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;   
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Loaded &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; from cache&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ticker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;except&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;ne&#34;&gt;OSError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ne&#34;&gt;IOError&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;e&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# The data is not locally avaiable. Download it and store it in a form of pickle file. &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Downloading &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; from yfinance&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ticker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;ticker_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;yf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;download&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ticker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;period&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;period&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;ticker_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;to_pickle&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cache_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Cached &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt; at &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{}&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ticker&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cache_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ticker_df&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Pull NVDA and AAPL price from exchange data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_yf_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;NVDA&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;period&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;10d&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# If you would like to download more tickers you try the following:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_yf_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;NVDA APP TSLA&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;period&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;1mo&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# The above will download the Nvidia, Apple and Tesla share prices. &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tail&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now that we have the data ready at hand, let&amp;rsquo;s have a look at my selection of Pandas tips and tricks.&lt;/p&gt;
&lt;h3 id=&#34;number-11-apply-aggregations-on-dataframe-and-series&#34;&gt;Number 11: Apply aggregations on DataFrame and Series&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;agg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;min&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;max&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;High&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;agg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;mean&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;min&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;max&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;.describe()&lt;/code&gt; provides the same functionality but &lt;code&gt;.agg()&lt;/code&gt; is more flexible as far as I can tell.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;describe&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;number-10-all-you-need-is-pandas_profiling&#34;&gt;Number 10: All you need is pandas_profiling&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;pandas_profiling&lt;/code&gt; is a Python library mainly used for initial data exploration. A running example:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Make sure that you have the pandas_profiling installed.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# To install pandas_profiling run: !pip install pandas_profiling&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas_profiling&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Chart the NVDA pricing data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;pandas_profiling&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ProfileReport&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This helps you with a few lines of code and time-saving tricks when exploring the data. The &lt;code&gt;pandas_profiling&lt;/code&gt; library comes with more features than the one listed above. Interested readers are referred to the official website.&lt;/p&gt;
&lt;p&gt;The package provides an extensive exploratory data analysis (EDA) that includes reporting on different types of correlations, such as Pearson&amp;rsquo;s r and Phik coefficient (φk). Phik (φk) is a new correlation coefficient that works consistently between categorical, ordinal, and interval variables. Phik can capture non-linear dependency and revert to the Pearson correlation coefficient in the case of the bivariate normal input distribution. Read more 
.&lt;/p&gt;
&lt;h3 id=&#34;number-9-easy-way-to-convert-data-toint-type&#34;&gt;&lt;strong&gt;Number 9: Easy way to convert data to&lt;/strong&gt;&lt;code&gt;int&lt;/code&gt; type&lt;/h3&gt;
&lt;p&gt;This is a simple but useful trick! When converting the data into &lt;code&gt;int&lt;/code&gt; type, the most common approach is to use &lt;code&gt;.astype(&#39;int&#39;)&lt;/code&gt;. However, if there are strings or other invalid types, you will face an error, and the conversion will fail. An alternative is to use the 
&lt;code&gt;_numeric()&lt;/code&gt; command which will handle the errors. You may try this for yourself.&lt;/p&gt;
&lt;h3 id=&#34;number-8-break-up-strings-in-multiple-columns&#34;&gt;Number 8: Break up strings in multiple columns&lt;/h3&gt;
&lt;p&gt;A simple trick for splitting a column into multiple columns using a split function:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;column_name&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;column_name&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;expand&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;number-7-str&#34;&gt;Number 7: .str&lt;/h3&gt;
&lt;p&gt;I’d like to show you how awesome &lt;code&gt;.str&lt;/code&gt; is with &lt;code&gt;Pandas Series&lt;/code&gt;. Check the following piece of code:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Covert the Data column to str.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Date_str&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;astype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# And take the first 4 elements of the resulting string. &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Year_str&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Date_str&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It&amp;rsquo;s awesome how they make this bit of magic work.&lt;/p&gt;
&lt;h3 id=&#34;number-6-concatenate-strings&#34;&gt;Number 6: Concatenate strings&lt;/h3&gt;
&lt;p&gt;It’s so easy with the pandas to create a new column by combining two existing columns:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;ts&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;values&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;astype&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;int64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;//&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;9&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;price_ts&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Open&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34; - &amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ts&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;map&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;price_ts&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and to get them back again:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;ts&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;price_ts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;str&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39; - &amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;price_ts&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;price&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;ts&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;number-5-percentage-of-missing-data&#34;&gt;Number 5: Percentage of missing data&lt;/h3&gt;
&lt;p&gt;To get the percentage of missing data in columns and print them in descending order:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# gives % of missing data in columns!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;isnull&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;isnull&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sort_values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ascending&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;number-4-highly-correlated-columns&#34;&gt;Number 4: Highly correlated columns&lt;/h3&gt;
&lt;p&gt;Let’s assume that we want to build a predictive model and we would like to know which columns are highly correlated (bigger than 0.10) to the column &lt;code&gt;Volume&lt;/code&gt;!&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;corr_coeff&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;NVDA_price&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;Volume&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;abs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sort_values&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;corr_coeff&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corr_coeff&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;0.10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;number-3-apply-and-lambda-functions&#34;&gt;Number 3: Apply and lambda functions&lt;/h3&gt;
&lt;p&gt;One of the most awesome things that you can do with pandas is that you can store objects such as &lt;code&gt;networkx&lt;/code&gt; graphs in them. Then you can easily &lt;code&gt;.apply()&lt;/code&gt; the metrics and algorithms on each graph and see the tabulated results in a nice pandas dataframe. You can also easily &lt;code&gt;.apply()&lt;/code&gt; custom functions or &lt;code&gt;lambdas&lt;/code&gt; to modify all objects at once. The index then can be used to keep track of everything and to keep results near their original objects.&lt;/p&gt;
&lt;p&gt;In the example let’s assume that you have multiple networks from different years. We will make a graph at &lt;code&gt;random&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;networkx&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;nx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;itertools&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;random&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;years&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;graphs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;year&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;barbell_graph&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;randint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;50&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)))&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;year&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2010&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2019&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DataFrame&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;({&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;graph&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;graphs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;index&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;years&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;number_of_nodes&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;graph&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;number_of_nodes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;number_of_edges&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;graph&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;number_of_edges&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;transitivity&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;graph&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nx&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transitivity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;nodes_odd_degree&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# Calculate list of nodes with odd degree&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;nodes_odd_degree&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;v&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;v&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;degree&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;%&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nodes_odd_degree&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;len_odd_node_pairs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# Compute all pairs of odd nodes and return the length of list of tuples!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;odd_node_pairs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;itertools&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;combinations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nodes_odd_degree&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;g&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;odd_node_pairs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;nodes_odd_degree&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;graph&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nodes_odd_degree&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;len_nodes_odd_degree&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;nodes_odd_degree&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;len_odd_node_pairs&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;graph&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;len_odd_node_pairs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# And now print a selection of columns:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;number_of_nodes&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;number_of_edges&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;transitivity&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;nodes_odd_degree&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;len_nodes_odd_degree&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;len_odd_node_pairs&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;number-2-drop_duplicates-vs-unique&#34;&gt;Number 2: drop_duplicates vs unique&lt;/h3&gt;
&lt;p&gt;Let’s assume that you have a column with millions of entries and you need to find the unique set of values It. &lt;code&gt;df.column_name.drop_duplicates(keep=&amp;quot;first&amp;quot;, inplace=False)&lt;/code&gt; is a lot faster than &lt;code&gt;df.column_name.unique()&lt;/code&gt;. Can be tested using &lt;code&gt;%timeit&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;keep=&amp;quot;first&amp;quot;&lt;/code&gt; option drops all the duplicates except for the first occurrence.&lt;/p&gt;
&lt;h3 id=&#34;number-1-ufuncs&#34;&gt;Number 1: Ufuncs&lt;/h3&gt;
&lt;p&gt;As explained 
 Pandas &lt;code&gt;Ufuncs&lt;/code&gt; is much better than &lt;code&gt;.apply&lt;/code&gt; command.&lt;/p&gt;
&lt;h2 id=&#34;wrap-up&#34;&gt;Wrap-Up&lt;/h2&gt;
&lt;p&gt;This post should give you an idea of some Pandas tricks commonly used by data scientists. The list aims to make your code more efficient.&lt;/p&gt;
&lt;p&gt;In my next post, I&amp;rsquo;ll share some basic concepts you need for working with data in &lt;code&gt;PySpark&lt;/code&gt;. The post will cover topics such as reading and writing data from CSV files, along with some basic statistical analysis and visualization of data.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;notes&#34;&gt;Notes:&lt;/h3&gt;
&lt;p&gt;1 - The official panda

.&lt;/p&gt;
&lt;p&gt;2- Link to the 
.&lt;/p&gt;
&lt;h3 id=&#34;refernces&#34;&gt;Refernces:&lt;/h3&gt;
&lt;p&gt;1 - 
&lt;/p&gt;
&lt;p&gt;2 - 
&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Please leave a message below if you would like to share your thoughts or if you like to share your tricks on Pandas.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Returning to My Roots - Embracing the Joys of Academic Life</title>
      <link>https://dataqubed.io/news/teach-courses/</link>
      <pubDate>Mon, 01 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/news/teach-courses/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve got some good news to share!&lt;/p&gt;
&lt;p&gt;After spending around 10 years in the industry, I&amp;rsquo;m happy to be back in the academic world. It turns out, I missed the joy of never-ending coffee and brainstorming sessions next to writing proposals, and scientific articles as well as serving as a reviewer of journals and conferences.&lt;/p&gt;
&lt;p&gt;And wow, I did miss it! I don&amp;rsquo;t regret a single moment of my time in the industry, but I do wonder why it took me so long to realize what truly drives and excites me. I guess you could say it was my &lt;code&gt;Eureka!&lt;/code&gt; moment.&lt;/p&gt;
&lt;p&gt;I’ve joined Breda University of Applied Sciences as a Faculty lecturer in &lt;code&gt;data_science&lt;/code&gt; and &lt;code&gt;machine_learning&lt;/code&gt;, and it’s been incredibly rewarding so far. Plus, I get to throw around terms like machine learning, data science and &lt;code&gt;AI&lt;/code&gt; even more and look really hashtag#smart.&lt;/p&gt;
&lt;p&gt;A huge shoutout to all my amazing colleagues in the industry especially folks at Basic-Fit, KLM Royal Dutch Airlines, Randstad, Capgemini, Philips, Shell. Let’s stay in touch! You never know when I might need a break from grading students and want to hear some good hashtag#gossip next to finding common group projects to collaborate on.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m looking forward to collaboration, not only with former colleagues from the academic world but also with new, enriching, and perhaps even quirky connections.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;#NewBeginnings&lt;/code&gt; &lt;code&gt;#AcademicLife&lt;/code&gt; &lt;code&gt;#DataScience&lt;/code&gt; &lt;code&gt;#MachineLearning&lt;/code&gt; &lt;code&gt;#FeelingGrateful&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Thank you Breda University of Applied Sciences!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>👩🏼‍🏫 Teach academic courses - Shorts</title>
      <link>https://dataqubed.io/blog/teach-courses/</link>
      <pubDate>Wed, 29 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/blog/teach-courses/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Embed videos, podcasts, code, LaTeX math, and even test students!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On this page, you&amp;rsquo;ll find some examples of the types of technical content that can be rendered with Hugo Blox.&lt;/p&gt;
&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;
&lt;p&gt;Teach your course by sharing videos with your students. Choose from one of the following approaches:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Youtube&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{&amp;lt; youtube D2vj0WcvH5c &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/D2vj0WcvH5c?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Bilibili&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{&amp;lt; bilibili BV1WV4y1r7DF &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;w-full h-auto aspect-video relative&#34;&gt;
  &lt;iframe src=&#34;//player.bilibili.com/player.html?bvid=BV1WV4y1r7DF&amp;page=1&#34;
  allow=&#34;accelerometer; clipboard-write; encrypted-media; gyroscope; fullscreen; picture-in-picture;&#34;
  class=&#34;w-full h-full&#34;
  &gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Video file&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Videos may be added to a page by either placing them in your &lt;code&gt;assets/media/&lt;/code&gt; media library or in your 
, and then embedding them with the &lt;em&gt;video&lt;/em&gt; shortcode:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{&amp;lt; video src=&amp;quot;my_video.mp4&amp;quot; controls=&amp;quot;yes&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;podcast&#34;&gt;Podcast&lt;/h2&gt;
&lt;p&gt;You can add a podcast or music to a page by placing the MP3 file in the page&amp;rsquo;s folder or the media library folder and then embedding the audio on your page with the &lt;em&gt;audio&lt;/em&gt; shortcode:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{&amp;lt; audio src=&amp;quot;ambient-piano.mp3&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Try it out:&lt;/p&gt;








  








&lt;audio controls &gt;
  &lt;source src=&#34;https://dataqubed.io/blog/teach-courses/ambient-piano.mp3&#34; type=&#34;audio/mpeg&#34;&gt;
&lt;/audio&gt;

&lt;h2 id=&#34;test-students&#34;&gt;Test students&lt;/h2&gt;
&lt;p&gt;Provide a simple yet fun self-assessment by revealing the solutions to challenges with the &lt;code&gt;spoiler&lt;/code&gt; shortcode:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;spoiler&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;👉 Click to view the solution&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;You found me!
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spoiler&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary class=&#34;cursor-pointer&#34;&gt;👉 Click to view the solution&lt;/summary&gt;
  &lt;div class=&#34;rounded-lg bg-neutral-50 dark:bg-neutral-800 p-2&#34;&gt;
    You found me 🎉
  &lt;/div&gt;
&lt;/details&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;Hugo Blox Builder supports a Markdown extension for $\LaTeX$ math. Enable math by setting the &lt;code&gt;math: true&lt;/code&gt; option in your page&amp;rsquo;s front matter, or enable math for your entire site by toggling math in your &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;math&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;enable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;true&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To render &lt;em&gt;inline&lt;/em&gt; or &lt;em&gt;block&lt;/em&gt; math, wrap your LaTeX math with &lt;code&gt;$...$&lt;/code&gt; or &lt;code&gt;$$...$$&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;math block&lt;/strong&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;$$&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\gamma&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;_{n} &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\frac&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{ &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\left&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; | &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\left&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; x_{n} &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; x_{n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;} &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\right&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;^T &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\left&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\nabla&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; F &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; x_{n}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\nabla&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; F &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; x_{n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\right&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\right&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; |}{&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\left&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\|\nabla&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{x}_{n}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\nabla&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; F&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\mathbf&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{x}_{n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\right&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\|&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;^&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;$$&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;
$$\gamma_{n} = \frac{ \left | \left (\mathbf x_{n} - \mathbf x_{n-1} \right )^T \left [\nabla F (\mathbf x_{n}) - \nabla F (\mathbf x_{n-1}) \right ] \right |}{\left \|\nabla F(\mathbf{x}_{n}) - \nabla F(\mathbf{x}_{n-1}) \right \|^2}$$&lt;p&gt;Example &lt;strong&gt;inline math&lt;/strong&gt; &lt;code&gt;$\nabla F(\mathbf{x}_{n})$&lt;/code&gt; renders as $\nabla F(\mathbf{x}_{n})$.&lt;/p&gt;
&lt;p&gt;Example &lt;strong&gt;multi-line math&lt;/strong&gt; using the math linebreak (&lt;code&gt;\\&lt;/code&gt;):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-latex&#34; data-lang=&#34;latex&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;$$&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;k;p_{&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}^{&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\begin&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{cases}p_{&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}^{&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;} &amp;amp; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\text&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{if }k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;, &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\\&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;p_{&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;}^{&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;} &amp;amp; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\text&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{if }k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;\end&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;{cases}&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;$$&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;
$$
f(k;p_{0}^{*}) = \begin{cases}p_{0}^{*} &amp; \text{if }k=1, \\
1-p_{0}^{*} &amp; \text{if }k=0.\end{cases}
$$&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;Hugo Blox Builder utilises Hugo&amp;rsquo;s Markdown extension for highlighting code syntax. The code theme can be selected in the &lt;code&gt;config/_default/params.yaml&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```python
import pandas as pd
data = pd.read_csv(&amp;quot;data.csv&amp;quot;)
data.head()
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pandas&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;pd&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pd&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_csv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data.csv&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;head&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;inline-images&#34;&gt;Inline Images&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{{&amp;lt;&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;icon&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;python&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;}}&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;Python&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;
&lt;p&gt;
  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; height=&#34;1em&#34; viewBox=&#34;0 0 448 512&#34; fill=&#34;currentColor&#34;&gt;&lt;path d=&#34;M439.8 200.5c-7.7-30.9-22.3-54.2-53.4-54.2h-40.1v47.4c0 36.8-31.2 67.8-66.8 67.8H172.7c-29.2 0-53.4 25-53.4 54.3v101.8c0 29 25.2 46 53.4 54.3 33.8 9.9 66.3 11.7 106.8 0 26.9-7.8 53.4-23.5 53.4-54.3v-40.7H226.2v-13.6h160.2c31.1 0 42.6-21.7 53.4-54.2 11.2-33.5 10.7-65.7 0-108.6zM286.2 404c11.1 0 20.1 9.1 20.1 20.3 0 11.3-9 20.4-20.1 20.4-11 0-20.1-9.2-20.1-20.4.1-11.3 9.1-20.3 20.1-20.3zM167.8 248.1h106.8c29.7 0 53.4-24.5 53.4-54.3V91.9c0-29-24.4-50.7-53.4-55.6-35.8-5.9-74.7-5.6-106.8.1-45.2 8-53.4 24.7-53.4 55.6v40.7h106.9v13.6h-147c-31.1 0-58.3 18.7-66.8 54.2-9.8 40.7-10.2 66.1 0 108.6 7.6 31.6 25.7 54.2 56.8 54.2H101v-48.8c0-35.3 30.5-66.4 66.8-66.4zm-6.7-142.6c-11.1 0-20.1-9.1-20.1-20.3.1-11.3 9-20.4 20.1-20.4 11 0 20.1 9.2 20.1 20.4s-9 20.3-20.1 20.3z&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; Python&lt;/p&gt;
&lt;h2 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>🎉 Easily create your own simple yet highly customizable blog - Shorts</title>
      <link>https://dataqubed.io/blog/get-started/</link>
      <pubDate>Fri, 27 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/blog/get-started/</guid>
      <description>&lt;p&gt;Welcome 👋&lt;/p&gt;



&lt;details class=&#34;print:hidden xl:hidden&#34; open&gt;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;
  &lt;div class=&#34;text-sm&#34;&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#get-started&#34;&gt;Get Started&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#crowd-funded-open-source-software&#34;&gt;Crowd-funded open-source software&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#-click-here-to-become-a-sponsor-and-help-support-hugo-bloxs-future-httpshugobloxcomsponsor&#34;&gt;&lt;a href=&#34;https://hugoblox.com/sponsor/&#34;&gt;❤️ Click here to become a sponsor and help support Hugo Blox&amp;rsquo;s future ❤️&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#ecosystem&#34;&gt;Ecosystem&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#inspiration&#34;&gt;Inspiration&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#themes&#34;&gt;Themes&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
  &lt;/div&gt;
&lt;/details&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;The Hugo Blox website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site&lt;/li&gt;
&lt;li&gt;The template can be modified and customised to suit your needs. It&amp;rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a &lt;strong&gt;no-code solution (write in Markdown and customize with YAML parameters)&lt;/strong&gt; and having &lt;strong&gt;flexibility to later add even deeper personalization with HTML and CSS&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;get-started&#34;&gt;Get Started&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;👉 
&lt;/li&gt;
&lt;li&gt;📚 
&lt;/li&gt;
&lt;li&gt;💬 
 or 
&lt;/li&gt;
&lt;li&gt;🐦 Twitter: 
 
 #MadeWithHugoBlox&lt;/li&gt;
&lt;li&gt;💡 
&lt;/li&gt;
&lt;li&gt;⬆️ &lt;strong&gt;Updating Hugo Blox?&lt;/strong&gt; View the 
 and 
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;crowd-funded-open-source-software&#34;&gt;Crowd-funded open-source software&lt;/h2&gt;
&lt;p&gt;To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.&lt;/p&gt;
&lt;h3 id=&#34;-click-here-to-become-a-sponsor-and-help-support-hugo-bloxs-future-httpshugobloxcomsponsor&#34;&gt;
&lt;/h3&gt;
&lt;p&gt;As a token of appreciation for sponsoring, you can &lt;strong&gt;unlock 
 awesome rewards and extra features 🦄✨&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;ecosystem&#34;&gt;Ecosystem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;
:&lt;/strong&gt; Automatically import publications from BibTeX&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inspiration&#34;&gt;Inspiration&lt;/h2&gt;
&lt;p&gt;
 are building with this template.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Page builder&lt;/strong&gt; - Create &lt;em&gt;anything&lt;/em&gt; with no-code 
 and 
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Edit any type of content&lt;/strong&gt; - Blog posts, publications, talks, slides, projects, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create content&lt;/strong&gt; in 
, 
, or 
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Plugin System&lt;/strong&gt; - Fully customizable 
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Display Code and Math&lt;/strong&gt; - Code syntax highlighting and LaTeX math supported&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrations&lt;/strong&gt; - 
, 
, Maps, Contact Forms, and more!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Beautiful Site&lt;/strong&gt; - Simple and refreshing one-page design&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Industry-Leading SEO&lt;/strong&gt; - Help get your website found on search engines and social media&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Galleries&lt;/strong&gt; - Display your images and videos with captions in a customizable gallery&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mobile Friendly&lt;/strong&gt; - Look amazing on every screen with a mobile friendly version of your site&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language&lt;/strong&gt; - 35+ language packs including English, 中文, and Português&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-user&lt;/strong&gt; - Each author gets their own profile page&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Privacy Pack&lt;/strong&gt; - Assists with GDPR&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stand Out&lt;/strong&gt; - Bring your site to life with animation, parallax backgrounds, and scroll effects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;One-Click Deployment&lt;/strong&gt; - No servers. No databases. Only files.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;p&gt;Hugo Blox and its templates come with &lt;strong&gt;automatic day (light) and night (dark) mode&lt;/strong&gt; built-in. Visitors can choose their preferred mode by clicking the sun/moon icon in the header.&lt;/p&gt;
&lt;p&gt;
 for your site. Themes are fully customizable.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2016-present 
.&lt;/p&gt;
&lt;p&gt;Released under the 
 license.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pandas</title>
      <link>https://dataqubed.io/blog/pandas/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/blog/pandas/</guid>
      <description>&lt;p&gt;Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PyTorch</title>
      <link>https://dataqubed.io/blog/pytorch/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/blog/pytorch/</guid>
      <description>&lt;p&gt;PyTorch is a Python package that provides tensor computation (like NumPy) with strong GPU acceleration.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>scikit-learn</title>
      <link>https://dataqubed.io/blog/scikit/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/blog/scikit/</guid>
      <description>&lt;p&gt;scikit-learn is a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>🧠 Sharpen your thinking with a second brain</title>
      <link>https://dataqubed.io/blog/second-brain/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/blog/second-brain/</guid>
      <description>&lt;p&gt;Create a personal knowledge base and share your knowledge with your peers.&lt;/p&gt;
&lt;p&gt;Hugo Blox web framework empowers you with one of the most flexible note-taking capabilities out there.&lt;/p&gt;
&lt;p&gt;Create a powerful knowledge base that works on top of a local folder of plain text Markdown files.&lt;/p&gt;
&lt;p&gt;Use it as your second brain, either publicly sharing your knowledge with your peers via your website, or via a private GitHub repository and password-protected site just for yourself.&lt;/p&gt;
&lt;h2 id=&#34;mindmaps&#34;&gt;Mindmaps&lt;/h2&gt;
&lt;p&gt;Hugo Blox supports a Markdown extension for mindmaps.&lt;/p&gt;
&lt;p&gt;With this open format, can even edit your mindmaps in other popular tools such as Obsidian.&lt;/p&gt;
&lt;p&gt;Simply insert a Markdown code block labelled as &lt;code&gt;markmap&lt;/code&gt; and optionally set the height of the mindmap as shown in the example below.&lt;/p&gt;
&lt;p&gt;Mindmaps can be created by simply writing the items as a Markdown list within the &lt;code&gt;markmap&lt;/code&gt; code block, indenting each item to create as many sub-levels as you need:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
```markmap {height=&#34;200px&#34;}
- Hugo Modules
  - Hugo Blox
  - blox-plugins-netlify
  - blox-plugins-netlify-cms
  - blox-plugins-reveal
```
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;markmap&#34; style=&#34;height: 200px;&#34;&gt;

&lt;pre&gt;- Hugo Modules
  - Hugo Blox
  - blox-plugins-netlify
  - blox-plugins-netlify-cms
  - blox-plugins-reveal&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Anh here&amp;rsquo;s a more advanced mindmap with formatting, code blocks, and math:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;
&lt;code&gt;
```markmap
- Mindmaps
  - Links
    - [Hugo Blox Docs](https://docs.hugoblox.com/)
    - [Discord Community](https://discord.gg/z8wNYzb)
    - [GitHub](https://github.com/HugoBlox/hugo-blox-builder)
  - Features
    - Markdown formatting
    - **inline** ~~text~~ *styles*
    - multiline
      text
    - `inline code`
    -
      ```js
      console.log(&#39;hello&#39;);
      console.log(&#39;code block&#39;);
      ```
    - Math: $x = {-b \pm \sqrt{b^2-4ac} \over 2a}$
```
&lt;/code&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;markmap&#34; style=&#34;height: 500px;&#34;&gt;

&lt;pre&gt;- Mindmaps
  - Links
    - [Hugo Blox Docs](https://docs.hugoblox.com/)
    - [Discord Community](https://discord.gg/z8wNYzb)
    - [GitHub](https://github.com/HugoBlox/hugo-blox-builder)
  - Features
    - Markdown formatting
    - **inline** ~~text~~ *styles*
    - multiline
      text
    - `inline code`
    -
      ```js
      console.log(&#39;hello&#39;);
      console.log(&#39;code block&#39;);
      ```
    - Math: $x = {-b \pm \sqrt{b^2-4ac} \over 2a}$&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&#34;highlighting&#34;&gt;Highlighting&lt;/h2&gt;
&lt;p&gt;&lt;mark&gt;Highlight&lt;/mark&gt; important text with &lt;code&gt;mark&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-html&#34; data-lang=&#34;html&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;mark&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;Highlighted text&lt;span class=&#34;p&#34;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;mark&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;callouts&#34;&gt;Callouts&lt;/h2&gt;
&lt;p&gt;Use 
 (aka &lt;em&gt;asides&lt;/em&gt;, &lt;em&gt;hints&lt;/em&gt;, or &lt;em&gt;alerts&lt;/em&gt;) to draw attention to notes, tips, and warnings.&lt;/p&gt;
&lt;p&gt;By wrapping a paragraph in &lt;code&gt;{{% callout note %}} ... {{% /callout %}}&lt;/code&gt;, it will render as an aside.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% callout note %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;A Markdown aside is useful for displaying notices, hints, or definitions to your readers.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% /callout %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;A Markdown aside is useful for displaying notices, hints, or definitions to your readers.&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;Or use the &lt;code&gt;warning&lt;/code&gt; callout type so your readers don&amp;rsquo;t miss critical details:&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-yellow-100 dark:bg-yellow-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-red-400&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M12 9v3.75m-9.303 3.376c-.866 1.5.217 3.374 1.948 3.374h14.71c1.73 0 2.813-1.874 1.948-3.374L13.949 3.378c-.866-1.5-3.032-1.5-3.898 0zM12 15.75h.007v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;A Markdown aside is useful for displaying notices, hints, or definitions to your readers.&lt;/span&gt;
&lt;/div&gt;
&lt;h2 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>📈 Communicate your results effectively with the best data visualizations - Shorts</title>
      <link>https://dataqubed.io/blog/data-visualization/</link>
      <pubDate>Wed, 25 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/blog/data-visualization/</guid>
      <description>&lt;p&gt;Hugo Blox is designed to give technical content creators a seamless experience. You can focus on the content and Hugo Blox handles the rest.&lt;/p&gt;
&lt;p&gt;Use popular tools such as Plotly, Mermaid, and data frames.&lt;/p&gt;
&lt;h2 id=&#34;charts&#34;&gt;Charts&lt;/h2&gt;
&lt;p&gt;Hugo Blox supports the popular 
 format for interactive data visualizations. With Plotly, you can design almost any kind of visualization you can imagine!&lt;/p&gt;
&lt;p&gt;Save your Plotly JSON in your page folder, for example &lt;code&gt;line-chart.json&lt;/code&gt;, and then add the &lt;code&gt;{{&amp;lt; chart data=&amp;quot;line-chart&amp;quot; &amp;gt;}}&lt;/code&gt; shortcode where you would like the chart to appear.&lt;/p&gt;
&lt;p&gt;Demo:&lt;/p&gt;




&lt;div id=&#34;chart-386124579&#34; class=&#34;chart&#34;&gt;&lt;/div&gt;
&lt;script&gt;
  async function fetchChartJSON() {
    console.debug(&#39;Hugo Blox fetching chart JSON...&#39;)
    const response = await fetch(&#39;.\/line-chart.json&#39;);
    return await response.json();
  }

  (function() {
    let a = setInterval( function() {
      if ( typeof window.Plotly === &#39;undefined&#39; ) {
        console.debug(&#39;Plotly not loaded yet...&#39;)
        return;
      }
      clearInterval( a );

      fetchChartJSON().then(chart =&gt; {
        console.debug(&#39;Plotting chart...&#39;)
        window.Plotly.newPlot(&#39;chart-386124579&#39;, chart.data, chart.layout, {responsive: true});
      });
    }, 500 );
  })();
&lt;/script&gt;

&lt;p&gt;You might also find the 
 useful.&lt;/p&gt;
&lt;h2 id=&#34;diagrams&#34;&gt;Diagrams&lt;/h2&gt;
&lt;p&gt;Hugo Blox supports the &lt;em&gt;Mermaid&lt;/em&gt; Markdown extension for diagrams.&lt;/p&gt;
&lt;p&gt;An example &lt;strong&gt;flowchart&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
graph TD
A[Hard] --&amp;gt;|Text| B(Round)
B --&amp;gt; C{Decision}
C --&amp;gt;|One| D[Result 1]
C --&amp;gt;|Two| E[Result 2]
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;graph TD
A[Hard] --&gt;|Text| B(Round)
B --&gt; C{Decision}
C --&gt;|One| D[Result 1]
C --&gt;|Two| E[Result 2]
&lt;/div&gt;
&lt;p&gt;An example &lt;strong&gt;sequence diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
sequenceDiagram
Alice-&amp;gt;&amp;gt;John: Hello John, how are you?
loop Healthcheck
    John-&amp;gt;&amp;gt;John: Fight against hypochondria
end
Note right of John: Rational thoughts!
John--&amp;gt;&amp;gt;Alice: Great!
John-&amp;gt;&amp;gt;Bob: How about you?
Bob--&amp;gt;&amp;gt;John: Jolly good!
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;sequenceDiagram
Alice-&gt;&gt;John: Hello John, how are you?
loop Healthcheck
    John-&gt;&gt;John: Fight against hypochondria
end
Note right of John: Rational thoughts!
John--&gt;&gt;Alice: Great!
John-&gt;&gt;Bob: How about you?
Bob--&gt;&gt;John: Jolly good!
&lt;/div&gt;
&lt;p&gt;An example &lt;strong&gt;class diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
classDiagram
Class01 &amp;lt;|-- AveryLongClass : Cool
Class03 *-- Class04
Class05 o-- Class06
Class07 .. Class08
Class09 --&amp;gt; C2 : Where am i?
Class09 --* C3
Class09 --|&amp;gt; Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
Class08 &amp;lt;--&amp;gt; C2: Cool label
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;classDiagram
Class01 &lt;|-- AveryLongClass : Cool
Class03 *-- Class04
Class05 o-- Class06
Class07 .. Class08
Class09 --&gt; C2 : Where am i?
Class09 --* C3
Class09 --|&gt; Class07
Class07 : equals()
Class07 : Object[] elementData
Class01 : size()
Class01 : int chimp
Class01 : int gorilla
Class08 &lt;--&gt; C2: Cool label
&lt;/div&gt;
&lt;p&gt;An example &lt;strong&gt;state diagram&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;```mermaid
stateDiagram
[*] --&amp;gt; Still
Still --&amp;gt; [*]
Still --&amp;gt; Moving
Moving --&amp;gt; Still
Moving --&amp;gt; Crash
Crash --&amp;gt; [*]
```
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;renders as&lt;/p&gt;
&lt;div class=&#34;mermaid&#34;&gt;stateDiagram
[*] --&gt; Still
Still --&gt; [*]
Still --&gt; Moving
Moving --&gt; Still
Moving --&gt; Crash
Crash --&gt; [*]
&lt;/div&gt;
&lt;h2 id=&#34;data-frames&#34;&gt;Data Frames&lt;/h2&gt;
&lt;p&gt;Save your spreadsheet as a CSV file in your page&amp;rsquo;s folder and then render it by adding the &lt;em&gt;Table&lt;/em&gt; shortcode to your page:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{{&amp;lt;&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;table&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;results.csv&amp;#34;&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;header&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt; &lt;span class=&#34;nx&#34;&gt;caption&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;Table 1: My results&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;}}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;renders as&lt;/p&gt;














&lt;table class=&#34;table-auto w-full&#34;&gt;
  
    
    
    &lt;thead&gt;
      &lt;tr&gt;  &lt;th class=&#34;border-b dark:border-slate-600 font-medium p-4 pt-0 pb-3 text-slate-400 dark:text-slate-200 text-left&#34;&gt;customer_id&lt;/th&gt;  &lt;th class=&#34;border-b dark:border-slate-600 font-medium p-4 pt-0 pb-3 text-slate-400 dark:text-slate-200 text-left&#34;&gt;score&lt;/th&gt;  &lt;/tr&gt;
    &lt;/thead&gt;
  
  &lt;tbody&gt;
  
    &lt;tr&gt;
      
        
          &lt;td data-table-dtype=&#34;number&#34; class=&#34;border-b border-slate-100 dark:border-slate-700 p-4 text-slate-500 dark:text-slate-400&#34;&gt;1&lt;/td&gt;
        
      
        
          &lt;td data-table-dtype=&#34;number&#34; class=&#34;border-b border-slate-100 dark:border-slate-700 p-4 text-slate-500 dark:text-slate-400&#34;&gt;0&lt;/td&gt;
        
      
    &lt;/tr&gt;
  
    &lt;tr&gt;
      
        
          &lt;td data-table-dtype=&#34;number&#34; class=&#34;border-b border-slate-100 dark:border-slate-700 p-4 text-slate-500 dark:text-slate-400&#34;&gt;2&lt;/td&gt;
        
      
        
          &lt;td data-table-dtype=&#34;text&#34; class=&#34;border-b border-slate-100 dark:border-slate-700 p-4 text-slate-500 dark:text-slate-400&#34;&gt;0.5&lt;/td&gt;
        
      
    &lt;/tr&gt;
  
    &lt;tr&gt;
      
        
          &lt;td data-table-dtype=&#34;number&#34; class=&#34;border-b border-slate-100 dark:border-slate-700 p-4 text-slate-500 dark:text-slate-400&#34;&gt;3&lt;/td&gt;
        
      
        
          &lt;td data-table-dtype=&#34;number&#34; class=&#34;border-b border-slate-100 dark:border-slate-700 p-4 text-slate-500 dark:text-slate-400&#34;&gt;1&lt;/td&gt;
        
      
    &lt;/tr&gt;
  
  &lt;/tbody&gt;
  
    &lt;caption class=&#34;table-caption&#34;&gt;Table 1: My results&lt;/caption&gt;
  
&lt;/table&gt;

&lt;h2 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Experience</title>
      <link>https://dataqubed.io/experience/</link>
      <pubDate>Tue, 24 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An example preprint / working paper</title>
      <link>https://dataqubed.io/publication/preprint/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/publication/preprint/</guid>
      <description>&lt;p&gt;This work is driven by the results in my 
 on LLMs.&lt;/p&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;Add the publication&amp;rsquo;s &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; here. You can use rich formatting such as including 
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example journal article</title>
      <link>https://dataqubed.io/publication/journal-article/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/publication/journal-article/</guid>
      <description>&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;Add the publication&amp;rsquo;s &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; here. You can use rich formatting such as including 
.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An example conference paper</title>
      <link>https://dataqubed.io/publication/conference-paper/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://dataqubed.io/publication/conference-paper/</guid>
      <description>&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&#34;flex px-4 py-3 mb-6 rounded-md bg-primary-100 dark:bg-primary-900&#34;&gt;
&lt;span class=&#34;pr-3 pt-1 text-primary-600 dark:text-primary-300&#34;&gt;
  &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m11.25 11.25l.041-.02a.75.75 0 0 1 1.063.852l-.708 2.836a.75.75 0 0 0 1.063.853l.041-.021M21 12a9 9 0 1 1-18 0a9 9 0 0 1 18 0m-9-3.75h.008v.008H12z&#34;/&gt;&lt;/svg&gt;
&lt;/span&gt;
  &lt;span class=&#34;dark:text-neutral-300&#34;&gt;Create your slides in Markdown - click the &lt;em&gt;Slides&lt;/em&gt; button to check out the example.&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;Add the publication&amp;rsquo;s &lt;strong&gt;full text&lt;/strong&gt; or &lt;strong&gt;supplementary notes&lt;/strong&gt; here. You can use rich formatting such as including 
.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
